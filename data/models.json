{
  "metadata": {
    "version": "2.0.0",
    "lastUpdated": "2026-01-04",
    "description": "LLM 벤치마크 통합 데이터",
    "totalModels": 267,
    "defaultModelIds": [],
    "dataSources": {
      "lmarena": {
        "lastFetched": "2025-12-31T14:32:25.281Z",
        "totalModels": 291
      },
      "artificialanalysis": {
        "lastFetched": "2026-01-04T00:21:40.383Z",
        "totalModels": 373
      }
    }
  },
  "models": [
    {
      "id": "gemini-3-pro",
      "name": "Gemini 3 Pro Preview (high)",
      "provider": "Google",
      "releaseDate": "2025-11-18",
      "benchmarks": {
        "MMLU Pro": 89.8,
        "GPQA Diamond": 90.8,
        "Humanity's Last Exam": 37.2,
        "AA-LCR": 70.7,
        "LiveCodeBench": 91.7,
        "SciCode": 56.10000000000001,
        "AIME 2025": 95.7,
        "MMMU Pro": 80,
        "AA-Omniscience": 13,
        "AA-Omniscience Accuracy": 53.65,
        "AA-Omniscience Hallucination Rate": 87.99,
        "LMArena-Text": 1490,
        "LMArena-Text-Creative-Writing": 1489,
        "LMArean-Text-Math": 1480,
        "LMArena-Text-Coding": 1520,
        "LMArena-Text-Expert": 1504,
        "LMArena-Text-Hard-Prompts": 1505,
        "LMArena-Text-Longer-Query": 1494,
        "LMArena-Text-Multi-Turn": 1496,
        "LMArena-Vision": 1309
      },
      "metadata": {
        "lmarenaRank": 1,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 12
        },
        "performance": {
          "outputTokensPerSecond": 135.997,
          "timeToFirstToken": 30.693
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "gemini-3-flash-reasoning",
      "name": "Gemini 3 Flash Preview (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-12-17",
      "benchmarks": {
        "MMLU Pro": 89,
        "GPQA Diamond": 89.8,
        "Humanity's Last Exam": 34.699999999999996,
        "AA-LCR": 66.3,
        "LiveCodeBench": 90.8,
        "SciCode": 50.6,
        "AIME 2025": 97,
        "MMMU Pro": 80,
        "AA-Omniscience": 13,
        "AA-Omniscience Accuracy": 54.5,
        "AA-Omniscience Hallucination Rate": 91.21,
        "LMArena-Text": 1478,
        "LMArena-Text-Creative-Writing": 1463,
        "LMArean-Text-Math": 1480,
        "LMArena-Text-Coding": 1505,
        "LMArena-Text-Expert": 1487,
        "LMArena-Text-Hard-Prompts": 1489,
        "LMArena-Text-Longer-Query": 1482,
        "LMArena-Text-Multi-Turn": 1474,
        "LMArena-Vision": 1284
      },
      "metadata": {
        "lmarenaRank": 2,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 3
        },
        "performance": {
          "outputTokensPerSecond": 226.004,
          "timeToFirstToken": 11.009
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-opus-4-5-thinking",
      "name": "Claude Opus 4.5 (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-11-24",
      "benchmarks": {
        "MMLU Pro": 89.5,
        "GPQA Diamond": 86.6,
        "Humanity's Last Exam": 28.4,
        "AA-LCR": 74,
        "LiveCodeBench": 87.1,
        "SciCode": 49.5,
        "AIME 2025": 91.3,
        "MMMU Pro": 74,
        "AA-Omniscience": 10,
        "AA-Omniscience Accuracy": 43.12,
        "AA-Omniscience Hallucination Rate": 57.81,
        "LMArena-Text": 1469,
        "LMArena-Text-Creative-Writing": 1451,
        "LMArean-Text-Math": 1466,
        "LMArena-Text-Coding": 1540,
        "LMArena-Text-Expert": 1512,
        "LMArena-Text-Hard-Prompts": 1502,
        "LMArena-Text-Longer-Query": 1496,
        "LMArena-Text-Multi-Turn": 1469,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 4,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 5,
          "output": 25
        },
        "performance": {
          "outputTokensPerSecond": 63.753,
          "timeToFirstToken": 1.668
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-opus-4-5",
      "name": "Claude Opus 4.5 (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-11-24",
      "benchmarks": {
        "MMLU Pro": 88.9,
        "GPQA Diamond": 81,
        "Humanity's Last Exam": 12.9,
        "AA-LCR": 65.3,
        "LiveCodeBench": 73.8,
        "SciCode": 47,
        "AIME 2025": 62.7,
        "MMMU Pro": 71,
        "AA-Omniscience": -6,
        "AA-Omniscience Accuracy": 38.9,
        "AA-Omniscience Hallucination Rate": 74.22,
        "LMArena-Text": 1467,
        "LMArena-Text-Creative-Writing": 1458,
        "LMArean-Text-Math": 1466,
        "LMArena-Text-Coding": 1515,
        "LMArena-Text-Expert": 1527,
        "LMArena-Text-Hard-Prompts": 1493,
        "LMArena-Text-Longer-Query": 1497,
        "LMArena-Text-Multi-Turn": 1476,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 5,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 5,
          "output": 25
        },
        "performance": {
          "outputTokensPerSecond": 80.38,
          "timeToFirstToken": 1.906
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-3-flash",
      "name": "Gemini 3 Flash Preview (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-12-17",
      "benchmarks": {
        "MMLU Pro": 88.2,
        "GPQA Diamond": 81.2,
        "Humanity's Last Exam": 14.099999999999998,
        "AA-LCR": 48,
        "LiveCodeBench": 79.7,
        "SciCode": 49.9,
        "AIME 2025": 55.7,
        "MMMU Pro": 79,
        "AA-Omniscience": -1,
        "AA-Omniscience Accuracy": 47.15,
        "AA-Omniscience Hallucination Rate": 90.95,
        "LMArena-Text": 1463,
        "LMArena-Text-Creative-Writing": 1438,
        "LMArean-Text-Math": 1474,
        "LMArena-Text-Coding": 1497,
        "LMArena-Text-Expert": 1467,
        "LMArena-Text-Hard-Prompts": 1481,
        "LMArena-Text-Longer-Query": 1469,
        "LMArena-Text-Multi-Turn": 1491,
        "LMArena-Vision": 1268
      },
      "metadata": {
        "lmarenaRank": 7,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 3
        },
        "performance": {
          "outputTokensPerSecond": 202.576,
          "timeToFirstToken": 0.677
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-1",
      "name": "GPT-5.1 (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 87,
        "GPQA Diamond": 87.3,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75,
        "LiveCodeBench": 86.8,
        "SciCode": 43.3,
        "AIME 2025": 94,
        "MMMU Pro": 76,
        "AA-Omniscience": 2,
        "AA-Omniscience Accuracy": 35.3,
        "AA-Omniscience Hallucination Rate": 51.16,
        "LMArena-Text": 1455,
        "LMArena-Text-Creative-Writing": 1435,
        "LMArean-Text-Math": 1474,
        "LMArena-Text-Coding": 1488,
        "LMArena-Text-Expert": 1484,
        "LMArena-Text-Hard-Prompts": 1473,
        "LMArena-Text-Longer-Query": 1463,
        "LMArena-Text-Multi-Turn": 1470,
        "LMArena-Vision": 1249
      },
      "metadata": {
        "lmarenaRank": 8,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 192.323,
          "timeToFirstToken": 21.671
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-pro",
      "name": "Gemini 2.5 Pro",
      "provider": "Google",
      "releaseDate": "2025-06-05",
      "benchmarks": {
        "MMLU Pro": 86.2,
        "GPQA Diamond": 84.39999999999999,
        "Humanity's Last Exam": 21.099999999999998,
        "AA-LCR": 66,
        "LiveCodeBench": 80.10000000000001,
        "SciCode": 42.8,
        "AIME 2025": 87.7,
        "MMMU Pro": 75,
        "AA-Omniscience": -18,
        "AA-Omniscience Accuracy": 37.48,
        "AA-Omniscience Hallucination Rate": 88.67,
        "LMArena-Text": 1451,
        "LMArena-Text-Creative-Writing": 1452,
        "LMArean-Text-Math": 1453,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1465,
        "LMArena-Text-Hard-Prompts": 1463,
        "LMArena-Text-Longer-Query": 1464,
        "LMArena-Text-Multi-Turn": 1454,
        "LMArena-Vision": 1249
      },
      "metadata": {
        "lmarenaRank": 9,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 162.359,
          "timeToFirstToken": 32.371
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "claude-4-5-sonnet-thinking",
      "name": "Claude 4.5 Sonnet (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 87.5,
        "GPQA Diamond": 83.39999999999999,
        "Humanity's Last Exam": 17.299999999999997,
        "AA-LCR": 65.7,
        "LiveCodeBench": 71.39999999999999,
        "SciCode": 44.7,
        "AIME 2025": 88,
        "MMMU Pro": 69,
        "AA-Omniscience": -2,
        "AA-Omniscience Accuracy": 30.9,
        "AA-Omniscience Hallucination Rate": 47.73,
        "LMArena-Text": 1450,
        "LMArena-Text-Creative-Writing": 1438,
        "LMArean-Text-Math": 1464,
        "LMArena-Text-Coding": 1525,
        "LMArena-Text-Expert": 1501,
        "LMArena-Text-Hard-Prompts": 1485,
        "LMArena-Text-Longer-Query": 1485,
        "LMArena-Text-Multi-Turn": 1467,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 10,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 67.444,
          "timeToFirstToken": 1.909
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-1-opus-thinking",
      "name": "Claude 4.1 Opus (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 88,
        "GPQA Diamond": 80.9,
        "Humanity's Last Exam": 11.899999999999999,
        "AA-LCR": 66.3,
        "LiveCodeBench": 65.4,
        "SciCode": 40.9,
        "AIME 2025": 80.30000000000001,
        "MMMU Pro": 68,
        "AA-Omniscience": 5,
        "AA-Omniscience Accuracy": 35.93,
        "AA-Omniscience Hallucination Rate": 48.39,
        "LMArena-Text": 1448,
        "LMArena-Text-Creative-Writing": 1441,
        "LMArean-Text-Math": 1448,
        "LMArena-Text-Coding": 1513,
        "LMArena-Text-Expert": 1483,
        "LMArena-Text-Hard-Prompts": 1479,
        "LMArena-Text-Longer-Query": 1485,
        "LMArena-Text-Multi-Turn": 1467,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 11,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 15,
          "output": 75
        },
        "performance": {
          "outputTokensPerSecond": 45.217,
          "timeToFirstToken": 1.306
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-5-sonnet",
      "name": "Claude 4.5 Sonnet (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 86,
        "GPQA Diamond": 72.7,
        "Humanity's Last Exam": 7.1,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 59,
        "SciCode": 42.8,
        "AIME 2025": 37,
        "MMMU Pro": 65,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 26.93,
        "AA-Omniscience Hallucination Rate": 51.44,
        "LMArena-Text": 1447,
        "LMArena-Text-Creative-Writing": 1441,
        "LMArean-Text-Math": 1429,
        "LMArena-Text-Coding": 1506,
        "LMArena-Text-Expert": 1489,
        "LMArena-Text-Hard-Prompts": 1474,
        "LMArena-Text-Longer-Query": 1479,
        "LMArena-Text-Multi-Turn": 1477,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 12,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 71.461,
          "timeToFirstToken": 1.97
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-2-non-reasoning",
      "name": "GPT-5.2 (Non-reasoning)",
      "provider": "OpenAI",
      "releaseDate": "2025-12-11",
      "benchmarks": {
        "MMLU Pro": 81.39999999999999,
        "GPQA Diamond": 71.2,
        "Humanity's Last Exam": 7.3,
        "AA-LCR": 38,
        "LiveCodeBench": 66.9,
        "SciCode": 40.400000000000006,
        "AIME 2025": 51,
        "MMMU Pro": 66,
        "AA-Omniscience": -15,
        "AA-Omniscience Accuracy": 27.95,
        "AA-Omniscience Hallucination Rate": 60.17,
        "LMArena-Text": 1443,
        "LMArena-Text-Creative-Writing": 1408,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 1480,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1472,
        "LMArena-Text-Longer-Query": 1451,
        "LMArena-Text-Multi-Turn": 1457,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 14,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.75,
          "output": 14
        },
        "performance": {
          "outputTokensPerSecond": 78.866,
          "timeToFirstToken": 0.666
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-4o-chatgpt-03-25",
      "name": "GPT-4o (March 2025, chatgpt-4o-latest)",
      "provider": "OpenAI",
      "releaseDate": "2025-03-27",
      "benchmarks": {
        "MMLU Pro": 80.30000000000001,
        "GPQA Diamond": 65.5,
        "Humanity's Last Exam": 5,
        "AA-LCR": 0,
        "LiveCodeBench": 42.5,
        "SciCode": 36.6,
        "AIME 2025": 25.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1440,
        "LMArena-Text-Creative-Writing": 1418,
        "LMArean-Text-Math": 1403,
        "LMArena-Text-Coding": 1464,
        "LMArena-Text-Expert": 1422,
        "LMArena-Text-Hard-Prompts": 1451,
        "LMArena-Text-Longer-Query": 1437,
        "LMArena-Text-Multi-Turn": 1464,
        "LMArena-Vision": 1236
      },
      "metadata": {
        "lmarenaRank": 17,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 5,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 225.942,
          "timeToFirstToken": 0.529
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-2",
      "name": "GPT-5.2 (xhigh)",
      "provider": "OpenAI",
      "releaseDate": "2025-12-11",
      "benchmarks": {
        "MMLU Pro": 87.4,
        "GPQA Diamond": 90.3,
        "Humanity's Last Exam": 31.4,
        "AA-LCR": 72.7,
        "LiveCodeBench": 89.2,
        "SciCode": 52.1,
        "AIME 2025": 99,
        "MMMU Pro": 0,
        "AA-Omniscience": -4,
        "AA-Omniscience Accuracy": 41.35,
        "AA-Omniscience Hallucination Rate": 77.86,
        "LMArena-Text": 1440,
        "LMArena-Text-Creative-Writing": 1387,
        "LMArean-Text-Math": 1503,
        "LMArena-Text-Coding": 1493,
        "LMArena-Text-Expert": 1490,
        "LMArena-Text-Hard-Prompts": 1460,
        "LMArena-Text-Longer-Query": 1443,
        "LMArena-Text-Multi-Turn": 1442,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 18,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.75,
          "output": 14
        },
        "performance": {
          "outputTokensPerSecond": 120.306,
          "timeToFirstToken": 39.36
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-1-non-reasoning",
      "name": "GPT-5.1 (Non-reasoning)",
      "provider": "OpenAI",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 80.10000000000001,
        "GPQA Diamond": 64.3,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 44,
        "LiveCodeBench": 49.4,
        "SciCode": 36.5,
        "AIME 2025": 38,
        "MMMU Pro": 62,
        "AA-Omniscience": -37,
        "AA-Omniscience Accuracy": 27.8,
        "AA-Omniscience Hallucination Rate": 89.17,
        "LMArena-Text": 1438,
        "LMArena-Text-Creative-Writing": 1408,
        "LMArean-Text-Math": 1418,
        "LMArena-Text-Coding": 1479,
        "LMArena-Text-Expert": 1465,
        "LMArena-Text-Hard-Prompts": 1456,
        "LMArena-Text-Longer-Query": 1447,
        "LMArena-Text-Multi-Turn": 1445,
        "LMArena-Vision": 1239
      },
      "metadata": {
        "lmarenaRank": 19,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 142.263,
          "timeToFirstToken": 0.648
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5",
      "name": "GPT-5 (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 87.1,
        "GPQA Diamond": 85.39999999999999,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75.6,
        "LiveCodeBench": 84.6,
        "SciCode": 42.9,
        "AIME 2025": 94.3,
        "MMMU Pro": 74,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 38.62,
        "AA-Omniscience Hallucination Rate": 80.99,
        "LMArena-Text": 1436,
        "LMArena-Text-Creative-Writing": 1381,
        "LMArean-Text-Math": 1438,
        "LMArena-Text-Coding": 1469,
        "LMArena-Text-Expert": 1458,
        "LMArena-Text-Hard-Prompts": 1448,
        "LMArena-Text-Longer-Query": 1417,
        "LMArena-Text-Multi-Turn": 1425,
        "LMArena-Vision": 1210
      },
      "metadata": {
        "lmarenaRank": 20,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 143.687,
          "timeToFirstToken": 72.594
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "o3",
      "name": "o3",
      "provider": "OpenAI",
      "releaseDate": "2025-04-16",
      "benchmarks": {
        "MMLU Pro": 85.3,
        "GPQA Diamond": 82.69999999999999,
        "Humanity's Last Exam": 20,
        "AA-LCR": 69.3,
        "LiveCodeBench": 80.80000000000001,
        "SciCode": 41,
        "AIME 2025": 88.3,
        "MMMU Pro": 70,
        "AA-Omniscience": -17,
        "AA-Omniscience Accuracy": 37.25,
        "AA-Omniscience Hallucination Rate": 86.75,
        "LMArena-Text": 1433,
        "LMArena-Text-Creative-Writing": 1383,
        "LMArean-Text-Math": 1455,
        "LMArena-Text-Coding": 1456,
        "LMArena-Text-Expert": 1444,
        "LMArena-Text-Hard-Prompts": 1440,
        "LMArena-Text-Longer-Query": 1410,
        "LMArena-Text-Multi-Turn": 1420,
        "LMArena-Vision": 1217
      },
      "metadata": {
        "lmarenaRank": 21,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 332.229,
          "timeToFirstToken": 9.185
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "qwen3-max-preview",
      "name": "Qwen3 Max (Preview)",
      "provider": "Alibaba",
      "releaseDate": "2025-09-05",
      "benchmarks": {
        "MMLU Pro": 83.8,
        "GPQA Diamond": 76.4,
        "Humanity's Last Exam": 9.3,
        "AA-LCR": 39.7,
        "LiveCodeBench": 65.10000000000001,
        "SciCode": 37,
        "AIME 2025": 75,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1433,
        "LMArena-Text-Creative-Writing": 1396,
        "LMArean-Text-Math": 1442,
        "LMArena-Text-Coding": 1481,
        "LMArena-Text-Expert": 1468,
        "LMArena-Text-Hard-Prompts": 1457,
        "LMArena-Text-Longer-Query": 1448,
        "LMArena-Text-Multi-Turn": 1445,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 22,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.2,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 41.762,
          "timeToFirstToken": 1.737
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-4-1-fast-reasoning",
      "name": "Grok 4.1 Fast (Reasoning)",
      "provider": "xAI",
      "releaseDate": "2025-11-19",
      "benchmarks": {
        "MMLU Pro": 85.39999999999999,
        "GPQA Diamond": 85.3,
        "Humanity's Last Exam": 17.599999999999998,
        "AA-LCR": 68,
        "LiveCodeBench": 82.19999999999999,
        "SciCode": 44.2,
        "AIME 2025": 89.3,
        "MMMU Pro": 63,
        "AA-Omniscience": -31,
        "AA-Omniscience Accuracy": 23.5,
        "AA-Omniscience Hallucination Rate": 71.74,
        "LMArena-Text": 1430,
        "LMArena-Text-Creative-Writing": 1408,
        "LMArean-Text-Math": 1406,
        "LMArena-Text-Coding": 1455,
        "LMArena-Text-Expert": 1443,
        "LMArena-Text-Hard-Prompts": 1438,
        "LMArena-Text-Longer-Query": 1421,
        "LMArena-Text-Multi-Turn": 1413,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 23,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 167.236,
          "timeToFirstToken": 6.232
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-chatgpt",
      "name": "GPT-5 (ChatGPT)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 82,
        "GPQA Diamond": 68.60000000000001,
        "Humanity's Last Exam": 5.800000000000001,
        "AA-LCR": 63.7,
        "LiveCodeBench": 54.300000000000004,
        "SciCode": 37.8,
        "AIME 2025": 48.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1425,
        "LMArena-Text-Creative-Writing": 1388,
        "LMArean-Text-Math": 1415,
        "LMArena-Text-Coding": 1459,
        "LMArena-Text-Expert": 1443,
        "LMArena-Text-Hard-Prompts": 1446,
        "LMArena-Text-Longer-Query": 1436,
        "LMArena-Text-Multi-Turn": 1446,
        "LMArena-Vision": 1223
      },
      "metadata": {
        "lmarenaRank": 26,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 179.698,
          "timeToFirstToken": 0.673
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-6",
      "name": "GLM-4.6 (Non-reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-09-30",
      "benchmarks": {
        "MMLU Pro": 78.4,
        "GPQA Diamond": 63.2,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 26.3,
        "LiveCodeBench": 56.10000000000001,
        "SciCode": 33.1,
        "AIME 2025": 44.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -33,
        "AA-Omniscience Accuracy": 20.27,
        "AA-Omniscience Hallucination Rate": 67.12,
        "LMArena-Text": 1424,
        "LMArena-Text-Creative-Writing": 1400,
        "LMArean-Text-Math": 1428,
        "LMArena-Text-Coding": 1462,
        "LMArena-Text-Expert": 1442,
        "LMArena-Text-Hard-Prompts": 1442,
        "LMArena-Text-Longer-Query": 1434,
        "LMArena-Text-Multi-Turn": 1422,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 27,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 2.2
        },
        "performance": {
          "outputTokensPerSecond": 11.846,
          "timeToFirstToken": 0.605
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "qwen3-max",
      "name": "Qwen3 Max",
      "provider": "Alibaba",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 84.1,
        "GPQA Diamond": 76.4,
        "Humanity's Last Exam": 11.1,
        "AA-LCR": 46.7,
        "LiveCodeBench": 76.7,
        "SciCode": 38.3,
        "AIME 2025": 80.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -45,
        "AA-Omniscience Accuracy": 23.35,
        "AA-Omniscience Hallucination Rate": 89.04,
        "LMArena-Text": 1424,
        "LMArena-Text-Creative-Writing": 1395,
        "LMArean-Text-Math": 1436,
        "LMArena-Text-Coding": 1475,
        "LMArena-Text-Expert": 1426,
        "LMArena-Text-Hard-Prompts": 1446,
        "LMArena-Text-Longer-Query": 1435,
        "LMArena-Text-Multi-Turn": 1446,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 28,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.2,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 30.75,
          "timeToFirstToken": 1.632
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-2-0925",
      "name": "DeepSeek V3.2 Exp (Non-reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 73.8,
        "Humanity's Last Exam": 8.6,
        "AA-LCR": 43,
        "LiveCodeBench": 55.400000000000006,
        "SciCode": 39.900000000000006,
        "AIME 2025": 57.699999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1423,
        "LMArena-Text-Creative-Writing": 1401,
        "LMArean-Text-Math": 1419,
        "LMArena-Text-Coding": 1468,
        "LMArena-Text-Expert": 1426,
        "LMArena-Text-Hard-Prompts": 1446,
        "LMArena-Text-Longer-Query": 1442,
        "LMArena-Text-Multi-Turn": 1429,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 29,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 30.252,
          "timeToFirstToken": 1.275
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-opus-thinking",
      "name": "Claude 4 Opus (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-05-22",
      "benchmarks": {
        "MMLU Pro": 87.3,
        "GPQA Diamond": 79.60000000000001,
        "Humanity's Last Exam": 11.700000000000001,
        "AA-LCR": 33.7,
        "LiveCodeBench": 63.6,
        "SciCode": 39.800000000000004,
        "AIME 2025": 73.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1423,
        "LMArena-Text-Creative-Writing": 1421,
        "LMArean-Text-Math": 1421,
        "LMArena-Text-Coding": 1495,
        "LMArena-Text-Expert": 1442,
        "LMArena-Text-Hard-Prompts": 1452,
        "LMArena-Text-Longer-Query": 1461,
        "LMArena-Text-Multi-Turn": 1434,
        "LMArena-Vision": 1209
      },
      "metadata": {
        "lmarenaRank": 30,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 15,
          "output": 75
        },
        "performance": {
          "outputTokensPerSecond": 44.871,
          "timeToFirstToken": 1.263
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-2-reasoning",
      "name": "DeepSeek V3.2 (Reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-12-01",
      "benchmarks": {
        "MMLU Pro": 86.2,
        "GPQA Diamond": 84,
        "Humanity's Last Exam": 22.2,
        "AA-LCR": 65,
        "LiveCodeBench": 86.2,
        "SciCode": 38.9,
        "AIME 2025": 92,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1422,
        "LMArena-Text-Creative-Writing": 1408,
        "LMArean-Text-Math": 1425,
        "LMArena-Text-Coding": 1466,
        "LMArena-Text-Expert": 1433,
        "LMArena-Text-Hard-Prompts": 1438,
        "LMArena-Text-Longer-Query": 1436,
        "LMArena-Text-Multi-Turn": 1419,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 32,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 29.409,
          "timeToFirstToken": 1.33
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-235b-a22b-instruct-2507",
      "name": "Qwen3 235B A22B 2507 Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-07-21",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 75.3,
        "Humanity's Last Exam": 10.6,
        "AA-LCR": 31.2,
        "LiveCodeBench": 52.400000000000006,
        "SciCode": 36,
        "AIME 2025": 71.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -45,
        "AA-Omniscience Accuracy": 17.58,
        "AA-Omniscience Hallucination Rate": 76.4,
        "LMArena-Text": 1421,
        "LMArena-Text-Creative-Writing": 1379,
        "LMArean-Text-Math": 1429,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1449,
        "LMArena-Text-Hard-Prompts": 1449,
        "LMArena-Text-Longer-Query": 1439,
        "LMArena-Text-Multi-Turn": 1433,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 33,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 42.786,
          "timeToFirstToken": 1.005
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "grok-4-fast",
      "name": "Grok 4 Fast (Non-reasoning)",
      "provider": "xAI",
      "releaseDate": "2025-09-19",
      "benchmarks": {
        "MMLU Pro": 73,
        "GPQA Diamond": 60.6,
        "Humanity's Last Exam": 5,
        "AA-LCR": 20,
        "LiveCodeBench": 40.1,
        "SciCode": 32.9,
        "AIME 2025": 41.3,
        "MMMU Pro": 48,
        "AA-Omniscience": -56,
        "AA-Omniscience Accuracy": 16.48,
        "AA-Omniscience Hallucination Rate": 86.41,
        "LMArena-Text": 1420,
        "LMArena-Text-Creative-Writing": 1382,
        "LMArean-Text-Math": 1426,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1424,
        "LMArena-Text-Hard-Prompts": 1430,
        "LMArena-Text-Longer-Query": 1425,
        "LMArena-Text-Multi-Turn": 1426,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 34,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 142.721,
          "timeToFirstToken": 0.614
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-2",
      "name": "DeepSeek V3.2 (Non-reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-12-01",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 75.1,
        "Humanity's Last Exam": 10.5,
        "AA-LCR": 39,
        "LiveCodeBench": 59.3,
        "SciCode": 38.7,
        "AIME 2025": 59,
        "MMMU Pro": 0,
        "AA-Omniscience": -49,
        "AA-Omniscience Accuracy": 22.77,
        "AA-Omniscience Hallucination Rate": 92.51,
        "LMArena-Text": 1419,
        "LMArena-Text-Creative-Writing": 1395,
        "LMArean-Text-Math": 1425,
        "LMArena-Text-Coding": 1457,
        "LMArena-Text-Expert": 1429,
        "LMArena-Text-Hard-Prompts": 1443,
        "LMArena-Text-Longer-Query": 1438,
        "LMArena-Text-Multi-Turn": 1423,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 35,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 29.055,
          "timeToFirstToken": 1.318
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "kimi-k2-0905",
      "name": "Kimi K2 0905",
      "provider": "Kimi",
      "releaseDate": "2025-09-05",
      "benchmarks": {
        "MMLU Pro": 81.89999999999999,
        "GPQA Diamond": 76.7,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 52.300000000000004,
        "LiveCodeBench": 61,
        "SciCode": 30.7,
        "AIME 2025": 57.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -28,
        "AA-Omniscience Accuracy": 24.03,
        "AA-Omniscience Hallucination Rate": 68.96,
        "LMArena-Text": 1419,
        "LMArena-Text-Creative-Writing": 1381,
        "LMArean-Text-Math": 1417,
        "LMArena-Text-Coding": 1465,
        "LMArena-Text-Expert": 1419,
        "LMArena-Text-Hard-Prompts": 1434,
        "LMArena-Text-Longer-Query": 1404,
        "LMArena-Text-Multi-Turn": 1405,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 36,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.99,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 95.886,
          "timeToFirstToken": 0.521
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1",
      "name": "DeepSeek R1 0528 (May '25)",
      "provider": "DeepSeek",
      "releaseDate": "2025-05-28",
      "benchmarks": {
        "MMLU Pro": 84.89999999999999,
        "GPQA Diamond": 81.3,
        "Humanity's Last Exam": 14.899999999999999,
        "AA-LCR": 54.7,
        "LiveCodeBench": 77,
        "SciCode": 40.300000000000004,
        "AIME 2025": 76,
        "MMMU Pro": 0,
        "AA-Omniscience": -30,
        "AA-Omniscience Accuracy": 29.28,
        "AA-Omniscience Hallucination Rate": 83.36,
        "LMArena-Text": 1418,
        "LMArena-Text-Creative-Writing": 1389,
        "LMArean-Text-Math": 1398,
        "LMArena-Text-Coding": 1462,
        "LMArena-Text-Expert": 1412,
        "LMArena-Text-Hard-Prompts": 1432,
        "LMArena-Text-Longer-Query": 1409,
        "LMArena-Text-Multi-Turn": 1405,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 37,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.35,
          "output": 3
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": true
    },
    {
      "id": "mistral-large-3",
      "name": "Mistral Large 3",
      "provider": "Mistral",
      "releaseDate": "2025-12-02",
      "benchmarks": {
        "MMLU Pro": 80.7,
        "GPQA Diamond": 68,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 34.699999999999996,
        "LiveCodeBench": 46.5,
        "SciCode": 36.199999999999996,
        "AIME 2025": 38,
        "MMMU Pro": 56,
        "AA-Omniscience": -41,
        "AA-Omniscience Accuracy": 23.68,
        "AA-Omniscience Hallucination Rate": 84.73,
        "LMArena-Text": 1417,
        "LMArena-Text-Creative-Writing": 1368,
        "LMArean-Text-Math": 1422,
        "LMArena-Text-Coding": 1471,
        "LMArena-Text-Expert": 1409,
        "LMArena-Text-Hard-Prompts": 1440,
        "LMArena-Text-Longer-Query": 1422,
        "LMArena-Text-Multi-Turn": 1416,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 38,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "performance": {
          "outputTokensPerSecond": 47.946,
          "timeToFirstToken": 0.552
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-1",
      "name": "DeepSeek V3.1 (Non-reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-08-21",
      "benchmarks": {
        "MMLU Pro": 83.3,
        "GPQA Diamond": 73.5,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 45,
        "LiveCodeBench": 57.699999999999996,
        "SciCode": 36.7,
        "AIME 2025": 49.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1417,
        "LMArena-Text-Creative-Writing": 1393,
        "LMArean-Text-Math": 1418,
        "LMArena-Text-Coding": 1447,
        "LMArena-Text-Expert": 1430,
        "LMArena-Text-Hard-Prompts": 1431,
        "LMArena-Text-Longer-Query": 1425,
        "LMArena-Text-Multi-Turn": 1405,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 39,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.555,
          "output": 1.67
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-1-reasoning",
      "name": "DeepSeek V3.1 (Reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-08-21",
      "benchmarks": {
        "MMLU Pro": 85.1,
        "GPQA Diamond": 77.9,
        "Humanity's Last Exam": 13,
        "AA-LCR": 53.300000000000004,
        "LiveCodeBench": 78.4,
        "SciCode": 39.1,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1416,
        "LMArena-Text-Creative-Writing": 1404,
        "LMArean-Text-Math": 1422,
        "LMArena-Text-Coding": 1456,
        "LMArena-Text-Expert": 1430,
        "LMArena-Text-Hard-Prompts": 1435,
        "LMArena-Text-Longer-Query": 1447,
        "LMArena-Text-Multi-Turn": 1414,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 41,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.58,
          "output": 1.68
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-1-terminus",
      "name": "DeepSeek V3.1 Terminus (Non-reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 75.1,
        "Humanity's Last Exam": 8.4,
        "AA-LCR": 43.3,
        "LiveCodeBench": 52.900000000000006,
        "SciCode": 32.1,
        "AIME 2025": 53.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -45,
        "AA-Omniscience Accuracy": 22.62,
        "AA-Omniscience Hallucination Rate": 86.84,
        "LMArena-Text": 1415,
        "LMArena-Text-Creative-Writing": 1405,
        "LMArean-Text-Math": 1399,
        "LMArena-Text-Coding": 1437,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1422,
        "LMArena-Text-Longer-Query": 1416,
        "LMArena-Text-Multi-Turn": 1393,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 42,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 1.68
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-235b-a22b-instruct",
      "name": "Qwen3 VL 235B A22B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 82.3,
        "GPQA Diamond": 71.2,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 31.7,
        "LiveCodeBench": 59.4,
        "SciCode": 35.9,
        "AIME 2025": 70.7,
        "MMMU Pro": 68,
        "AA-Omniscience": -54,
        "AA-Omniscience Accuracy": 19.22,
        "AA-Omniscience Hallucination Rate": 90.47,
        "LMArena-Text": 1414,
        "LMArena-Text-Creative-Writing": 1363,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1465,
        "LMArena-Text-Expert": 1436,
        "LMArena-Text-Hard-Prompts": 1439,
        "LMArena-Text-Longer-Query": 1424,
        "LMArena-Text-Multi-Turn": 1423,
        "LMArena-Vision": 1209
      },
      "metadata": {
        "lmarenaRank": 43,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 35.503,
          "timeToFirstToken": 1.107
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-1-terminus-reasoning",
      "name": "DeepSeek V3.1 Terminus (Reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 85.1,
        "GPQA Diamond": 79.2,
        "Humanity's Last Exam": 15.2,
        "AA-LCR": 65,
        "LiveCodeBench": 79.80000000000001,
        "SciCode": 40.6,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1414,
        "LMArena-Text-Creative-Writing": 1377,
        "LMArean-Text-Math": 1415,
        "LMArena-Text-Coding": 1457,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1441,
        "LMArena-Text-Longer-Query": 1439,
        "LMArena-Text-Multi-Turn": 1417,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 44,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-4-1",
      "name": "GPT-4.1",
      "provider": "OpenAI",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 80.60000000000001,
        "GPQA Diamond": 66.60000000000001,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 61,
        "LiveCodeBench": 45.7,
        "SciCode": 38.1,
        "AIME 2025": 34.699999999999996,
        "MMMU Pro": 61,
        "AA-Omniscience": -42,
        "AA-Omniscience Accuracy": 26.08,
        "AA-Omniscience Hallucination Rate": 92.29,
        "LMArena-Text": 1412,
        "LMArena-Text-Creative-Writing": 1399,
        "LMArean-Text-Math": 1374,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1400,
        "LMArena-Text-Hard-Prompts": 1428,
        "LMArena-Text-Longer-Query": 1425,
        "LMArena-Text-Multi-Turn": 1427,
        "LMArena-Vision": 1215
      },
      "metadata": {
        "lmarenaRank": 45,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 116.474,
          "timeToFirstToken": 0.502
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-opus",
      "name": "Claude 4 Opus (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-05-22",
      "benchmarks": {
        "MMLU Pro": 86,
        "GPQA Diamond": 70.1,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 36,
        "LiveCodeBench": 54.2,
        "SciCode": 40.9,
        "AIME 2025": 36.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1412,
        "LMArena-Text-Creative-Writing": 1412,
        "LMArean-Text-Math": 1402,
        "LMArena-Text-Coding": 1461,
        "LMArena-Text-Expert": 1430,
        "LMArena-Text-Hard-Prompts": 1433,
        "LMArena-Text-Longer-Query": 1444,
        "LMArena-Text-Multi-Turn": 1424,
        "LMArena-Vision": 1188
      },
      "metadata": {
        "lmarenaRank": 46,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 15,
          "output": 75
        },
        "performance": {
          "outputTokensPerSecond": 39.904,
          "timeToFirstToken": 1.37
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-medium-3-1",
      "name": "Mistral Medium 3.1",
      "provider": "Mistral",
      "releaseDate": "2025-08-12",
      "benchmarks": {
        "MMLU Pro": 68.30000000000001,
        "GPQA Diamond": 58.8,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 19.7,
        "LiveCodeBench": 40.6,
        "SciCode": 33.800000000000004,
        "AIME 2025": 38.3,
        "MMMU Pro": 54,
        "AA-Omniscience": -48,
        "AA-Omniscience Accuracy": 18.95,
        "AA-Omniscience Hallucination Rate": 82.48,
        "LMArena-Text": 1411,
        "LMArena-Text-Creative-Writing": 1379,
        "LMArean-Text-Math": 1402,
        "LMArena-Text-Coding": 1453,
        "LMArena-Text-Expert": 1405,
        "LMArena-Text-Hard-Prompts": 1430,
        "LMArena-Text-Longer-Query": 1412,
        "LMArena-Text-Multi-Turn": 1413,
        "LMArena-Vision": 1149
      },
      "metadata": {
        "lmarenaRank": 47,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 85.251,
          "timeToFirstToken": 0.431
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-4",
      "name": "Grok 4",
      "provider": "xAI",
      "releaseDate": "2025-07-10",
      "benchmarks": {
        "MMLU Pro": 86.6,
        "GPQA Diamond": 87.7,
        "Humanity's Last Exam": 23.9,
        "AA-LCR": 68,
        "LiveCodeBench": 81.89999999999999,
        "SciCode": 45.7,
        "AIME 2025": 92.7,
        "MMMU Pro": 69,
        "AA-Omniscience": 1,
        "AA-Omniscience Accuracy": 39.57,
        "AA-Omniscience Hallucination Rate": 63.9,
        "LMArena-Text": 1409,
        "LMArena-Text-Creative-Writing": 1395,
        "LMArean-Text-Math": 1437,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1431,
        "LMArena-Text-Hard-Prompts": 1417,
        "LMArena-Text-Longer-Query": 1417,
        "LMArena-Text-Multi-Turn": 1413,
        "LMArena-Vision": 1180
      },
      "metadata": {
        "lmarenaRank": 49,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 51.51,
          "timeToFirstToken": 8.097
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "glm-4.5",
      "name": "GLM-4.5 (Reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-07-28",
      "benchmarks": {
        "MMLU Pro": 83.5,
        "GPQA Diamond": 78.2,
        "Humanity's Last Exam": 12.2,
        "AA-LCR": 48.3,
        "LiveCodeBench": 73.8,
        "SciCode": 34.8,
        "AIME 2025": 73.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -29,
        "AA-Omniscience Accuracy": 23.87,
        "AA-Omniscience Hallucination Rate": 69.46,
        "LMArena-Text": 1409,
        "LMArena-Text-Creative-Writing": 1375,
        "LMArean-Text-Math": 1420,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1440,
        "LMArena-Text-Hard-Prompts": 1431,
        "LMArena-Text-Longer-Query": 1422,
        "LMArena-Text-Multi-Turn": 1407,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 50,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 2.2
        },
        "performance": {
          "outputTokensPerSecond": 49.272,
          "timeToFirstToken": 0.679
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-reasoning",
      "name": "Gemini 2.5 Flash (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 83.2,
        "GPQA Diamond": 79,
        "Humanity's Last Exam": 11.1,
        "AA-LCR": 61.7,
        "LiveCodeBench": 69.5,
        "SciCode": 39.4,
        "AIME 2025": 73.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1408,
        "LMArena-Text-Creative-Writing": 1398,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1422,
        "LMArena-Text-Expert": 1428,
        "LMArena-Text-Hard-Prompts": 1415,
        "LMArena-Text-Longer-Query": 1422,
        "LMArena-Text-Multi-Turn": 1401,
        "LMArena-Vision": 1213
      },
      "metadata": {
        "lmarenaRank": 51,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 276.364,
          "timeToFirstToken": 15.493
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-preview-09-2025",
      "name": "Gemini 2.5 Flash Preview (Sep '25) (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-09-25",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 76.6,
        "Humanity's Last Exam": 7.8,
        "AA-LCR": 56.699999999999996,
        "LiveCodeBench": 62.5,
        "SciCode": 37.5,
        "AIME 2025": 56.699999999999996,
        "MMMU Pro": 70,
        "AA-Omniscience": -41,
        "AA-Omniscience Accuracy": 25.77,
        "AA-Omniscience Hallucination Rate": 90.37,
        "LMArena-Text": 1405,
        "LMArena-Text-Creative-Writing": 1382,
        "LMArean-Text-Math": 1422,
        "LMArena-Text-Coding": 1424,
        "LMArena-Text-Expert": 1442,
        "LMArena-Text-Hard-Prompts": 1418,
        "LMArena-Text-Longer-Query": 1418,
        "LMArena-Text-Multi-Turn": 1398,
        "LMArena-Vision": 1224
      },
      "metadata": {
        "lmarenaRank": 52,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 278.466,
          "timeToFirstToken": 0.342
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-5-haiku",
      "name": "Claude 4.5 Haiku (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-10-15",
      "benchmarks": {
        "MMLU Pro": 80,
        "GPQA Diamond": 64.60000000000001,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 43.7,
        "LiveCodeBench": 51.1,
        "SciCode": 34.4,
        "AIME 2025": 39,
        "MMMU Pro": 55,
        "AA-Omniscience": -8,
        "AA-Omniscience Accuracy": 13.42,
        "AA-Omniscience Hallucination Rate": 24.68,
        "LMArena-Text": 1402,
        "LMArena-Text-Creative-Writing": 1376,
        "LMArean-Text-Math": 1396,
        "LMArena-Text-Coding": 1475,
        "LMArena-Text-Expert": 1437,
        "LMArena-Text-Hard-Prompts": 1434,
        "LMArena-Text-Longer-Query": 1433,
        "LMArena-Text-Multi-Turn": 1419,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 53,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1,
          "output": 5
        },
        "performance": {
          "outputTokensPerSecond": 103.563,
          "timeToFirstToken": 0.403
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-4-fast-reasoning",
      "name": "Grok 4 Fast (Reasoning)",
      "provider": "xAI",
      "releaseDate": "2025-09-19",
      "benchmarks": {
        "MMLU Pro": 85,
        "GPQA Diamond": 84.7,
        "Humanity's Last Exam": 17,
        "AA-LCR": 64.7,
        "LiveCodeBench": 83.2,
        "SciCode": 44.2,
        "AIME 2025": 89.7,
        "MMMU Pro": 62,
        "AA-Omniscience": -31,
        "AA-Omniscience Accuracy": 22.03,
        "AA-Omniscience Hallucination Rate": 67.38,
        "LMArena-Text": 1402,
        "LMArena-Text-Creative-Writing": 1377,
        "LMArean-Text-Math": 1412,
        "LMArena-Text-Coding": 1438,
        "LMArena-Text-Expert": 1414,
        "LMArena-Text-Hard-Prompts": 1410,
        "LMArena-Text-Longer-Query": 1410,
        "LMArena-Text-Multi-Turn": 1404,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 54,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 158.624,
          "timeToFirstToken": 4.897
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "qwen3-next-80b-a3b-instruct",
      "name": "Qwen3 Next 80B A3B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-09-11",
      "benchmarks": {
        "MMLU Pro": 81.89999999999999,
        "GPQA Diamond": 73.8,
        "Humanity's Last Exam": 7.3,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 68.4,
        "SciCode": 30.7,
        "AIME 2025": 66.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -60,
        "AA-Omniscience Accuracy": 16.72,
        "AA-Omniscience Hallucination Rate": 92.7,
        "LMArena-Text": 1400,
        "LMArena-Text-Creative-Writing": 1318,
        "LMArean-Text-Math": 1426,
        "LMArena-Text-Coding": 1446,
        "LMArena-Text-Expert": 1399,
        "LMArena-Text-Hard-Prompts": 1421,
        "LMArena-Text-Longer-Query": 1395,
        "LMArena-Text-Multi-Turn": 1402,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 56,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 183.974,
          "timeToFirstToken": 1.064
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "claude-4-sonnet-thinking",
      "name": "Claude 4 Sonnet (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-05-22",
      "benchmarks": {
        "MMLU Pro": 84.2,
        "GPQA Diamond": 77.7,
        "Humanity's Last Exam": 9.6,
        "AA-LCR": 64.7,
        "LiveCodeBench": 65.5,
        "SciCode": 40,
        "AIME 2025": 74.3,
        "MMMU Pro": 62,
        "AA-Omniscience": -2,
        "AA-Omniscience Accuracy": 21.05,
        "AA-Omniscience Hallucination Rate": 28.9,
        "LMArena-Text": 1399,
        "LMArena-Text-Creative-Writing": 1392,
        "LMArean-Text-Math": 1406,
        "LMArena-Text-Coding": 1469,
        "LMArena-Text-Expert": 1432,
        "LMArena-Text-Hard-Prompts": 1429,
        "LMArena-Text-Longer-Query": 1439,
        "LMArena-Text-Multi-Turn": 1415,
        "LMArena-Vision": 1204
      },
      "metadata": {
        "lmarenaRank": 59,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 57.335,
          "timeToFirstToken": 0.819
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-235b-a22b-instruct-2507-reasoning",
      "name": "Qwen3 235B A22B 2507 (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-07-25",
      "benchmarks": {
        "MMLU Pro": 84.3,
        "GPQA Diamond": 79,
        "Humanity's Last Exam": 15,
        "AA-LCR": 67,
        "LiveCodeBench": 78.8,
        "SciCode": 42.4,
        "AIME 2025": 91,
        "MMMU Pro": 0,
        "AA-Omniscience": -48,
        "AA-Omniscience Accuracy": 22.12,
        "AA-Omniscience Hallucination Rate": 89.64,
        "LMArena-Text": 1397,
        "LMArena-Text-Creative-Writing": 1370,
        "LMArean-Text-Math": 1403,
        "LMArena-Text-Coding": 1441,
        "LMArena-Text-Expert": 1460,
        "LMArena-Text-Hard-Prompts": 1417,
        "LMArena-Text-Longer-Query": 1404,
        "LMArena-Text-Multi-Turn": 1394,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 60,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 66.373,
          "timeToFirstToken": 1.1
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-0120",
      "name": "DeepSeek R1 (Jan '25)",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 84.39999999999999,
        "GPQA Diamond": 70.8,
        "Humanity's Last Exam": 9.3,
        "AA-LCR": 52.300000000000004,
        "LiveCodeBench": 61.7,
        "SciCode": 35.699999999999996,
        "AIME 2025": 68,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1396,
        "LMArena-Text-Creative-Writing": 1373,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1443,
        "LMArena-Text-Expert": 1395,
        "LMArena-Text-Hard-Prompts": 1415,
        "LMArena-Text-Longer-Query": 1398,
        "LMArena-Text-Multi-Turn": 1406,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 61,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.35,
          "output": 4
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "qwen3-vl-235b-a22b-reasoning",
      "name": "Qwen3 VL 235B A22B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 77.2,
        "Humanity's Last Exam": 10.100000000000001,
        "AA-LCR": 58.699999999999996,
        "LiveCodeBench": 64.60000000000001,
        "SciCode": 39.900000000000006,
        "AIME 2025": 88.3,
        "MMMU Pro": 69,
        "AA-Omniscience": -47,
        "AA-Omniscience Accuracy": 20.45,
        "AA-Omniscience Hallucination Rate": 84.24,
        "LMArena-Text": 1394,
        "LMArena-Text-Creative-Writing": 1342,
        "LMArean-Text-Math": 1414,
        "LMArena-Text-Coding": 1452,
        "LMArena-Text-Expert": 1436,
        "LMArena-Text-Hard-Prompts": 1416,
        "LMArena-Text-Longer-Query": 1406,
        "LMArena-Text-Multi-Turn": 1382,
        "LMArena-Vision": 1190
      },
      "metadata": {
        "lmarenaRank": 62,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 42.648,
          "timeToFirstToken": 1.053
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-0324",
      "name": "DeepSeek V3 0324",
      "provider": "DeepSeek",
      "releaseDate": "2025-03-25",
      "benchmarks": {
        "MMLU Pro": 81.89999999999999,
        "GPQA Diamond": 65.5,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 41,
        "LiveCodeBench": 40.5,
        "SciCode": 35.8,
        "AIME 2025": 41,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1392,
        "LMArena-Text-Creative-Writing": 1389,
        "LMArean-Text-Math": 1375,
        "LMArena-Text-Coding": 1425,
        "LMArena-Text-Expert": 1390,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1393,
        "LMArena-Text-Multi-Turn": 1406,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 63,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.14,
          "output": 1.25
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-mini",
      "name": "GPT-5 mini (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 82.8,
        "Humanity's Last Exam": 19.7,
        "AA-LCR": 68,
        "LiveCodeBench": 83.8,
        "SciCode": 39.2,
        "AIME 2025": 90.7,
        "MMMU Pro": 70,
        "AA-Omniscience": -20,
        "AA-Omniscience Accuracy": 22.97,
        "AA-Omniscience Hallucination Rate": 55.28,
        "LMArena-Text": 1392,
        "LMArena-Text-Creative-Writing": 1329,
        "LMArean-Text-Math": 1414,
        "LMArena-Text-Coding": 1429,
        "LMArena-Text-Expert": 1404,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1375,
        "LMArena-Text-Multi-Turn": 1375,
        "LMArena-Vision": 1184
      },
      "metadata": {
        "lmarenaRank": 64,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.25,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 67.815,
          "timeToFirstToken": 118.037
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "o4-mini",
      "name": "o4-mini (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-04-16",
      "benchmarks": {
        "MMLU Pro": 83.2,
        "GPQA Diamond": 78.4,
        "Humanity's Last Exam": 17.5,
        "AA-LCR": 55.00000000000001,
        "LiveCodeBench": 85.9,
        "SciCode": 46.5,
        "AIME 2025": 90.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1391,
        "LMArena-Text-Creative-Writing": 1339,
        "LMArean-Text-Math": 1419,
        "LMArena-Text-Coding": 1429,
        "LMArena-Text-Expert": 1406,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1368,
        "LMArena-Text-Multi-Turn": 1381,
        "LMArena-Vision": 1202
      },
      "metadata": {
        "lmarenaRank": 66,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 144.666,
          "timeToFirstToken": 48.906
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-sonnet",
      "name": "Claude 4 Sonnet (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-05-22",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 68.30000000000001,
        "Humanity's Last Exam": 4,
        "AA-LCR": 44.3,
        "LiveCodeBench": 44.9,
        "SciCode": 37.3,
        "AIME 2025": 38,
        "MMMU Pro": 62,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 21.45,
        "AA-Omniscience Hallucination Rate": 40.5,
        "LMArena-Text": 1389,
        "LMArena-Text-Creative-Writing": 1380,
        "LMArean-Text-Math": 1389,
        "LMArena-Text-Coding": 1446,
        "LMArena-Text-Expert": 1397,
        "LMArena-Text-Hard-Prompts": 1415,
        "LMArena-Text-Longer-Query": 1423,
        "LMArena-Text-Multi-Turn": 1403,
        "LMArena-Vision": 1186
      },
      "metadata": {
        "lmarenaRank": 68,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 58.702,
          "timeToFirstToken": 1.079
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mimo-v2-flash-reasoning",
      "name": "MiMo-V2-Flash (Reasoning)",
      "provider": "Xiaomi",
      "releaseDate": "2025-12-16",
      "benchmarks": {
        "MMLU Pro": 84.3,
        "GPQA Diamond": 84.6,
        "Humanity's Last Exam": 21.099999999999998,
        "AA-LCR": 63,
        "LiveCodeBench": 86.8,
        "SciCode": 39.4,
        "AIME 2025": 96.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1388,
        "LMArena-Text-Creative-Writing": 1352,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 1423,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1414,
        "LMArena-Text-Longer-Query": 1403,
        "LMArena-Text-Multi-Turn": 1398,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 69,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 153.38,
          "timeToFirstToken": 1.424
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-3-7-sonnet-thinking",
      "name": "Claude 3.7 Sonnet (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-02-24",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 77.2,
        "Humanity's Last Exam": 10.299999999999999,
        "AA-LCR": 60.699999999999996,
        "LiveCodeBench": 47.3,
        "SciCode": 40.300000000000004,
        "AIME 2025": 56.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1387,
        "LMArena-Text-Creative-Writing": 1388,
        "LMArean-Text-Math": 1386,
        "LMArena-Text-Coding": 1449,
        "LMArena-Text-Expert": 1406,
        "LMArena-Text-Hard-Prompts": 1414,
        "LMArena-Text-Longer-Query": 1425,
        "LMArena-Text-Multi-Turn": 1395,
        "LMArena-Vision": 1195
      },
      "metadata": {
        "lmarenaRank": 71,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-coder-480b-a35b-instruct",
      "name": "Qwen3 Coder 480B A35B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-07-22",
      "benchmarks": {
        "MMLU Pro": 78.8,
        "GPQA Diamond": 61.8,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 42.3,
        "LiveCodeBench": 58.5,
        "SciCode": 35.9,
        "AIME 2025": 39.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1386,
        "LMArena-Text-Creative-Writing": 1362,
        "LMArean-Text-Math": 1380,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1379,
        "LMArena-Text-Hard-Prompts": 1412,
        "LMArena-Text-Longer-Query": 1406,
        "LMArena-Text-Multi-Turn": 1394,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 72,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.5,
          "output": 7.5
        },
        "performance": {
          "outputTokensPerSecond": 53.713,
          "timeToFirstToken": 1.502
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-30b-a3b-2507",
      "name": "Qwen3 30B A3B 2507 Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-07-29",
      "benchmarks": {
        "MMLU Pro": 77.7,
        "GPQA Diamond": 65.9,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 22.7,
        "LiveCodeBench": 51.5,
        "SciCode": 30.4,
        "AIME 2025": 66.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -67,
        "AA-Omniscience Accuracy": 14.3,
        "AA-Omniscience Hallucination Rate": 94.63,
        "LMArena-Text": 1382,
        "LMArena-Text-Creative-Writing": 1324,
        "LMArean-Text-Math": 1386,
        "LMArena-Text-Coding": 1438,
        "LMArena-Text-Expert": 1385,
        "LMArena-Text-Hard-Prompts": 1406,
        "LMArena-Text-Longer-Query": 1383,
        "LMArena-Text-Multi-Turn": 1382,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 75,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "performance": {
          "outputTokensPerSecond": 61.307,
          "timeToFirstToken": 0.979
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "gpt-4-1-mini",
      "name": "GPT-4.1 mini",
      "provider": "OpenAI",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 78.10000000000001,
        "GPQA Diamond": 66.4,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 42.3,
        "LiveCodeBench": 48.3,
        "SciCode": 40.400000000000006,
        "AIME 2025": 46.300000000000004,
        "MMMU Pro": 59,
        "AA-Omniscience": -56,
        "AA-Omniscience Accuracy": 18.73,
        "AA-Omniscience Hallucination Rate": 91.59,
        "LMArena-Text": 1381,
        "LMArena-Text-Creative-Writing": 1345,
        "LMArean-Text-Math": 1357,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1380,
        "LMArena-Text-Hard-Prompts": 1399,
        "LMArena-Text-Longer-Query": 1388,
        "LMArena-Text-Multi-Turn": 1389,
        "LMArena-Vision": 1202
      },
      "metadata": {
        "lmarenaRank": 76,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "performance": {
          "outputTokensPerSecond": 78.339,
          "timeToFirstToken": 0.497
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-lite-preview-09-2025",
      "name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-09-25",
      "benchmarks": {
        "MMLU Pro": 79.60000000000001,
        "GPQA Diamond": 65.10000000000001,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 48,
        "LiveCodeBench": 64.1,
        "SciCode": 28.499999999999996,
        "AIME 2025": 46.7,
        "MMMU Pro": 63,
        "AA-Omniscience": -44,
        "AA-Omniscience Accuracy": 13.35,
        "AA-Omniscience Hallucination Rate": 65.86,
        "LMArena-Text": 1378,
        "LMArena-Text-Creative-Writing": 1360,
        "LMArean-Text-Math": 1370,
        "LMArena-Text-Coding": 1393,
        "LMArena-Text-Expert": 1383,
        "LMArena-Text-Hard-Prompts": 1389,
        "LMArena-Text-Longer-Query": 1388,
        "LMArena-Text-Multi-Turn": 1369,
        "LMArena-Vision": 1178
      },
      "metadata": {
        "lmarenaRank": 78,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 462.298,
          "timeToFirstToken": 0.297
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-lite-reasoning",
      "name": "Gemini 2.5 Flash-Lite (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-06-17",
      "benchmarks": {
        "MMLU Pro": 75.9,
        "GPQA Diamond": 62.5,
        "Humanity's Last Exam": 6.4,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 59.3,
        "SciCode": 19.3,
        "AIME 2025": 53.300000000000004,
        "MMMU Pro": 58,
        "AA-Omniscience": -47,
        "AA-Omniscience Accuracy": 17.13,
        "AA-Omniscience Hallucination Rate": 77.37,
        "LMArena-Text": 1375,
        "LMArena-Text-Creative-Writing": 1370,
        "LMArean-Text-Math": 1372,
        "LMArena-Text-Coding": 1384,
        "LMArena-Text-Expert": 1366,
        "LMArena-Text-Hard-Prompts": 1381,
        "LMArena-Text-Longer-Query": 1383,
        "LMArena-Text-Multi-Turn": 1367,
        "LMArena-Vision": 1188
      },
      "metadata": {
        "lmarenaRank": 79,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 304.615,
          "timeToFirstToken": 18.901
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-235b-a22b-instruct-reasoning",
      "name": "Qwen3 235B A22B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 70,
        "Humanity's Last Exam": 11.700000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 62.2,
        "SciCode": 39.900000000000006,
        "AIME 2025": 82,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1374,
        "LMArena-Text-Creative-Writing": 1329,
        "LMArean-Text-Math": 1401,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1381,
        "LMArena-Text-Hard-Prompts": 1390,
        "LMArena-Text-Longer-Query": 1381,
        "LMArena-Text-Multi-Turn": 1369,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 80,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 61.414,
          "timeToFirstToken": 1.059
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen-2-5-max",
      "name": "Qwen2.5 Max",
      "provider": "Alibaba",
      "releaseDate": "2025-01-28",
      "benchmarks": {
        "MMLU Pro": 76.2,
        "GPQA Diamond": 58.699999999999996,
        "Humanity's Last Exam": 4.5,
        "AA-LCR": 0,
        "LiveCodeBench": 35.9,
        "SciCode": 33.7,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1373,
        "LMArena-Text-Creative-Writing": 1353,
        "LMArean-Text-Math": 1368,
        "LMArena-Text-Coding": 1400,
        "LMArena-Text-Expert": 1364,
        "LMArena-Text-Hard-Prompts": 1382,
        "LMArena-Text-Longer-Query": 1384,
        "LMArena-Text-Multi-Turn": 1369,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 81,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.6,
          "output": 6.4
        },
        "performance": {
          "outputTokensPerSecond": 30.905,
          "timeToFirstToken": 1.153
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-3-7-sonnet",
      "name": "Claude 3.7 Sonnet (Non-reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-02-24",
      "benchmarks": {
        "MMLU Pro": 80.30000000000001,
        "GPQA Diamond": 65.60000000000001,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 48.3,
        "LiveCodeBench": 39.4,
        "SciCode": 37.6,
        "AIME 2025": 21,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1371,
        "LMArena-Text-Creative-Writing": 1373,
        "LMArean-Text-Math": 1361,
        "LMArena-Text-Coding": 1427,
        "LMArena-Text-Expert": 1382,
        "LMArena-Text-Hard-Prompts": 1395,
        "LMArena-Text-Longer-Query": 1410,
        "LMArena-Text-Multi-Turn": 1394,
        "LMArena-Vision": 1177
      },
      "metadata": {
        "lmarenaRank": 83,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-5-air",
      "name": "GLM-4.5-Air",
      "provider": "Z AI",
      "releaseDate": "2025-07-28",
      "benchmarks": {
        "MMLU Pro": 81.5,
        "GPQA Diamond": 73.3,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 43.7,
        "LiveCodeBench": 68.4,
        "SciCode": 30.599999999999998,
        "AIME 2025": 80.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -63,
        "AA-Omniscience Accuracy": 15.05,
        "AA-Omniscience Hallucination Rate": 92.05,
        "LMArena-Text": 1370,
        "LMArena-Text-Creative-Writing": 1327,
        "LMArean-Text-Math": 1396,
        "LMArena-Text-Coding": 1426,
        "LMArena-Text-Expert": 1388,
        "LMArena-Text-Hard-Prompts": 1389,
        "LMArena-Text-Longer-Query": 1381,
        "LMArena-Text-Multi-Turn": 1369,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 84,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 1.1
        },
        "performance": {
          "outputTokensPerSecond": 106.747,
          "timeToFirstToken": 0.635
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": true
    },
    {
      "id": "qwen3-next-80b-a3b-reasoning",
      "name": "Qwen3 Next 80B A3B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-09-11",
      "benchmarks": {
        "MMLU Pro": 82.39999999999999,
        "GPQA Diamond": 75.9,
        "Humanity's Last Exam": 11.700000000000001,
        "AA-LCR": 60.3,
        "LiveCodeBench": 78.4,
        "SciCode": 38.800000000000004,
        "AIME 2025": 84.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -53,
        "AA-Omniscience Accuracy": 18.22,
        "AA-Omniscience Hallucination Rate": 86.81,
        "LMArena-Text": 1367,
        "LMArena-Text-Creative-Writing": 1321,
        "LMArean-Text-Math": 1400,
        "LMArena-Text-Coding": 1423,
        "LMArena-Text-Expert": 1385,
        "LMArena-Text-Hard-Prompts": 1383,
        "LMArena-Text-Longer-Query": 1370,
        "LMArena-Text-Multi-Turn": 1345,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 85,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 182.961,
          "timeToFirstToken": 1.013
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "gemma-3-27b",
      "name": "Gemma 3 27B Instruct",
      "provider": "Google",
      "releaseDate": "2025-03-12",
      "benchmarks": {
        "MMLU Pro": 66.9,
        "GPQA Diamond": 42.8,
        "Humanity's Last Exam": 4.7,
        "AA-LCR": 5.7,
        "LiveCodeBench": 13.700000000000001,
        "SciCode": 21.2,
        "AIME 2025": 20.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1364,
        "LMArena-Text-Creative-Writing": 1347,
        "LMArean-Text-Math": 1325,
        "LMArena-Text-Coding": 1359,
        "LMArena-Text-Expert": 1332,
        "LMArena-Text-Hard-Prompts": 1363,
        "LMArena-Text-Longer-Query": 1365,
        "LMArena-Text-Multi-Turn": 1358,
        "LMArena-Vision": 1155
      },
      "metadata": {
        "lmarenaRank": 88,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 46.718,
          "timeToFirstToken": 2.183
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "o3-mini-high",
      "name": "o3-mini (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-01-31",
      "benchmarks": {
        "MMLU Pro": 80.2,
        "GPQA Diamond": 77.3,
        "Humanity's Last Exam": 12.3,
        "AA-LCR": 0,
        "LiveCodeBench": 73.4,
        "SciCode": 39.800000000000004,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1362,
        "LMArena-Text-Creative-Writing": 1310,
        "LMArean-Text-Math": 1409,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1394,
        "LMArena-Text-Hard-Prompts": 1398,
        "LMArena-Text-Longer-Query": 1371,
        "LMArena-Text-Multi-Turn": 1343,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 89,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 169.523,
          "timeToFirstToken": 53.133
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-0-flash",
      "name": "Gemini 2.0 Flash (Feb '25)",
      "provider": "Google",
      "releaseDate": "2025-02-05",
      "benchmarks": {
        "MMLU Pro": 77.9,
        "GPQA Diamond": 62.3,
        "Humanity's Last Exam": 5.3,
        "AA-LCR": 28.299999999999997,
        "LiveCodeBench": 33.4,
        "SciCode": 33.300000000000004,
        "AIME 2025": 21.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1360,
        "LMArena-Text-Creative-Writing": 1346,
        "LMArean-Text-Math": 1358,
        "LMArena-Text-Coding": 1363,
        "LMArena-Text-Expert": 1351,
        "LMArena-Text-Hard-Prompts": 1359,
        "LMArena-Text-Longer-Query": 1365,
        "LMArena-Text-Multi-Turn": 1356,
        "LMArena-Vision": 1168
      },
      "metadata": {
        "lmarenaRank": 91,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-oss-120b",
      "name": "gpt-oss-120B (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 80.80000000000001,
        "GPQA Diamond": 78.2,
        "Humanity's Last Exam": 18.5,
        "AA-LCR": 50.7,
        "LiveCodeBench": 87.8,
        "SciCode": 38.9,
        "AIME 2025": 93.4,
        "MMMU Pro": 0,
        "AA-Omniscience": -52,
        "AA-Omniscience Accuracy": 20.02,
        "AA-Omniscience Hallucination Rate": 89.96,
        "LMArena-Text": 1353,
        "LMArena-Text-Creative-Writing": 1282,
        "LMArean-Text-Math": 1387,
        "LMArena-Text-Coding": 1389,
        "LMArena-Text-Expert": 1359,
        "LMArena-Text-Hard-Prompts": 1360,
        "LMArena-Text-Longer-Query": 1325,
        "LMArena-Text-Multi-Turn": 1323,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 96,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.15,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 378.261,
          "timeToFirstToken": 0.424
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-0-flash-lite-preview",
      "name": "Gemini 2.0 Flash-Lite (Preview)",
      "provider": "Google",
      "releaseDate": "2025-02-05",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 54.2,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 17.9,
        "SciCode": 24.7,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1352,
        "LMArena-Text-Creative-Writing": 1344,
        "LMArean-Text-Math": 1328,
        "LMArena-Text-Coding": 1342,
        "LMArena-Text-Expert": 1326,
        "LMArena-Text-Hard-Prompts": 1347,
        "LMArena-Text-Longer-Query": 1347,
        "LMArena-Text-Multi-Turn": 1326,
        "LMArena-Vision": 1133
      },
      "metadata": {
        "lmarenaRank": 97,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.075,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-5v-reasoning",
      "name": "GLM-4.5V (Reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-08-11",
      "benchmarks": {
        "MMLU Pro": 78.8,
        "GPQA Diamond": 68.4,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 60.4,
        "SciCode": 22.1,
        "AIME 2025": 73,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1352,
        "LMArena-Text-Creative-Writing": 1308,
        "LMArean-Text-Math": 1365,
        "LMArena-Text-Coding": 1403,
        "LMArena-Text-Expert": 1404,
        "LMArena-Text-Hard-Prompts": 1370,
        "LMArena-Text-Longer-Query": 1341,
        "LMArena-Text-Multi-Turn": 1355,
        "LMArena-Vision": 1157
      },
      "metadata": {
        "lmarenaRank": 98,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 1.8
        },
        "performance": {
          "outputTokensPerSecond": 63.551,
          "timeToFirstToken": 0.686
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "command-a",
      "name": "Command A",
      "provider": "Cohere",
      "releaseDate": "2025-03-13",
      "benchmarks": {
        "MMLU Pro": 71.2,
        "GPQA Diamond": 52.7,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 18,
        "LiveCodeBench": 28.7,
        "SciCode": 28.1,
        "AIME 2025": 13,
        "MMMU Pro": 0,
        "AA-Omniscience": -50,
        "AA-Omniscience Accuracy": 15.07,
        "AA-Omniscience Hallucination Rate": 76.12,
        "LMArena-Text": 1352,
        "LMArena-Text-Creative-Writing": 1334,
        "LMArean-Text-Math": 1313,
        "LMArena-Text-Coding": 1387,
        "LMArena-Text-Expert": 1336,
        "LMArena-Text-Hard-Prompts": 1365,
        "LMArena-Text-Longer-Query": 1368,
        "LMArena-Text-Multi-Turn": 1358,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 99,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2.5,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 104.223,
          "timeToFirstToken": 0.209
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "o3-mini",
      "name": "o3-mini",
      "provider": "OpenAI",
      "releaseDate": "2025-01-31",
      "benchmarks": {
        "MMLU Pro": 79.10000000000001,
        "GPQA Diamond": 74.8,
        "Humanity's Last Exam": 8.7,
        "AA-LCR": 0,
        "LiveCodeBench": 71.7,
        "SciCode": 39.900000000000006,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1348,
        "LMArena-Text-Creative-Writing": 1302,
        "LMArean-Text-Math": 1386,
        "LMArena-Text-Coding": 1413,
        "LMArena-Text-Expert": 1362,
        "LMArena-Text-Hard-Prompts": 1368,
        "LMArena-Text-Longer-Query": 1359,
        "LMArena-Text-Multi-Turn": 1337,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 101,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 163.438,
          "timeToFirstToken": 14.309
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "ling-flash-2-0",
      "name": "Ling-flash-2.0",
      "provider": "InclusionAI",
      "releaseDate": "2025-09-17",
      "benchmarks": {
        "MMLU Pro": 77.7,
        "GPQA Diamond": 65.7,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 15,
        "LiveCodeBench": 58.9,
        "SciCode": 28.9,
        "AIME 2025": 65.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -67,
        "AA-Omniscience Accuracy": 13.73,
        "AA-Omniscience Hallucination Rate": 94.11,
        "LMArena-Text": 1346,
        "LMArena-Text-Creative-Writing": 1272,
        "LMArean-Text-Math": 1358,
        "LMArena-Text-Coding": 1412,
        "LMArena-Text-Expert": 1359,
        "LMArena-Text-Hard-Prompts": 1365,
        "LMArena-Text-Longer-Query": 1329,
        "LMArena-Text-Multi-Turn": 1317,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 105,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.14,
          "output": 0.57
        },
        "performance": {
          "outputTokensPerSecond": 57.324,
          "timeToFirstToken": 1.511
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "minimax-m2",
      "name": "MiniMax-M2",
      "provider": "MiniMax",
      "releaseDate": "2025-10-26",
      "benchmarks": {
        "MMLU Pro": 82,
        "GPQA Diamond": 77.7,
        "Humanity's Last Exam": 12.5,
        "AA-LCR": 61,
        "LiveCodeBench": 82.6,
        "SciCode": 36.1,
        "AIME 2025": 78.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -50,
        "AA-Omniscience Accuracy": 20.83,
        "AA-Omniscience Hallucination Rate": 88.88,
        "LMArena-Text": 1346,
        "LMArena-Text-Creative-Writing": 1288,
        "LMArean-Text-Math": 1363,
        "LMArena-Text-Coding": 1376,
        "LMArena-Text-Expert": 1332,
        "LMArena-Text-Hard-Prompts": 1365,
        "LMArena-Text-Longer-Query": 1348,
        "LMArena-Text-Multi-Turn": 1363,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 107,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 1.2
        },
        "performance": {
          "outputTokensPerSecond": 65.181,
          "timeToFirstToken": 1.705
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "qwen3-32b-instruct",
      "name": "Qwen3 32B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 72.7,
        "GPQA Diamond": 53.5,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 0,
        "LiveCodeBench": 28.799999999999997,
        "SciCode": 28.000000000000004,
        "AIME 2025": 19.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1345,
        "LMArena-Text-Creative-Writing": 1304,
        "LMArean-Text-Math": 1403,
        "LMArena-Text-Coding": 1405,
        "LMArena-Text-Expert": 1395,
        "LMArena-Text-Hard-Prompts": 1363,
        "LMArena-Text-Longer-Query": 1356,
        "LMArena-Text-Multi-Turn": 1335,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 110,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 83.95,
          "timeToFirstToken": 0.909
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3-12b",
      "name": "Gemma 3 12B Instruct",
      "provider": "Google",
      "releaseDate": "2025-03-12",
      "benchmarks": {
        "MMLU Pro": 59.5,
        "GPQA Diamond": 34.9,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 6.7,
        "LiveCodeBench": 13.700000000000001,
        "SciCode": 17.4,
        "AIME 2025": 18.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1340,
        "LMArena-Text-Creative-Writing": 1331,
        "LMArean-Text-Math": 1322,
        "LMArena-Text-Coding": 1317,
        "LMArena-Text-Expert": 1269,
        "LMArena-Text-Hard-Prompts": 1330,
        "LMArena-Text-Longer-Query": 1349,
        "LMArena-Text-Multi-Turn": 1344,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 114,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 45.811,
          "timeToFirstToken": 4.453
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-nemotron-super-49b-v1-5",
      "name": "Llama Nemotron Super 49B v1.5 (Non-reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-07-25",
      "benchmarks": {
        "MMLU Pro": 69.19999999999999,
        "GPQA Diamond": 48.1,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 22,
        "LiveCodeBench": 28.999999999999996,
        "SciCode": 23.799999999999997,
        "AIME 2025": 8,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1340,
        "LMArena-Text-Creative-Writing": 1306,
        "LMArean-Text-Math": 1393,
        "LMArena-Text-Coding": 1395,
        "LMArena-Text-Expert": 1366,
        "LMArena-Text-Hard-Prompts": 1358,
        "LMArena-Text-Longer-Query": 1341,
        "LMArena-Text-Multi-Turn": 1335,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 115,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 71.366,
          "timeToFirstToken": 0.221
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-nano",
      "name": "GPT-5 nano (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 78,
        "GPQA Diamond": 67.60000000000001,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 41.699999999999996,
        "LiveCodeBench": 78.9,
        "SciCode": 36.6,
        "AIME 2025": 83.7,
        "MMMU Pro": 61,
        "AA-Omniscience": -30,
        "AA-Omniscience Accuracy": 18.28,
        "AA-Omniscience Hallucination Rate": 58.66,
        "LMArena-Text": 1338,
        "LMArena-Text-Creative-Writing": 1253,
        "LMArean-Text-Math": 1356,
        "LMArena-Text-Coding": 1380,
        "LMArena-Text-Expert": 1357,
        "LMArena-Text-Hard-Prompts": 1353,
        "LMArena-Text-Longer-Query": 1332,
        "LMArena-Text-Multi-Turn": 1329,
        "LMArena-Vision": 1146
      },
      "metadata": {
        "lmarenaRank": 117,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.05,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 138.404,
          "timeToFirstToken": 98.873
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwq-32b",
      "name": "QwQ 32B",
      "provider": "Alibaba",
      "releaseDate": "2025-03-05",
      "benchmarks": {
        "MMLU Pro": 76.4,
        "GPQA Diamond": 59.3,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 25,
        "LiveCodeBench": 63.1,
        "SciCode": 35.8,
        "AIME 2025": 28.999999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1334,
        "LMArena-Text-Creative-Writing": 1294,
        "LMArean-Text-Math": 1365,
        "LMArena-Text-Coding": 1384,
        "LMArena-Text-Expert": 1358,
        "LMArena-Text-Hard-Prompts": 1355,
        "LMArena-Text-Longer-Query": 1336,
        "LMArena-Text-Multi-Turn": 1319,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 122,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.43,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 31.774,
          "timeToFirstToken": 0.873
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-lite",
      "name": "Nova 2.0 Lite (Non-reasoning)",
      "provider": "Amazon",
      "releaseDate": "2025-10-29",
      "benchmarks": {
        "MMLU Pro": 74.3,
        "GPQA Diamond": 60.3,
        "Humanity's Last Exam": 3,
        "AA-LCR": 17.7,
        "LiveCodeBench": 34.599999999999994,
        "SciCode": 24,
        "AIME 2025": 33.7,
        "MMMU Pro": 49,
        "AA-Omniscience": -60,
        "AA-Omniscience Accuracy": 13.28,
        "AA-Omniscience Hallucination Rate": 85.07,
        "LMArena-Text": 1334,
        "LMArena-Text-Creative-Writing": 1268,
        "LMArean-Text-Math": 1345,
        "LMArena-Text-Coding": 1399,
        "LMArena-Text-Expert": 1360,
        "LMArena-Text-Hard-Prompts": 1367,
        "LMArena-Text-Longer-Query": 1346,
        "LMArena-Text-Multi-Turn": 1327,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 124,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 223.281,
          "timeToFirstToken": 0.525
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-4-maverick",
      "name": "Llama 4 Maverick",
      "provider": "Meta",
      "releaseDate": "2025-04-05",
      "benchmarks": {
        "MMLU Pro": 80.9,
        "GPQA Diamond": 67.10000000000001,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 46,
        "LiveCodeBench": 39.7,
        "SciCode": 33.1,
        "AIME 2025": 19.3,
        "MMMU Pro": 62,
        "AA-Omniscience": -43,
        "AA-Omniscience Accuracy": 23.52,
        "AA-Omniscience Hallucination Rate": 87.58,
        "LMArena-Text": 1327,
        "LMArena-Text-Creative-Writing": 1307,
        "LMArean-Text-Math": 1324,
        "LMArena-Text-Coding": 1371,
        "LMArena-Text-Expert": 1323,
        "LMArena-Text-Hard-Prompts": 1337,
        "LMArena-Text-Longer-Query": 1335,
        "LMArena-Text-Multi-Turn": 1323,
        "LMArena-Vision": 1146
      },
      "metadata": {
        "lmarenaRank": 128,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.27,
          "output": 0.85
        },
        "performance": {
          "outputTokensPerSecond": 132.082,
          "timeToFirstToken": 0.411
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-30b-a3b-instruct-reasoning",
      "name": "Qwen3 30B A3B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 77.7,
        "GPQA Diamond": 61.6,
        "Humanity's Last Exam": 6.6000000000000005,
        "AA-LCR": 0,
        "LiveCodeBench": 50.6,
        "SciCode": 28.499999999999996,
        "AIME 2025": 72.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -52,
        "AA-Omniscience Accuracy": 15.35,
        "AA-Omniscience Hallucination Rate": 79.96,
        "LMArena-Text": 1326,
        "LMArena-Text-Creative-Writing": 1285,
        "LMArean-Text-Math": 1360,
        "LMArena-Text-Coding": 1384,
        "LMArena-Text-Expert": 1343,
        "LMArena-Text-Hard-Prompts": 1343,
        "LMArena-Text-Longer-Query": 1340,
        "LMArena-Text-Multi-Turn": 1319,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 129,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 2.4
        },
        "performance": {
          "outputTokensPerSecond": 93.388,
          "timeToFirstToken": 0.985
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "llama-3-3-nemotron-super-49b",
      "name": "Llama 3.3 Nemotron Super 49B v1 (Non-reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-03-18",
      "benchmarks": {
        "MMLU Pro": 69.8,
        "GPQA Diamond": 51.7,
        "Humanity's Last Exam": 3.5000000000000004,
        "AA-LCR": 11.3,
        "LiveCodeBench": 28.000000000000004,
        "SciCode": 22.900000000000002,
        "AIME 2025": 7.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -51,
        "AA-Omniscience Accuracy": 12.92,
        "AA-Omniscience Hallucination Rate": 73.42,
        "LMArena-Text": 1326,
        "LMArena-Text-Creative-Writing": 1295,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 1362,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1358,
        "LMArena-Text-Longer-Query": 1334,
        "LMArena-Text-Multi-Turn": 1326,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 130,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-3-nano-30b-a3b",
      "name": "NVIDIA Nemotron 3 Nano 30B A3B (Non-reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-12-15",
      "benchmarks": {
        "MMLU Pro": 57.9,
        "GPQA Diamond": 39.900000000000006,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 6.7,
        "LiveCodeBench": 36,
        "SciCode": 23,
        "AIME 2025": 13.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -65,
        "AA-Omniscience Accuracy": 12.97,
        "AA-Omniscience Hallucination Rate": 89.81,
        "LMArena-Text": 1323,
        "LMArena-Text-Creative-Writing": 1255,
        "LMArean-Text-Math": 1372,
        "LMArena-Text-Coding": 1368,
        "LMArena-Text-Expert": 1338,
        "LMArena-Text-Hard-Prompts": 1336,
        "LMArena-Text-Longer-Query": 1308,
        "LMArena-Text-Multi-Turn": 1295,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 133,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.06,
          "output": 0.24
        },
        "performance": {
          "outputTokensPerSecond": 266.731,
          "timeToFirstToken": 0.212
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-4-scout",
      "name": "Llama 4 Scout",
      "provider": "Meta",
      "releaseDate": "2025-04-05",
      "benchmarks": {
        "MMLU Pro": 75.2,
        "GPQA Diamond": 58.699999999999996,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 25.8,
        "LiveCodeBench": 29.9,
        "SciCode": 17,
        "AIME 2025": 14.000000000000002,
        "MMMU Pro": 53,
        "AA-Omniscience": -53,
        "AA-Omniscience Accuracy": 14.35,
        "AA-Omniscience Hallucination Rate": 78.69,
        "LMArena-Text": 1322,
        "LMArena-Text-Creative-Writing": 1290,
        "LMArean-Text-Math": 1316,
        "LMArena-Text-Coding": 1361,
        "LMArena-Text-Expert": 1305,
        "LMArena-Text-Hard-Prompts": 1327,
        "LMArena-Text-Longer-Query": 1328,
        "LMArena-Text-Multi-Turn": 1320,
        "LMArena-Vision": 1125
      },
      "metadata": {
        "lmarenaRank": 135,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.14,
          "output": 0.545
        },
        "performance": {
          "outputTokensPerSecond": 116.856,
          "timeToFirstToken": 0.438
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-4-1-nano",
      "name": "GPT-4.1 nano",
      "provider": "OpenAI",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 65.7,
        "GPQA Diamond": 51.2,
        "Humanity's Last Exam": 3.9,
        "AA-LCR": 17,
        "LiveCodeBench": 32.6,
        "SciCode": 25.900000000000002,
        "AIME 2025": 24,
        "MMMU Pro": 40,
        "AA-Omniscience": -59,
        "AA-Omniscience Accuracy": 12.78,
        "AA-Omniscience Hallucination Rate": 82.25,
        "LMArena-Text": 1321,
        "LMArena-Text-Creative-Writing": 1306,
        "LMArean-Text-Math": 1278,
        "LMArena-Text-Coding": 1372,
        "LMArena-Text-Expert": 1310,
        "LMArena-Text-Hard-Prompts": 1330,
        "LMArena-Text-Longer-Query": 1326,
        "LMArena-Text-Multi-Turn": 1311,
        "LMArena-Vision": 1088
      },
      "metadata": {
        "lmarenaRank": 139,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 187.37,
          "timeToFirstToken": 0.38
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ring-flash-2-0",
      "name": "Ring-flash-2.0",
      "provider": "InclusionAI",
      "releaseDate": "2025-09-19",
      "benchmarks": {
        "MMLU Pro": 79.3,
        "GPQA Diamond": 72.5,
        "Humanity's Last Exam": 8.9,
        "AA-LCR": 21,
        "LiveCodeBench": 62.8,
        "SciCode": 16.8,
        "AIME 2025": 83.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 6.98,
        "AA-Omniscience Hallucination Rate": 38.7,
        "LMArena-Text": 1320,
        "LMArena-Text-Creative-Writing": 1258,
        "LMArean-Text-Math": 1353,
        "LMArena-Text-Coding": 1390,
        "LMArena-Text-Expert": 1359,
        "LMArena-Text-Hard-Prompts": 1350,
        "LMArena-Text-Longer-Query": 1332,
        "LMArena-Text-Multi-Turn": 1282,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 140,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.14,
          "output": 0.57
        },
        "performance": {
          "outputTokensPerSecond": 88.884,
          "timeToFirstToken": 1.404
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": true
    },
    {
      "id": "gemma-3n-e4b",
      "name": "Gemma 3n E4B Instruct",
      "provider": "Google",
      "releaseDate": "2025-06-26",
      "benchmarks": {
        "MMLU Pro": 48.8,
        "GPQA Diamond": 29.599999999999998,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 14.6,
        "SciCode": 8.1,
        "AIME 2025": 14.299999999999999,
        "MMMU Pro": 26,
        "AA-Omniscience": -82,
        "AA-Omniscience Accuracy": 7.42,
        "AA-Omniscience Hallucination Rate": 96.56,
        "LMArena-Text": 1318,
        "LMArena-Text-Creative-Writing": 1298,
        "LMArean-Text-Math": 1269,
        "LMArena-Text-Coding": 1311,
        "LMArena-Text-Expert": 1279,
        "LMArena-Text-Hard-Prompts": 1312,
        "LMArena-Text-Longer-Query": 1313,
        "LMArena-Text-Multi-Turn": 1292,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 143,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.02,
          "output": 0.04
        },
        "performance": {
          "outputTokensPerSecond": 57.025,
          "timeToFirstToken": 0.416
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-oss-20b",
      "name": "gpt-oss-20B (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 74.8,
        "GPQA Diamond": 68.8,
        "Humanity's Last Exam": 9.8,
        "AA-LCR": 30.7,
        "LiveCodeBench": 77.7,
        "SciCode": 34.4,
        "AIME 2025": 89.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -65,
        "AA-Omniscience Accuracy": 14.65,
        "AA-Omniscience Hallucination Rate": 93.2,
        "LMArena-Text": 1318,
        "LMArena-Text-Creative-Writing": 1239,
        "LMArean-Text-Math": 1338,
        "LMArena-Text-Coding": 1369,
        "LMArena-Text-Expert": 1317,
        "LMArena-Text-Hard-Prompts": 1321,
        "LMArena-Text-Longer-Query": 1305,
        "LMArena-Text-Multi-Turn": 1288,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 145,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.07,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 308.945,
          "timeToFirstToken": 0.512
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "olmo-3-32b-think",
      "name": "OLMo 3 32B Think",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-11-20",
      "benchmarks": {
        "MMLU Pro": 75.9,
        "GPQA Diamond": 61,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 67.2,
        "SciCode": 28.599999999999998,
        "AIME 2025": 73.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -60,
        "AA-Omniscience Accuracy": 13.98,
        "AA-Omniscience Hallucination Rate": 86.3,
        "LMArena-Text": 1304,
        "LMArena-Text-Creative-Writing": 1260,
        "LMArean-Text-Math": 1324,
        "LMArena-Text-Coding": 1361,
        "LMArena-Text-Expert": 1291,
        "LMArena-Text-Hard-Prompts": 1324,
        "LMArena-Text-Longer-Query": 1319,
        "LMArena-Text-Multi-Turn": 1296,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 161,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.35
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-small-3-1",
      "name": "Mistral Small 3.1",
      "provider": "Mistral",
      "releaseDate": "2025-03-17",
      "benchmarks": {
        "MMLU Pro": 65.9,
        "GPQA Diamond": 45.4,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 19.7,
        "LiveCodeBench": 21.2,
        "SciCode": 26.5,
        "AIME 2025": 3.6999999999999997,
        "MMMU Pro": 0,
        "AA-Omniscience": -52,
        "AA-Omniscience Accuracy": 14.32,
        "AA-Omniscience Hallucination Rate": 77.61,
        "LMArena-Text": 1303,
        "LMArena-Text-Creative-Writing": 1272,
        "LMArean-Text-Math": 1281,
        "LMArena-Text-Coding": 1359,
        "LMArena-Text-Expert": 1307,
        "LMArena-Text-Hard-Prompts": 1318,
        "LMArena-Text-Longer-Query": 1326,
        "LMArena-Text-Multi-Turn": 1291,
        "LMArena-Vision": 1125
      },
      "metadata": {
        "lmarenaRank": 162,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 110.382,
          "timeToFirstToken": 0.296
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3-4b",
      "name": "Gemma 3 4B Instruct",
      "provider": "Google",
      "releaseDate": "2025-03-12",
      "benchmarks": {
        "MMLU Pro": 41.699999999999996,
        "GPQA Diamond": 29.099999999999998,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 5.7,
        "LiveCodeBench": 11.200000000000001,
        "SciCode": 7.3,
        "AIME 2025": 12.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1302,
        "LMArena-Text-Creative-Writing": 1276,
        "LMArean-Text-Math": 1257,
        "LMArena-Text-Coding": 1274,
        "LMArena-Text-Expert": 1253,
        "LMArena-Text-Hard-Prompts": 1282,
        "LMArena-Text-Longer-Query": 1313,
        "LMArena-Text-Multi-Turn": 1271,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 163,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 43.829,
          "timeToFirstToken": 1.019
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ministral-8b",
      "name": "Ministral 8B (Dec '25)",
      "provider": "Mistral",
      "releaseDate": "2025-12-02",
      "benchmarks": {
        "MMLU Pro": 64.2,
        "GPQA Diamond": 47.099999999999994,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 24,
        "LiveCodeBench": 30.3,
        "SciCode": 20.8,
        "AIME 2025": 31.7,
        "MMMU Pro": 46,
        "AA-Omniscience": -70,
        "AA-Omniscience Accuracy": 11.82,
        "AA-Omniscience Hallucination Rate": 92.76,
        "LMArena-Text": 1237,
        "LMArena-Text-Creative-Writing": 1219,
        "LMArean-Text-Math": 1215,
        "LMArena-Text-Coding": 1271,
        "LMArena-Text-Expert": 1232,
        "LMArena-Text-Hard-Prompts": 1248,
        "LMArena-Text-Longer-Query": 1257,
        "LMArena-Text-Multi-Turn": 1205,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": 201,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.15,
          "output": 0.15
        },
        "performance": {
          "outputTokensPerSecond": 192.514,
          "timeToFirstToken": 0.28
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-oss-20b-low",
      "name": "gpt-oss-20B (low)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 71.8,
        "GPQA Diamond": 61.1,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 31,
        "LiveCodeBench": 65.2,
        "SciCode": 34,
        "AIME 2025": 62.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.07,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 273.191,
          "timeToFirstToken": 0.623
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-oss-120b-low",
      "name": "gpt-oss-120B (low)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 77.5,
        "GPQA Diamond": 67.2,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 43.7,
        "LiveCodeBench": 70.7,
        "SciCode": 36,
        "AIME 2025": 66.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.15,
          "output": 0.595
        },
        "performance": {
          "outputTokensPerSecond": 331.452,
          "timeToFirstToken": 0.457
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-1-codex",
      "name": "GPT-5.1 Codex (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 86,
        "GPQA Diamond": 86,
        "Humanity's Last Exam": 23.400000000000002,
        "AA-LCR": 67.30000000000001,
        "LiveCodeBench": 84.89999999999999,
        "SciCode": 40.2,
        "AIME 2025": 95.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 296.821,
          "timeToFirstToken": 11.149
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-1-codex-mini",
      "name": "GPT-5.1 Codex mini (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 82,
        "GPQA Diamond": 81.3,
        "Humanity's Last Exam": 16.900000000000002,
        "AA-LCR": 62.7,
        "LiveCodeBench": 83.6,
        "SciCode": 42.6,
        "AIME 2025": 91.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.25,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 172.304,
          "timeToFirstToken": 9.299
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-2-medium",
      "name": "GPT-5.2 (medium)",
      "provider": "OpenAI",
      "releaseDate": "2025-12-11",
      "benchmarks": {
        "MMLU Pro": 85.9,
        "GPQA Diamond": 86.4,
        "Humanity's Last Exam": 24.9,
        "AA-LCR": 63.3,
        "LiveCodeBench": 89.4,
        "SciCode": 46.2,
        "AIME 2025": 96.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.75,
          "output": 14
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3-1b",
      "name": "Gemma 3 1B Instruct",
      "provider": "Google",
      "releaseDate": "2025-03-13",
      "benchmarks": {
        "MMLU Pro": 13.5,
        "GPQA Diamond": 23.7,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 0,
        "LiveCodeBench": 1.7000000000000002,
        "SciCode": 0.7000000000000001,
        "AIME 2025": 3.3000000000000003,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 41.846,
          "timeToFirstToken": 0.549
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-3-pro-low",
      "name": "Gemini 3 Pro Preview (low)",
      "provider": "Google",
      "releaseDate": "2025-11-18",
      "benchmarks": {
        "MMLU Pro": 89.5,
        "GPQA Diamond": 88.7,
        "Humanity's Last Exam": 27.6,
        "AA-LCR": 67.30000000000001,
        "LiveCodeBench": 85.7,
        "SciCode": 49.9,
        "AIME 2025": 86.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2,
          "output": 12
        },
        "performance": {
          "outputTokensPerSecond": 135.29,
          "timeToFirstToken": 3.994
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3n-e2b",
      "name": "Gemma 3n E2B Instruct",
      "provider": "Google",
      "releaseDate": "2025-06-26",
      "benchmarks": {
        "MMLU Pro": 37.8,
        "GPQA Diamond": 22.900000000000002,
        "Humanity's Last Exam": 4,
        "AA-LCR": 0,
        "LiveCodeBench": 9.5,
        "SciCode": 5.2,
        "AIME 2025": 10.299999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 45.338,
          "timeToFirstToken": 0.285
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3-270m",
      "name": "Gemma 3 270M",
      "provider": "Google",
      "releaseDate": "2025-08-14",
      "benchmarks": {
        "MMLU Pro": 5.5,
        "GPQA Diamond": 22.400000000000002,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 0,
        "LiveCodeBench": 0.3,
        "SciCode": 0,
        "AIME 2025": 2.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-lite-preview-09-2025-reasoning",
      "name": "Gemini 2.5 Flash-Lite Preview (Sep '25) (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-09-08",
      "benchmarks": {
        "MMLU Pro": 80.80000000000001,
        "GPQA Diamond": 70.89999999999999,
        "Humanity's Last Exam": 6.6000000000000005,
        "AA-LCR": 59,
        "LiveCodeBench": 68.8,
        "SciCode": 28.7,
        "AIME 2025": 68.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 544.108,
          "timeToFirstToken": 6.675
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "claude-4-5-haiku-reasoning",
      "name": "Claude 4.5 Haiku (Reasoning)",
      "provider": "Anthropic",
      "releaseDate": "2025-10-15",
      "benchmarks": {
        "MMLU Pro": 76,
        "GPQA Diamond": 67.2,
        "Humanity's Last Exam": 9.700000000000001,
        "AA-LCR": 70.3,
        "LiveCodeBench": 61.5,
        "SciCode": 43.3,
        "AIME 2025": 83.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1,
          "output": 5
        },
        "performance": {
          "outputTokensPerSecond": 89.073,
          "timeToFirstToken": 0.396
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-small-3-2",
      "name": "Mistral Small 3.2",
      "provider": "Mistral",
      "releaseDate": "2025-06-20",
      "benchmarks": {
        "MMLU Pro": 68.10000000000001,
        "GPQA Diamond": 50.5,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 17.299999999999997,
        "LiveCodeBench": 27.500000000000004,
        "SciCode": 26.400000000000002,
        "AIME 2025": 27,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 117.608,
          "timeToFirstToken": 0.322
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ministral-3b",
      "name": "Ministral 3B (Dec '25)",
      "provider": "Mistral",
      "releaseDate": "2025-12-02",
      "benchmarks": {
        "MMLU Pro": 52.400000000000006,
        "GPQA Diamond": 35.8,
        "Humanity's Last Exam": 5.3,
        "AA-LCR": 11.700000000000001,
        "LiveCodeBench": 24.7,
        "SciCode": 14.399999999999999,
        "AIME 2025": 22,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.1
        },
        "performance": {
          "outputTokensPerSecond": 270.798,
          "timeToFirstToken": 0.265
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "devstral-2",
      "name": "Devstral 2",
      "provider": "Mistral",
      "releaseDate": "2025-12-09",
      "benchmarks": {
        "MMLU Pro": 76.2,
        "GPQA Diamond": 59.4,
        "Humanity's Last Exam": 3.5999999999999996,
        "AA-LCR": 30,
        "LiveCodeBench": 44.800000000000004,
        "SciCode": 33.1,
        "AIME 2025": 36.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 55.708,
          "timeToFirstToken": 0.434
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "devstral-small-2",
      "name": "Devstral Small 2",
      "provider": "Mistral",
      "releaseDate": "2025-12-09",
      "benchmarks": {
        "MMLU Pro": 67.80000000000001,
        "GPQA Diamond": 53.2,
        "Humanity's Last Exam": 3.4000000000000004,
        "AA-LCR": 24,
        "LiveCodeBench": 34.8,
        "SciCode": 28.799999999999997,
        "AIME 2025": 34.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 203.388,
          "timeToFirstToken": 0.36
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "magistral-small-2509",
      "name": "Magistral Small 1.2",
      "provider": "Mistral",
      "releaseDate": "2025-09-17",
      "benchmarks": {
        "MMLU Pro": 76.8,
        "GPQA Diamond": 66.3,
        "Humanity's Last Exam": 6.1,
        "AA-LCR": 16.3,
        "LiveCodeBench": 72.3,
        "SciCode": 35.199999999999996,
        "AIME 2025": 80.30000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "performance": {
          "outputTokensPerSecond": 212.872,
          "timeToFirstToken": 0.329
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "magistral-medium-2509",
      "name": "Magistral Medium 1.2",
      "provider": "Mistral",
      "releaseDate": "2025-09-18",
      "benchmarks": {
        "MMLU Pro": 81.5,
        "GPQA Diamond": 73.9,
        "Humanity's Last Exam": 9.6,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 75,
        "SciCode": 39.2,
        "AIME 2025": 82,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2,
          "output": 5
        },
        "performance": {
          "outputTokensPerSecond": 47.29,
          "timeToFirstToken": 0.499
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "devstral-medium",
      "name": "Devstral Medium",
      "provider": "Mistral",
      "releaseDate": "2025-07-10",
      "benchmarks": {
        "MMLU Pro": 70.8,
        "GPQA Diamond": 49.2,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 28.7,
        "LiveCodeBench": 33.7,
        "SciCode": 29.4,
        "AIME 2025": 4.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 112.863,
          "timeToFirstToken": 0.438
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ministral-14b",
      "name": "Ministral 14B (Dec '25)",
      "provider": "Mistral",
      "releaseDate": "2025-12-02",
      "benchmarks": {
        "MMLU Pro": 69.3,
        "GPQA Diamond": 57.199999999999996,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 22,
        "LiveCodeBench": 35.099999999999994,
        "SciCode": 23.599999999999998,
        "AIME 2025": 30,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 131.789,
          "timeToFirstToken": 0.318
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "devstral-small",
      "name": "Devstral Small (Jul '25)",
      "provider": "Mistral",
      "releaseDate": "2025-07-10",
      "benchmarks": {
        "MMLU Pro": 62.2,
        "GPQA Diamond": 41.4,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 17,
        "LiveCodeBench": 25.4,
        "SciCode": 24.3,
        "AIME 2025": 29.299999999999997,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 237.176,
          "timeToFirstToken": 0.363
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-distill-llama-70b",
      "name": "DeepSeek R1 Distill Llama 70B",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 79.5,
        "GPQA Diamond": 40.2,
        "Humanity's Last Exam": 6.1,
        "AA-LCR": 11,
        "LiveCodeBench": 26.6,
        "SciCode": 31.2,
        "AIME 2025": 53.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.8,
          "output": 1.2
        },
        "performance": {
          "outputTokensPerSecond": 86.984,
          "timeToFirstToken": 0.847
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-2-speciale",
      "name": "DeepSeek V3.2 Speciale",
      "provider": "DeepSeek",
      "releaseDate": "2025-12-01",
      "benchmarks": {
        "MMLU Pro": 86.3,
        "GPQA Diamond": 87.1,
        "Humanity's Last Exam": 26.1,
        "AA-LCR": 59.3,
        "LiveCodeBench": 89.60000000000001,
        "SciCode": 44,
        "AIME 2025": 96.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-qwen3-8b",
      "name": "DeepSeek R1 0528 Qwen3 8B",
      "provider": "DeepSeek",
      "releaseDate": "2025-05-29",
      "benchmarks": {
        "MMLU Pro": 73.9,
        "GPQA Diamond": 61.199999999999996,
        "Humanity's Last Exam": 5.6000000000000005,
        "AA-LCR": 13,
        "LiveCodeBench": 51.300000000000004,
        "SciCode": 20.4,
        "AIME 2025": 63.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.06,
          "output": 0.09
        },
        "performance": {
          "outputTokensPerSecond": 38.124,
          "timeToFirstToken": 1.219
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-code-fast-1",
      "name": "Grok Code Fast 1",
      "provider": "xAI",
      "releaseDate": "2025-08-28",
      "benchmarks": {
        "MMLU Pro": 79.3,
        "GPQA Diamond": 72.7,
        "Humanity's Last Exam": 7.5,
        "AA-LCR": 48.3,
        "LiveCodeBench": 65.7,
        "SciCode": 36.199999999999996,
        "AIME 2025": 43.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 1.5
        },
        "performance": {
          "outputTokensPerSecond": 254.394,
          "timeToFirstToken": 4.869
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-3-mini-reasoning",
      "name": "Grok 3 mini Reasoning (high)",
      "provider": "xAI",
      "releaseDate": "2025-02-19",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 79.10000000000001,
        "Humanity's Last Exam": 11.1,
        "AA-LCR": 50.3,
        "LiveCodeBench": 69.6,
        "SciCode": 40.6,
        "AIME 2025": 84.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 192.894,
          "timeToFirstToken": 0.694
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-4-1-fast",
      "name": "Grok 4.1 Fast (Non-reasoning)",
      "provider": "xAI",
      "releaseDate": "2025-11-19",
      "benchmarks": {
        "MMLU Pro": 74.3,
        "GPQA Diamond": 63.7,
        "Humanity's Last Exam": 5,
        "AA-LCR": 22,
        "LiveCodeBench": 39.900000000000006,
        "SciCode": 29.599999999999998,
        "AIME 2025": 34.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 139.148,
          "timeToFirstToken": 0.736
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-omni",
      "name": "Nova 2.0 Omni (Non-reasoning)",
      "provider": "Amazon",
      "releaseDate": "2025-11-26",
      "benchmarks": {
        "MMLU Pro": 71.89999999999999,
        "GPQA Diamond": 55.50000000000001,
        "Humanity's Last Exam": 3.9,
        "AA-LCR": 22.3,
        "LiveCodeBench": 30.5,
        "SciCode": 27.900000000000002,
        "AIME 2025": 37,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 232.024,
          "timeToFirstToken": 0.684
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-omni-reasoning-medium",
      "name": "Nova 2.0 Omni (medium)",
      "provider": "Amazon",
      "releaseDate": "2025-11-26",
      "benchmarks": {
        "MMLU Pro": 80.9,
        "GPQA Diamond": 76,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 53.7,
        "LiveCodeBench": 66,
        "SciCode": 36.199999999999996,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-omni-reasoning-low",
      "name": "Nova 2.0 Omni (low)",
      "provider": "Amazon",
      "releaseDate": "2025-11-26",
      "benchmarks": {
        "MMLU Pro": 79.80000000000001,
        "GPQA Diamond": 69.89999999999999,
        "Humanity's Last Exam": 4,
        "AA-LCR": 51,
        "LiveCodeBench": 59.199999999999996,
        "SciCode": 34.300000000000004,
        "AIME 2025": 56.00000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-pro-reasoning-low",
      "name": "Nova 2.0 Pro Preview (low)",
      "provider": "Amazon",
      "releaseDate": "2025-11-27",
      "benchmarks": {
        "MMLU Pro": 82.19999999999999,
        "GPQA Diamond": 75.1,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 61.7,
        "LiveCodeBench": 63.800000000000004,
        "SciCode": 38.7,
        "AIME 2025": 63.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 137.366,
          "timeToFirstToken": 11.391
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-pro",
      "name": "Nova 2.0 Pro Preview (Non-reasoning)",
      "provider": "Amazon",
      "releaseDate": "2025-11-27",
      "benchmarks": {
        "MMLU Pro": 77.2,
        "GPQA Diamond": 63.6,
        "Humanity's Last Exam": 4,
        "AA-LCR": 28.299999999999997,
        "LiveCodeBench": 47.3,
        "SciCode": 28.1,
        "AIME 2025": 30.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 158.587,
          "timeToFirstToken": 0.476
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-lite-reasoning-low",
      "name": "Nova 2.0 Lite (low)",
      "provider": "Amazon",
      "releaseDate": "2025-10-29",
      "benchmarks": {
        "MMLU Pro": 78.8,
        "GPQA Diamond": 69.8,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 52,
        "LiveCodeBench": 46.9,
        "SciCode": 33.300000000000004,
        "AIME 2025": 46.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 227.304,
          "timeToFirstToken": 4.889
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-pro-reasoning-medium",
      "name": "Nova 2.0 Pro Preview (medium)",
      "provider": "Amazon",
      "releaseDate": "2025-11-27",
      "benchmarks": {
        "MMLU Pro": 83,
        "GPQA Diamond": 78.5,
        "Humanity's Last Exam": 8.9,
        "AA-LCR": 54.300000000000004,
        "LiveCodeBench": 73,
        "SciCode": 42.699999999999996,
        "AIME 2025": 89,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 136.613,
          "timeToFirstToken": 18.963
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-2-0-lite-reasoning-medium",
      "name": "Nova 2.0 Lite (medium)",
      "provider": "Amazon",
      "releaseDate": "2025-10-29",
      "benchmarks": {
        "MMLU Pro": 81.3,
        "GPQA Diamond": 76.8,
        "Humanity's Last Exam": 8.6,
        "AA-LCR": 58.3,
        "LiveCodeBench": 66.3,
        "SciCode": 36.8,
        "AIME 2025": 88.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 244.53,
          "timeToFirstToken": 14.052
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nova-premier",
      "name": "Nova Premier",
      "provider": "Amazon",
      "releaseDate": "2025-04-30",
      "benchmarks": {
        "MMLU Pro": 73.3,
        "GPQA Diamond": 56.89999999999999,
        "Humanity's Last Exam": 4.7,
        "AA-LCR": 30,
        "LiveCodeBench": 31.7,
        "SciCode": 27.900000000000002,
        "AIME 2025": 17.299999999999997,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2.5,
          "output": 12.5
        },
        "performance": {
          "outputTokensPerSecond": 79.876,
          "timeToFirstToken": 0.806
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "phi-4-multimodal",
      "name": "Phi-4 Multimodal Instruct",
      "provider": "Microsoft Azure",
      "releaseDate": "2025-02-26",
      "benchmarks": {
        "MMLU Pro": 48.5,
        "GPQA Diamond": 31.5,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 13.100000000000001,
        "SciCode": 11,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 17.182,
          "timeToFirstToken": 0.325
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "lfm2-8b-a1b",
      "name": "LFM2 8B A1B",
      "provider": "Liquid AI",
      "releaseDate": "2025-10-07",
      "benchmarks": {
        "MMLU Pro": 50.5,
        "GPQA Diamond": 34.4,
        "Humanity's Last Exam": 4.9,
        "AA-LCR": 0,
        "LiveCodeBench": 15.1,
        "SciCode": 6.800000000000001,
        "AIME 2025": 25.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "lfm2-1-2b",
      "name": "LFM2 1.2B",
      "provider": "Liquid AI",
      "releaseDate": "2025-07-10",
      "benchmarks": {
        "MMLU Pro": 25.7,
        "GPQA Diamond": 22.8,
        "Humanity's Last Exam": 5.7,
        "AA-LCR": 0,
        "LiveCodeBench": 2,
        "SciCode": 2.5,
        "AIME 2025": 3.3000000000000003,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "lfm2-2-6b",
      "name": "LFM2 2.6B",
      "provider": "Liquid AI",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 29.799999999999997,
        "GPQA Diamond": 30.599999999999998,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 0,
        "LiveCodeBench": 8.1,
        "SciCode": 2.5,
        "AIME 2025": 8.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "solar-pro-2-reasoning",
      "name": "Solar Pro 2 (Reasoning)",
      "provider": "Upstage",
      "releaseDate": "2025-07-09",
      "benchmarks": {
        "MMLU Pro": 80.5,
        "GPQA Diamond": 68.7,
        "Humanity's Last Exam": 7.000000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 61.6,
        "SciCode": 30.2,
        "AIME 2025": 61.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 113.126,
          "timeToFirstToken": 0.982
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "solar-pro-2",
      "name": "Solar Pro 2 (Non-reasoning)",
      "provider": "Upstage",
      "releaseDate": "2025-07-09",
      "benchmarks": {
        "MMLU Pro": 75,
        "GPQA Diamond": 56.10000000000001,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 0,
        "LiveCodeBench": 42.4,
        "SciCode": 24.8,
        "AIME 2025": 30,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.5,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 114.432,
          "timeToFirstToken": 1.01
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "minimax-m2-1",
      "name": "MiniMax-M2.1",
      "provider": "MiniMax",
      "releaseDate": "2025-12-23",
      "benchmarks": {
        "MMLU Pro": 87.5,
        "GPQA Diamond": 83,
        "Humanity's Last Exam": 22.2,
        "AA-LCR": 59,
        "LiveCodeBench": 81,
        "SciCode": 40.699999999999996,
        "AIME 2025": 82.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 1.2
        },
        "performance": {
          "outputTokensPerSecond": 70.982,
          "timeToFirstToken": 1.399
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-nano-12b-v2-vl-reasoning",
      "name": "NVIDIA Nemotron Nano 12B v2 VL (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 75.9,
        "GPQA Diamond": 57.199999999999996,
        "Humanity's Last Exam": 5.3,
        "AA-LCR": 40,
        "LiveCodeBench": 69.39999999999999,
        "SciCode": 26.200000000000003,
        "AIME 2025": 75,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 126.896,
          "timeToFirstToken": 0.211
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-3-3-nemotron-super-49b-reasoning",
      "name": "Llama 3.3 Nemotron Super 49B v1 (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-03-18",
      "benchmarks": {
        "MMLU Pro": 78.5,
        "GPQA Diamond": 64.3,
        "Humanity's Last Exam": 6.5,
        "AA-LCR": 17,
        "LiveCodeBench": 27.700000000000003,
        "SciCode": 28.199999999999996,
        "AIME 2025": 54.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-3-1-nemotron-nano-4b-reasoning",
      "name": "Llama 3.1 Nemotron Nano 4B v1.1 (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 55.60000000000001,
        "GPQA Diamond": 40.8,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 0,
        "LiveCodeBench": 49.3,
        "SciCode": 10.100000000000001,
        "AIME 2025": 50,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-3-1-nemotron-ultra-253b-v1-reasoning",
      "name": "Llama 3.1 Nemotron Ultra 253B v1 (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-04-07",
      "benchmarks": {
        "MMLU Pro": 82.5,
        "GPQA Diamond": 72.8,
        "Humanity's Last Exam": 8.1,
        "AA-LCR": 7.3,
        "LiveCodeBench": 64.1,
        "SciCode": 34.699999999999996,
        "AIME 2025": 63.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 1.8
        },
        "performance": {
          "outputTokensPerSecond": 36.744,
          "timeToFirstToken": 0.734
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-nano-9b-v2",
      "name": "NVIDIA Nemotron Nano 9B V2 (Non-reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-08-18",
      "benchmarks": {
        "MMLU Pro": 73.9,
        "GPQA Diamond": 55.7,
        "Humanity's Last Exam": 4,
        "AA-LCR": 22.7,
        "LiveCodeBench": 70.1,
        "SciCode": 20.9,
        "AIME 2025": 62.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.06,
          "output": 0.23
        },
        "performance": {
          "outputTokensPerSecond": 73.347,
          "timeToFirstToken": 0.312
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-nano-9b-v2-reasoning",
      "name": "NVIDIA Nemotron Nano 9B V2 (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-08-18",
      "benchmarks": {
        "MMLU Pro": 74.2,
        "GPQA Diamond": 56.99999999999999,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 21,
        "LiveCodeBench": 72.39999999999999,
        "SciCode": 22,
        "AIME 2025": 69.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.04,
          "output": 0.16
        },
        "performance": {
          "outputTokensPerSecond": 52.162,
          "timeToFirstToken": 0.252
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-nano-12b-v2-vl",
      "name": "NVIDIA Nemotron Nano 12B v2 VL (Non-reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 64.9,
        "GPQA Diamond": 43.9,
        "Humanity's Last Exam": 4.5,
        "AA-LCR": 17,
        "LiveCodeBench": 34.5,
        "SciCode": 17.599999999999998,
        "AIME 2025": 26.700000000000003,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 125.273,
          "timeToFirstToken": 0.605
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "llama-nemotron-super-49b-v1-5-reasoning",
      "name": "Llama Nemotron Super 49B v1.5 (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-07-25",
      "benchmarks": {
        "MMLU Pro": 81.39999999999999,
        "GPQA Diamond": 74.8,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 34,
        "LiveCodeBench": 73.7,
        "SciCode": 34.8,
        "AIME 2025": 76.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 75.001,
          "timeToFirstToken": 0.228
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "nvidia-nemotron-3-nano-30b-a3b-reasoning",
      "name": "NVIDIA Nemotron 3 Nano 30B A3B (Reasoning)",
      "provider": "NVIDIA",
      "releaseDate": "2025-12-15",
      "benchmarks": {
        "MMLU Pro": 79.4,
        "GPQA Diamond": 75.7,
        "Humanity's Last Exam": 10.2,
        "AA-LCR": 33.7,
        "LiveCodeBench": 74.1,
        "SciCode": 29.599999999999998,
        "AIME 2025": 91,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.06,
          "output": 0.24
        },
        "performance": {
          "outputTokensPerSecond": 294.953,
          "timeToFirstToken": 0.206
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "kimi-k2-thinking",
      "name": "Kimi K2 Thinking",
      "provider": "Kimi",
      "releaseDate": "2025-11-06",
      "benchmarks": {
        "MMLU Pro": 84.8,
        "GPQA Diamond": 83.8,
        "Humanity's Last Exam": 22.3,
        "AA-LCR": 66.3,
        "LiveCodeBench": 85.3,
        "SciCode": 42.4,
        "AIME 2025": 94.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 101.629,
          "timeToFirstToken": 0.659
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "kimi-linear-48b-a3b-instruct",
      "name": "Kimi Linear 48B A3B Instruct",
      "provider": "Kimi",
      "releaseDate": "2025-10-30",
      "benchmarks": {
        "MMLU Pro": 58.5,
        "GPQA Diamond": 41.199999999999996,
        "Humanity's Last Exam": 2.7,
        "AA-LCR": 25.7,
        "LiveCodeBench": 37.8,
        "SciCode": 19.900000000000002,
        "AIME 2025": 36.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "olmo-3-7b-instruct",
      "name": "OLMo 3 7B Instruct",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-11-20",
      "benchmarks": {
        "MMLU Pro": 52.2,
        "GPQA Diamond": 40,
        "Humanity's Last Exam": 5.800000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 26.6,
        "SciCode": 10.299999999999999,
        "AIME 2025": 41.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 35.541,
          "timeToFirstToken": 0.629
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "olmo-3-1-32b-think",
      "name": "OLMo 3.1 32B Think",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-12-12",
      "benchmarks": {
        "MMLU Pro": 76.3,
        "GPQA Diamond": 59.099999999999994,
        "Humanity's Last Exam": 6,
        "AA-LCR": 0,
        "LiveCodeBench": 69.5,
        "SciCode": 29.299999999999997,
        "AIME 2025": 77.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 37.102,
          "timeToFirstToken": 0.511
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "olmo-3-7b-think",
      "name": "OLMo 3 7B Think",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-11-20",
      "benchmarks": {
        "MMLU Pro": 65.5,
        "GPQA Diamond": 51.6,
        "Humanity's Last Exam": 5.7,
        "AA-LCR": 0,
        "LiveCodeBench": 61.7,
        "SciCode": 21.2,
        "AIME 2025": 70.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.12,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 114.89,
          "timeToFirstToken": 0.533
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-micro",
      "name": "Granite 4.0 Micro",
      "provider": "IBM",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 44.7,
        "GPQA Diamond": 33.6,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 4,
        "LiveCodeBench": 18,
        "SciCode": 11.899999999999999,
        "AIME 2025": 6,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-h-350m",
      "name": "Granite 4.0 H 350M",
      "provider": "IBM",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 12.7,
        "GPQA Diamond": 25.7,
        "Humanity's Last Exam": 6.4,
        "AA-LCR": 0,
        "LiveCodeBench": 1.9,
        "SciCode": 1.7000000000000002,
        "AIME 2025": 1.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-350m",
      "name": "Granite 4.0 350M",
      "provider": "IBM",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 12.4,
        "GPQA Diamond": 26.1,
        "Humanity's Last Exam": 5.7,
        "AA-LCR": 0,
        "LiveCodeBench": 2.4,
        "SciCode": 0.8999999999999999,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-h-nano-1b",
      "name": "Granite 4.0 H 1B",
      "provider": "IBM",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 27.700000000000003,
        "GPQA Diamond": 26.3,
        "Humanity's Last Exam": 5,
        "AA-LCR": 6.3,
        "LiveCodeBench": 11.5,
        "SciCode": 8.200000000000001,
        "AIME 2025": 6.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-nano-1b",
      "name": "Granite 4.0 1B",
      "provider": "IBM",
      "releaseDate": "2025-10-28",
      "benchmarks": {
        "MMLU Pro": 32.5,
        "GPQA Diamond": 28.1,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 4,
        "LiveCodeBench": 4.7,
        "SciCode": 8.7,
        "AIME 2025": 6.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-4-0-h-small",
      "name": "Granite 4.0 H Small",
      "provider": "IBM",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 62.4,
        "GPQA Diamond": 41.6,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 9,
        "LiveCodeBench": 25.1,
        "SciCode": 20.9,
        "AIME 2025": 13.700000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.06,
          "output": 0.25
        },
        "performance": {
          "outputTokensPerSecond": 191.304,
          "timeToFirstToken": 8.833
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "reka-flash-3",
      "name": "Reka Flash 3",
      "provider": "Reka AI",
      "releaseDate": "2025-03-10",
      "benchmarks": {
        "MMLU Pro": 66.9,
        "GPQA Diamond": 52.900000000000006,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 0,
        "LiveCodeBench": 43.5,
        "SciCode": 26.700000000000003,
        "AIME 2025": 33.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "performance": {
          "outputTokensPerSecond": 48.808,
          "timeToFirstToken": 1.329
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deephermes-3-llama-3-1-8b-preview",
      "name": "DeepHermes 3 - Llama-3.1 8B Preview (Non-reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-02-13",
      "benchmarks": {
        "MMLU Pro": 36.5,
        "GPQA Diamond": 27,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 0,
        "LiveCodeBench": 8.5,
        "SciCode": 9.1,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deephermes-3-mistral-24b-preview",
      "name": "DeepHermes 3 - Mistral 24B Preview (Non-reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-03-13",
      "benchmarks": {
        "MMLU Pro": 57.99999999999999,
        "GPQA Diamond": 38.2,
        "Humanity's Last Exam": 3.9,
        "AA-LCR": 0,
        "LiveCodeBench": 19.5,
        "SciCode": 22.8,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "hermes-4-llama-3-1-70b-reasoning",
      "name": "Hermes 4 - Llama-3.1 70B (Reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-08-27",
      "benchmarks": {
        "MMLU Pro": 81.10000000000001,
        "GPQA Diamond": 69.89999999999999,
        "Humanity's Last Exam": 7.9,
        "AA-LCR": 6.7,
        "LiveCodeBench": 65.3,
        "SciCode": 34.1,
        "AIME 2025": 68.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.13,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 78.88,
          "timeToFirstToken": 0.61
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "hermes-4-llama-3-1-405b-reasoning",
      "name": "Hermes 4 - Llama-3.1 405B (Reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-08-27",
      "benchmarks": {
        "MMLU Pro": 82.89999999999999,
        "GPQA Diamond": 72.7,
        "Humanity's Last Exam": 10.299999999999999,
        "AA-LCR": 20.7,
        "LiveCodeBench": 68.60000000000001,
        "SciCode": 25.2,
        "AIME 2025": 69.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1,
          "output": 3
        },
        "performance": {
          "outputTokensPerSecond": 35.196,
          "timeToFirstToken": 0.773
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "hermes-4-llama-3-1-405b",
      "name": "Hermes 4 - Llama-3.1 405B (Non-reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-08-27",
      "benchmarks": {
        "MMLU Pro": 72.89999999999999,
        "GPQA Diamond": 53.6,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 20,
        "LiveCodeBench": 54.6,
        "SciCode": 34.599999999999994,
        "AIME 2025": 15.299999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1,
          "output": 3
        },
        "performance": {
          "outputTokensPerSecond": 33.492,
          "timeToFirstToken": 0.774
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "hermes-4-llama-3-1-70b",
      "name": "Hermes 4 - Llama-3.1 70B (Non-reasoning)",
      "provider": "Nous Research",
      "releaseDate": "2025-08-27",
      "benchmarks": {
        "MMLU Pro": 66.4,
        "GPQA Diamond": 49.1,
        "Humanity's Last Exam": 3.5999999999999996,
        "AA-LCR": 2,
        "LiveCodeBench": 26.900000000000002,
        "SciCode": 27.700000000000003,
        "AIME 2025": 11.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.13,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 69.948,
          "timeToFirstToken": 0.633
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "exaone-4-0-32b",
      "name": "EXAONE 4.0 32B (Non-reasoning)",
      "provider": "LG AI Research",
      "releaseDate": "2025-07-15",
      "benchmarks": {
        "MMLU Pro": 76.8,
        "GPQA Diamond": 62.8,
        "Humanity's Last Exam": 4.9,
        "AA-LCR": 8,
        "LiveCodeBench": 47.199999999999996,
        "SciCode": 25.2,
        "AIME 2025": 39.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 1
        },
        "performance": {
          "outputTokensPerSecond": 86.705,
          "timeToFirstToken": 0.305
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "exaone-4-0-1-2b-reasoning",
      "name": "Exaone 4.0 1.2B (Reasoning)",
      "provider": "LG AI Research",
      "releaseDate": "2025-07-15",
      "benchmarks": {
        "MMLU Pro": 58.8,
        "GPQA Diamond": 51.5,
        "Humanity's Last Exam": 5.800000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 51.6,
        "SciCode": 9.3,
        "AIME 2025": 50.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "exaone-4-0-1-2b",
      "name": "Exaone 4.0 1.2B (Non-reasoning)",
      "provider": "LG AI Research",
      "releaseDate": "2025-07-15",
      "benchmarks": {
        "MMLU Pro": 50,
        "GPQA Diamond": 42.4,
        "Humanity's Last Exam": 5.800000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 29.299999999999997,
        "SciCode": 7.3999999999999995,
        "AIME 2025": 24,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "exaone-4-0-32b-reasoning",
      "name": "EXAONE 4.0 32B (Reasoning)",
      "provider": "LG AI Research",
      "releaseDate": "2025-07-15",
      "benchmarks": {
        "MMLU Pro": 81.8,
        "GPQA Diamond": 73.9,
        "Humanity's Last Exam": 10.5,
        "AA-LCR": 14.000000000000002,
        "LiveCodeBench": 74.7,
        "SciCode": 34.4,
        "AIME 2025": 80,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 1
        },
        "performance": {
          "outputTokensPerSecond": 93.734,
          "timeToFirstToken": 0.309
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mimo-v2-flash",
      "name": "MiMo-V2-Flash (Non-reasoning)",
      "provider": "Xiaomi",
      "releaseDate": "2025-12-16",
      "benchmarks": {
        "MMLU Pro": 74.4,
        "GPQA Diamond": 65.60000000000001,
        "Humanity's Last Exam": 8,
        "AA-LCR": 31.3,
        "LiveCodeBench": 40.2,
        "SciCode": 25.900000000000002,
        "AIME 2025": 67.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 116.532,
          "timeToFirstToken": 1.467
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ernie-5-0-thinking-preview",
      "name": "ERNIE 5.0 Thinking Preview",
      "provider": "Baidu",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 83,
        "GPQA Diamond": 77.7,
        "Humanity's Last Exam": 12.7,
        "AA-LCR": 6.7,
        "LiveCodeBench": 81.2,
        "SciCode": 37.5,
        "AIME 2025": 85,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.84,
          "output": 3.37
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ernie-4-5-300b-a47b",
      "name": "ERNIE 4.5 300B A47B",
      "provider": "Baidu",
      "releaseDate": "2025-06-30",
      "benchmarks": {
        "MMLU Pro": 77.60000000000001,
        "GPQA Diamond": 81.10000000000001,
        "Humanity's Last Exam": 3.5000000000000004,
        "AA-LCR": 2.3,
        "LiveCodeBench": 46.7,
        "SciCode": 31.5,
        "AIME 2025": 41.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.28,
          "output": 1.1
        },
        "performance": {
          "outputTokensPerSecond": 28.68,
          "timeToFirstToken": 1.964
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "cogito-v2-1-reasoning",
      "name": "Cogito v2.1 (Reasoning)",
      "provider": "Deep Cogito",
      "releaseDate": "2025-11-18",
      "benchmarks": {
        "MMLU Pro": 84.89999999999999,
        "GPQA Diamond": 76.8,
        "Humanity's Last Exam": 11,
        "AA-LCR": 21.7,
        "LiveCodeBench": 68.8,
        "SciCode": 41,
        "AIME 2025": 72.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 1.25
        },
        "performance": {
          "outputTokensPerSecond": 73.722,
          "timeToFirstToken": 0.327
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "kat-coder-pro-v1",
      "name": "KAT-Coder-Pro V1",
      "provider": "KwaiKAT",
      "releaseDate": "2025-11-11",
      "benchmarks": {
        "MMLU Pro": 81.3,
        "GPQA Diamond": 76.4,
        "Humanity's Last Exam": 33.4,
        "AA-LCR": 74,
        "LiveCodeBench": 74.7,
        "SciCode": 36.6,
        "AIME 2025": 94.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 63.204,
          "timeToFirstToken": 1.03
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "motif-2-12-7b",
      "name": "Motif-2-12.7B-Reasoning",
      "provider": "Motif Technologies",
      "releaseDate": "2025-12-04",
      "benchmarks": {
        "MMLU Pro": 79.60000000000001,
        "GPQA Diamond": 69.5,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 13,
        "LiveCodeBench": 65.10000000000001,
        "SciCode": 28.199999999999996,
        "AIME 2025": 80.30000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "k2-v2",
      "name": "K2-V2 (high)",
      "provider": "MBZUAI Institute of Foundation Models",
      "releaseDate": "2025-12-05",
      "benchmarks": {
        "MMLU Pro": 78.60000000000001,
        "GPQA Diamond": 68.10000000000001,
        "Humanity's Last Exam": 9.8,
        "AA-LCR": 33.300000000000004,
        "LiveCodeBench": 69.39999999999999,
        "SciCode": 28.599999999999998,
        "AIME 2025": 78.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "k2-v2-low",
      "name": "K2-V2 (low)",
      "provider": "MBZUAI Institute of Foundation Models",
      "releaseDate": "2025-12-05",
      "benchmarks": {
        "MMLU Pro": 71.3,
        "GPQA Diamond": 54.1,
        "Humanity's Last Exam": 3.9,
        "AA-LCR": 19,
        "LiveCodeBench": 39.300000000000004,
        "SciCode": 22.3,
        "AIME 2025": 35.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "k2-v2-medium",
      "name": "K2-V2 (medium)",
      "provider": "MBZUAI Institute of Foundation Models",
      "releaseDate": "2025-12-05",
      "benchmarks": {
        "MMLU Pro": 76.1,
        "GPQA Diamond": 59.8,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 28.000000000000004,
        "LiveCodeBench": 54.1,
        "SciCode": 25.2,
        "AIME 2025": 64.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "midm-250-pro-rsnsft",
      "name": "Mi:dm K 2.5 Pro Preview",
      "provider": "Korea Telecom",
      "releaseDate": "2025-12-11",
      "benchmarks": {
        "MMLU Pro": 81.3,
        "GPQA Diamond": 72.2,
        "Humanity's Last Exam": 8.799999999999999,
        "AA-LCR": 11,
        "LiveCodeBench": 57.599999999999994,
        "SciCode": 29.7,
        "AIME 2025": 78.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "hyperclova-x-seed-think-32b",
      "name": "HyperCLOVA X SEED Think (32B)",
      "provider": "Naver",
      "releaseDate": "2025-12-26",
      "benchmarks": {
        "MMLU Pro": 78.5,
        "GPQA Diamond": 61.5,
        "Humanity's Last Exam": 5.5,
        "AA-LCR": 11.700000000000001,
        "LiveCodeBench": 62.9,
        "SciCode": 28.4,
        "AIME 2025": 59,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-6v-reasoning",
      "name": "GLM-4.6V (Reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-12-08",
      "benchmarks": {
        "MMLU Pro": 79.9,
        "GPQA Diamond": 71.89999999999999,
        "Humanity's Last Exam": 8.9,
        "AA-LCR": 40.300000000000004,
        "LiveCodeBench": 16,
        "SciCode": 30.4,
        "AIME 2025": 85.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 0.9
        },
        "performance": {
          "outputTokensPerSecond": 72.619,
          "timeToFirstToken": 0.678
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-7-non-reasoning",
      "name": "GLM-4.7 (Non-reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-12-22",
      "benchmarks": {
        "MMLU Pro": 79.4,
        "GPQA Diamond": 66.4,
        "Humanity's Last Exam": 6.1,
        "AA-LCR": 36.3,
        "LiveCodeBench": 56.2,
        "SciCode": 35.4,
        "AIME 2025": 48,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.475,
          "output": 2.05
        },
        "performance": {
          "outputTokensPerSecond": 74.776,
          "timeToFirstToken": 0.81
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-7",
      "name": "GLM-4.7 (Reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-12-22",
      "benchmarks": {
        "MMLU Pro": 85.6,
        "GPQA Diamond": 85.9,
        "Humanity's Last Exam": 25.1,
        "AA-LCR": 64,
        "LiveCodeBench": 89.4,
        "SciCode": 45.1,
        "AIME 2025": 95,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.5,
          "output": 2.1
        },
        "performance": {
          "outputTokensPerSecond": 86.042,
          "timeToFirstToken": 0.697
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-6v",
      "name": "GLM-4.6V (Non-reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-12-08",
      "benchmarks": {
        "MMLU Pro": 75.2,
        "GPQA Diamond": 56.599999999999994,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 12.3,
        "LiveCodeBench": 41.099999999999994,
        "SciCode": 27.200000000000003,
        "AIME 2025": 26.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 0.9
        },
        "performance": {
          "outputTokensPerSecond": 57.227,
          "timeToFirstToken": 0.772
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "apriel-v1-6-15b-thinker",
      "name": "Apriel-v1.6-15B-Thinker",
      "provider": "ServiceNow",
      "releaseDate": "2025-11-25",
      "benchmarks": {
        "MMLU Pro": 79,
        "GPQA Diamond": 73.3,
        "Humanity's Last Exam": 9.8,
        "AA-LCR": 50.3,
        "LiveCodeBench": 80.7,
        "SciCode": 37.3,
        "AIME 2025": 88,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 148.466,
          "timeToFirstToken": 0.24
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "jamba-1-7-large",
      "name": "Jamba 1.7 Large",
      "provider": "AI21 Labs",
      "releaseDate": "2025-07-07",
      "benchmarks": {
        "MMLU Pro": 57.699999999999996,
        "GPQA Diamond": 39,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 17.299999999999997,
        "LiveCodeBench": 18.099999999999998,
        "SciCode": 18.8,
        "AIME 2025": 2.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 50.388,
          "timeToFirstToken": 0.818
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "jamba-reasoning-3b",
      "name": "Jamba Reasoning 3B",
      "provider": "AI21 Labs",
      "releaseDate": "2025-10-08",
      "benchmarks": {
        "MMLU Pro": 57.699999999999996,
        "GPQA Diamond": 33.300000000000004,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 7.000000000000001,
        "LiveCodeBench": 21,
        "SciCode": 5.8999999999999995,
        "AIME 2025": 10.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "jamba-1-7-mini",
      "name": "Jamba 1.7 Mini",
      "provider": "AI21 Labs",
      "releaseDate": "2025-07-07",
      "benchmarks": {
        "MMLU Pro": 38.800000000000004,
        "GPQA Diamond": 32.2,
        "Humanity's Last Exam": 4.5,
        "AA-LCR": 12.7,
        "LiveCodeBench": 6.1,
        "SciCode": 9.3,
        "AIME 2025": 0.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 127.039,
          "timeToFirstToken": 0.745
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-max-thinking",
      "name": "Qwen3 Max Thinking",
      "provider": "Alibaba",
      "releaseDate": "2025-11-03",
      "benchmarks": {
        "MMLU Pro": 82.39999999999999,
        "GPQA Diamond": 77.60000000000001,
        "Humanity's Last Exam": 12,
        "AA-LCR": 57.699999999999996,
        "LiveCodeBench": 53.5,
        "SciCode": 38.7,
        "AIME 2025": 82.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.2,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 35.794,
          "timeToFirstToken": 1.714
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-1.7b-instruct-reasoning",
      "name": "Qwen3 1.7B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 56.99999999999999,
        "GPQA Diamond": 35.6,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 0,
        "LiveCodeBench": 30.8,
        "SciCode": 4.3,
        "AIME 2025": 38.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 1.26
        },
        "performance": {
          "outputTokensPerSecond": 125.719,
          "timeToFirstToken": 0.937
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-4b-2507-instruct-reasoning",
      "name": "Qwen3 4B 2507 (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-08-06",
      "benchmarks": {
        "MMLU Pro": 74.3,
        "GPQA Diamond": 66.7,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 37.7,
        "LiveCodeBench": 64.1,
        "SciCode": 25.6,
        "AIME 2025": 82.69999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-32b-reasoning",
      "name": "Qwen3 VL 32B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-10-21",
      "benchmarks": {
        "MMLU Pro": 81.8,
        "GPQA Diamond": 73.3,
        "Humanity's Last Exam": 9.6,
        "AA-LCR": 55.300000000000004,
        "LiveCodeBench": 73.8,
        "SciCode": 28.499999999999996,
        "AIME 2025": 84.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 51.932,
          "timeToFirstToken": 0.983
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-30b-a3b-reasoning",
      "name": "Qwen3 VL 30B A3B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-10-03",
      "benchmarks": {
        "MMLU Pro": 80.7,
        "GPQA Diamond": 72,
        "Humanity's Last Exam": 8.7,
        "AA-LCR": 40.699999999999996,
        "LiveCodeBench": 69.69999999999999,
        "SciCode": 28.799999999999997,
        "AIME 2025": 82.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 2.4
        },
        "performance": {
          "outputTokensPerSecond": 104.589,
          "timeToFirstToken": 0.917
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-4b-2507-instruct",
      "name": "Qwen3 4B 2507 Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-08-06",
      "benchmarks": {
        "MMLU Pro": 67.2,
        "GPQA Diamond": 51.7,
        "Humanity's Last Exam": 4.7,
        "AA-LCR": 7.3,
        "LiveCodeBench": 37.7,
        "SciCode": 18.099999999999998,
        "AIME 2025": 52.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-30b-a3b-instruct",
      "name": "Qwen3 VL 30B A3B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-10-03",
      "benchmarks": {
        "MMLU Pro": 76.4,
        "GPQA Diamond": 69.5,
        "Humanity's Last Exam": 6.4,
        "AA-LCR": 23.7,
        "LiveCodeBench": 47.599999999999994,
        "SciCode": 30.8,
        "AIME 2025": 72.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "performance": {
          "outputTokensPerSecond": 95.735,
          "timeToFirstToken": 0.989
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-30b-a3b-2507-reasoning",
      "name": "Qwen3 30B A3B 2507 (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-07-30",
      "benchmarks": {
        "MMLU Pro": 80.5,
        "GPQA Diamond": 70.7,
        "Humanity's Last Exam": 9.8,
        "AA-LCR": 59,
        "LiveCodeBench": 70.7,
        "SciCode": 33.300000000000004,
        "AIME 2025": 56.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 2.4
        },
        "performance": {
          "outputTokensPerSecond": 180.044,
          "timeToFirstToken": 0.959
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-4b-instruct",
      "name": "Qwen3 VL 4B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-10-14",
      "benchmarks": {
        "MMLU Pro": 63.4,
        "GPQA Diamond": 37.1,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 13,
        "LiveCodeBench": 28.999999999999996,
        "SciCode": 13.700000000000001,
        "AIME 2025": 37,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-8b-reasoning",
      "name": "Qwen3 VL 8B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-10-14",
      "benchmarks": {
        "MMLU Pro": 74.9,
        "GPQA Diamond": 57.9,
        "Humanity's Last Exam": 3.3000000000000003,
        "AA-LCR": 31,
        "LiveCodeBench": 35.3,
        "SciCode": 21.9,
        "AIME 2025": 30.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.18,
          "output": 2.1
        },
        "performance": {
          "outputTokensPerSecond": 64.028,
          "timeToFirstToken": 0.946
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-omni-30b-a3b-instruct",
      "name": "Qwen3 Omni 30B A3B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 72.5,
        "GPQA Diamond": 62,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 0,
        "LiveCodeBench": 42.199999999999996,
        "SciCode": 18.6,
        "AIME 2025": 52.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.25,
          "output": 0.97
        },
        "performance": {
          "outputTokensPerSecond": 88.882,
          "timeToFirstToken": 0.924
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-8b-instruct",
      "name": "Qwen3 VL 8B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-10-14",
      "benchmarks": {
        "MMLU Pro": 68.60000000000001,
        "GPQA Diamond": 42.699999999999996,
        "Humanity's Last Exam": 2.9000000000000004,
        "AA-LCR": 15.299999999999999,
        "LiveCodeBench": 33.2,
        "SciCode": 17.4,
        "AIME 2025": 27.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.18,
          "output": 0.7
        },
        "performance": {
          "outputTokensPerSecond": 104.281,
          "timeToFirstToken": 0.875
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-0.6b-instruct-reasoning",
      "name": "Qwen3 0.6B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 34.699999999999996,
        "GPQA Diamond": 23.9,
        "Humanity's Last Exam": 5.7,
        "AA-LCR": 0,
        "LiveCodeBench": 12.1,
        "SciCode": 2.8000000000000003,
        "AIME 2025": 18,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 1.26
        },
        "performance": {
          "outputTokensPerSecond": 201.411,
          "timeToFirstToken": 0.815
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-4b-reasoning",
      "name": "Qwen3 VL 4B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-10-14",
      "benchmarks": {
        "MMLU Pro": 70,
        "GPQA Diamond": 49.4,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 21.3,
        "LiveCodeBench": 32,
        "SciCode": 17.1,
        "AIME 2025": 25.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-coder-30b-a3b-instruct",
      "name": "Qwen3 Coder 30B A3B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-07-31",
      "benchmarks": {
        "MMLU Pro": 70.6,
        "GPQA Diamond": 51.6,
        "Humanity's Last Exam": 4,
        "AA-LCR": 28.999999999999996,
        "LiveCodeBench": 40.300000000000004,
        "SciCode": 27.800000000000004,
        "AIME 2025": 28.999999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.45,
          "output": 2.25
        },
        "performance": {
          "outputTokensPerSecond": 102.767,
          "timeToFirstToken": 1.447
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-vl-32b-instruct",
      "name": "Qwen3 VL 32B Instruct",
      "provider": "Alibaba",
      "releaseDate": "2025-10-21",
      "benchmarks": {
        "MMLU Pro": 79.10000000000001,
        "GPQA Diamond": 67.10000000000001,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 31.3,
        "LiveCodeBench": 51.4,
        "SciCode": 30.099999999999998,
        "AIME 2025": 68.30000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 43.386,
          "timeToFirstToken": 0.996
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-0.6b-instruct",
      "name": "Qwen3 0.6B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 23.1,
        "GPQA Diamond": 23.1,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 0,
        "LiveCodeBench": 7.3,
        "SciCode": 4.1000000000000005,
        "AIME 2025": 10.299999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 189.206,
          "timeToFirstToken": 0.87
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-omni-30b-a3b-reasoning",
      "name": "Qwen3 Omni 30B A3B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 79.2,
        "GPQA Diamond": 72.6,
        "Humanity's Last Exam": 7.3,
        "AA-LCR": 0,
        "LiveCodeBench": 67.9,
        "SciCode": 30.599999999999998,
        "AIME 2025": 74,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.25,
          "output": 0.97
        },
        "performance": {
          "outputTokensPerSecond": 95.522,
          "timeToFirstToken": 0.846
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-1.7b-instruct",
      "name": "Qwen3 1.7B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 41.099999999999994,
        "GPQA Diamond": 28.299999999999997,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 0,
        "LiveCodeBench": 12.6,
        "SciCode": 6.9,
        "AIME 2025": 7.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 116.657,
          "timeToFirstToken": 0.867
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ring-1t",
      "name": "Ring-1T",
      "provider": "InclusionAI",
      "releaseDate": "2025-10-13",
      "benchmarks": {
        "MMLU Pro": 80.60000000000001,
        "GPQA Diamond": 59.5,
        "Humanity's Last Exam": 10.2,
        "AA-LCR": 0,
        "LiveCodeBench": 64.3,
        "SciCode": 36.7,
        "AIME 2025": 89.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.56,
          "output": 2.24
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ling-mini-2-0",
      "name": "Ling-mini-2.0",
      "provider": "InclusionAI",
      "releaseDate": "2025-09-09",
      "benchmarks": {
        "MMLU Pro": 67.10000000000001,
        "GPQA Diamond": 56.2,
        "Humanity's Last Exam": 5,
        "AA-LCR": 6.7,
        "LiveCodeBench": 42.9,
        "SciCode": 13.5,
        "AIME 2025": 49.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.07,
          "output": 0.28
        },
        "performance": {
          "outputTokensPerSecond": 182.334,
          "timeToFirstToken": 1.385
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "ling-1t",
      "name": "Ling-1T",
      "provider": "InclusionAI",
      "releaseDate": "2025-10-08",
      "benchmarks": {
        "MMLU Pro": 82.19999999999999,
        "GPQA Diamond": 71.89999999999999,
        "Humanity's Last Exam": 7.199999999999999,
        "AA-LCR": 34.699999999999996,
        "LiveCodeBench": 67.7,
        "SciCode": 35.199999999999996,
        "AIME 2025": 71.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "doubao-seed-1-8",
      "name": "Doubao-Seed-1.8",
      "provider": "ByteDance Seed",
      "releaseDate": "2025-12-18",
      "benchmarks": {
        "MMLU Pro": 85,
        "GPQA Diamond": 80.10000000000001,
        "Humanity's Last Exam": 14.799999999999999,
        "AA-LCR": 62,
        "LiveCodeBench": 74.5,
        "SciCode": 44.9,
        "AIME 2025": 84.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 0.28
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "doubao-seed-code",
      "name": "Doubao Seed Code",
      "provider": "ByteDance Seed",
      "releaseDate": "2025-11-11",
      "benchmarks": {
        "MMLU Pro": 85.39999999999999,
        "GPQA Diamond": 76.4,
        "Humanity's Last Exam": 13.3,
        "AA-LCR": 65.3,
        "LiveCodeBench": 76.6,
        "SciCode": 40.699999999999996,
        "AIME 2025": 79.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.17,
          "output": 1.12
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-nano-minimal",
      "name": "GPT-5 nano (minimal)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 55.60000000000001,
        "GPQA Diamond": 42.8,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 20,
        "LiveCodeBench": 47,
        "SciCode": 29.099999999999998,
        "AIME 2025": 27.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.05,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 146.02,
          "timeToFirstToken": 0.706
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-mini-minimal",
      "name": "GPT-5 mini (minimal)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 77.5,
        "GPQA Diamond": 68.7,
        "Humanity's Last Exam": 5,
        "AA-LCR": 35.699999999999996,
        "LiveCodeBench": 54.50000000000001,
        "SciCode": 36.9,
        "AIME 2025": 46.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.25,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 62.68,
          "timeToFirstToken": 0.729
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-minimal",
      "name": "GPT-5 (minimal)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 80.60000000000001,
        "GPQA Diamond": 67.30000000000001,
        "Humanity's Last Exam": 5.4,
        "AA-LCR": 25,
        "LiveCodeBench": 55.800000000000004,
        "SciCode": 38.800000000000004,
        "AIME 2025": 31.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 96.057,
          "timeToFirstToken": 0.962
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-low",
      "name": "GPT-5 (low)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 86,
        "GPQA Diamond": 80.80000000000001,
        "Humanity's Last Exam": 18.4,
        "AA-LCR": 58.699999999999996,
        "LiveCodeBench": 76.3,
        "SciCode": 39.1,
        "AIME 2025": 83,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 119.709,
          "timeToFirstToken": 23.942
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-4o-chatgpt",
      "name": "GPT-4o (ChatGPT)",
      "provider": "OpenAI",
      "releaseDate": "2025-02-15",
      "benchmarks": {
        "MMLU Pro": 77.3,
        "GPQA Diamond": 51.1,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 53,
        "LiveCodeBench": 0,
        "SciCode": 33.4,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 5,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 221.897,
          "timeToFirstToken": 0.507
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-medium",
      "name": "GPT-5 (medium)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 86.7,
        "GPQA Diamond": 84.2,
        "Humanity's Last Exam": 23.5,
        "AA-LCR": 72.8,
        "LiveCodeBench": 70.3,
        "SciCode": 41.099999999999994,
        "AIME 2025": 91.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 127.752,
          "timeToFirstToken": 41.251
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-nano-medium",
      "name": "GPT-5 nano (medium)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 77.2,
        "GPQA Diamond": 67,
        "Humanity's Last Exam": 7.6,
        "AA-LCR": 40,
        "LiveCodeBench": 76.3,
        "SciCode": 33.800000000000004,
        "AIME 2025": 78.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.05,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 142.083,
          "timeToFirstToken": 54.339
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-mini-medium",
      "name": "GPT-5 mini (medium)",
      "provider": "OpenAI",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 80.30000000000001,
        "Humanity's Last Exam": 14.6,
        "AA-LCR": 66,
        "LiveCodeBench": 69.19999999999999,
        "SciCode": 41,
        "AIME 2025": 85,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.25,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 68.793,
          "timeToFirstToken": 38.323
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gpt-5-codex",
      "name": "GPT-5 Codex (high)",
      "provider": "OpenAI",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 86.5,
        "GPQA Diamond": 83.7,
        "Humanity's Last Exam": 25.6,
        "AA-LCR": 69,
        "LiveCodeBench": 84,
        "SciCode": 40.9,
        "AIME 2025": 98.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 262.679,
          "timeToFirstToken": 15.56
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-0-pro-experimental-02-05",
      "name": "Gemini 2.0 Pro Experimental (Feb '25)",
      "provider": "Google",
      "releaseDate": "2025-02-05",
      "benchmarks": {
        "MMLU Pro": 80.5,
        "GPQA Diamond": 62.2,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 34.699999999999996,
        "SciCode": 31.2,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-preview-09-2025-reasoning",
      "name": "Gemini 2.5 Flash Preview (Sep '25) (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-09-25",
      "benchmarks": {
        "MMLU Pro": 84.2,
        "GPQA Diamond": 79.3,
        "Humanity's Last Exam": 12.7,
        "AA-LCR": 64.3,
        "LiveCodeBench": 71.3,
        "SciCode": 40.5,
        "AIME 2025": 78.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 316.659,
          "timeToFirstToken": 9.89
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemma-3n-e4b-preview-0520",
      "name": "Gemma 3n E4B Instruct Preview (May '25)",
      "provider": "Google",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 48.3,
        "GPQA Diamond": 27.800000000000004,
        "Humanity's Last Exam": 4.9,
        "AA-LCR": 0,
        "LiveCodeBench": 13.8,
        "SciCode": 8.6,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-lite",
      "name": "Gemini 2.5 Flash-Lite (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-06-17",
      "benchmarks": {
        "MMLU Pro": 72.39999999999999,
        "GPQA Diamond": 47.4,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 31.3,
        "LiveCodeBench": 40,
        "SciCode": 17.7,
        "AIME 2025": 35.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 302.239,
          "timeToFirstToken": 0.429
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-reasoning-04-2025",
      "name": "Gemini 2.5 Flash Preview (Reasoning)",
      "provider": "Google",
      "releaseDate": "2025-04-17",
      "benchmarks": {
        "MMLU Pro": 80,
        "GPQA Diamond": 69.8,
        "Humanity's Last Exam": 11.600000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 50.5,
        "SciCode": 35.9,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash",
      "name": "Gemini 2.5 Flash (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 80.9,
        "GPQA Diamond": 68.30000000000001,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 45.9,
        "LiveCodeBench": 49.5,
        "SciCode": 29.099999999999998,
        "AIME 2025": 60.3,
        "MMMU Pro": 66,
        "AA-Omniscience": -44,
        "AA-Omniscience Accuracy": 25.35,
        "AA-Omniscience Hallucination Rate": 92.57,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 237.573,
          "timeToFirstToken": 0.385
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-flash-04-2025",
      "name": "Gemini 2.5 Flash Preview (Non-reasoning)",
      "provider": "Google",
      "releaseDate": "2025-04-17",
      "benchmarks": {
        "MMLU Pro": 78.3,
        "GPQA Diamond": 59.4,
        "Humanity's Last Exam": 5,
        "AA-LCR": 0,
        "LiveCodeBench": 40.6,
        "SciCode": 23.3,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-0-flash-lite-001",
      "name": "Gemini 2.0 Flash-Lite (Feb '25)",
      "provider": "Google",
      "releaseDate": "2025-02-25",
      "benchmarks": {
        "MMLU Pro": 72.39999999999999,
        "GPQA Diamond": 53.5,
        "Humanity's Last Exam": 3.5999999999999996,
        "AA-LCR": 0,
        "LiveCodeBench": 18.5,
        "SciCode": 25,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.075,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-pro-05-06",
      "name": "Gemini 2.5 Pro Preview (May' 25)",
      "provider": "Google",
      "releaseDate": "2025-05-06",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 82.19999999999999,
        "Humanity's Last Exam": 15.4,
        "AA-LCR": 0,
        "LiveCodeBench": 77,
        "SciCode": 41.6,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-0-flash-thinking-exp-0121",
      "name": "Gemini 2.0 Flash Thinking Experimental (Jan '25)",
      "provider": "Google",
      "releaseDate": "2025-01-21",
      "benchmarks": {
        "MMLU Pro": 79.80000000000001,
        "GPQA Diamond": 70.1,
        "Humanity's Last Exam": 7.1,
        "AA-LCR": 0,
        "LiveCodeBench": 32.1,
        "SciCode": 32.9,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "gemini-2-5-pro-03-25",
      "name": "Gemini 2.5 Pro Preview (Mar' 25)",
      "provider": "Google",
      "releaseDate": "2025-03-25",
      "benchmarks": {
        "MMLU Pro": 85.8,
        "GPQA Diamond": 83.6,
        "Humanity's Last Exam": 17.1,
        "AA-LCR": 0,
        "LiveCodeBench": 77.8,
        "SciCode": 39.5,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-small-3",
      "name": "Mistral Small 3",
      "provider": "Mistral",
      "releaseDate": "2025-01-30",
      "benchmarks": {
        "MMLU Pro": 65.2,
        "GPQA Diamond": 46.2,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 0,
        "LiveCodeBench": 25.2,
        "SciCode": 23.599999999999998,
        "AIME 2025": 4.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 234.97,
          "timeToFirstToken": 0.352
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "devstral-small-2505",
      "name": "Devstral Small (May '25)",
      "provider": "Mistral",
      "releaseDate": "2025-05-21",
      "benchmarks": {
        "MMLU Pro": 63.2,
        "GPQA Diamond": 43.4,
        "Humanity's Last Exam": 4,
        "AA-LCR": 0,
        "LiveCodeBench": 25.8,
        "SciCode": 24.5,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.1,
          "output": 0.3
        },
        "performance": {
          "outputTokensPerSecond": 199.15,
          "timeToFirstToken": 0.365
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "magistral-small",
      "name": "Magistral Small 1",
      "provider": "Mistral",
      "releaseDate": "2025-06-10",
      "benchmarks": {
        "MMLU Pro": 74.6,
        "GPQA Diamond": 64.1,
        "Humanity's Last Exam": 7.199999999999999,
        "AA-LCR": 0,
        "LiveCodeBench": 51.4,
        "SciCode": 24.099999999999998,
        "AIME 2025": 41.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "performance": {
          "outputTokensPerSecond": 208.314,
          "timeToFirstToken": 0.327
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-medium-3",
      "name": "Mistral Medium 3",
      "provider": "Mistral",
      "releaseDate": "2025-05-07",
      "benchmarks": {
        "MMLU Pro": 76,
        "GPQA Diamond": 57.8,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 28.000000000000004,
        "LiveCodeBench": 40,
        "SciCode": 33.1,
        "AIME 2025": 30.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 91.825,
          "timeToFirstToken": 0.353
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "magistral-medium",
      "name": "Magistral Medium 1",
      "provider": "Mistral",
      "releaseDate": "2025-06-10",
      "benchmarks": {
        "MMLU Pro": 75.3,
        "GPQA Diamond": 67.9,
        "Humanity's Last Exam": 9.5,
        "AA-LCR": 0,
        "LiveCodeBench": 52.7,
        "SciCode": 29.7,
        "AIME 2025": 40.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2,
          "output": 5
        },
        "performance": {
          "outputTokensPerSecond": 39.119,
          "timeToFirstToken": 0.471
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "mistral-saba",
      "name": "Mistral Saba",
      "provider": "Mistral",
      "releaseDate": "2025-02-17",
      "benchmarks": {
        "MMLU Pro": 61.1,
        "GPQA Diamond": 42.4,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 24.099999999999998,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-distill-qwen-32b",
      "name": "DeepSeek R1 Distill Qwen 32B",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 73.9,
        "GPQA Diamond": 61.5,
        "Humanity's Last Exam": 5.5,
        "AA-LCR": 9.700000000000001,
        "LiveCodeBench": 27,
        "SciCode": 37.6,
        "AIME 2025": 63,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.285,
          "output": 0.285
        },
        "performance": {
          "outputTokensPerSecond": 81.621,
          "timeToFirstToken": 0.301
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-distill-qwen-14b",
      "name": "DeepSeek R1 Distill Qwen 14B",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 74,
        "GPQA Diamond": 48.4,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 7.000000000000001,
        "LiveCodeBench": 37.6,
        "SciCode": 23.9,
        "AIME 2025": 55.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.15,
          "output": 0.15
        },
        "performance": {
          "outputTokensPerSecond": 62.643,
          "timeToFirstToken": 0.938
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-distill-llama-8b",
      "name": "DeepSeek R1 Distill Llama 8B",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 54.300000000000004,
        "GPQA Diamond": 30.2,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 0,
        "LiveCodeBench": 23.3,
        "SciCode": 11.899999999999999,
        "AIME 2025": 41.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-r1-distill-qwen-1-5b",
      "name": "DeepSeek R1 Distill Qwen 1.5B",
      "provider": "DeepSeek",
      "releaseDate": "2025-01-20",
      "benchmarks": {
        "MMLU Pro": 26.900000000000002,
        "GPQA Diamond": 9.8,
        "Humanity's Last Exam": 3.3000000000000003,
        "AA-LCR": 0.3,
        "LiveCodeBench": 7.000000000000001,
        "SciCode": 6.6000000000000005,
        "AIME 2025": 22,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "deepseek-v3-2-reasoning-0925",
      "name": "DeepSeek V3.2 Exp (Reasoning)",
      "provider": "DeepSeek",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 85,
        "GPQA Diamond": 79.7,
        "Humanity's Last Exam": 13.8,
        "AA-LCR": 69,
        "LiveCodeBench": 78.9,
        "SciCode": 37.7,
        "AIME 2025": 87.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 30.986,
          "timeToFirstToken": 1.406
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "sonar-pro",
      "name": "Sonar Pro",
      "provider": "Perplexity",
      "releaseDate": "2025-01-21",
      "benchmarks": {
        "MMLU Pro": 75.5,
        "GPQA Diamond": 57.8,
        "Humanity's Last Exam": 7.9,
        "AA-LCR": 0,
        "LiveCodeBench": 27.500000000000004,
        "SciCode": 22.6,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 146.445,
          "timeToFirstToken": 1.464
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "sonar",
      "name": "Sonar",
      "provider": "Perplexity",
      "releaseDate": "2025-01-21",
      "benchmarks": {
        "MMLU Pro": 68.89999999999999,
        "GPQA Diamond": 47.099999999999994,
        "Humanity's Last Exam": 7.3,
        "AA-LCR": 0,
        "LiveCodeBench": 29.5,
        "SciCode": 22.900000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 1,
          "output": 1
        },
        "performance": {
          "outputTokensPerSecond": 124.413,
          "timeToFirstToken": 1.511
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "grok-3",
      "name": "Grok 3",
      "provider": "xAI",
      "releaseDate": "2025-02-19",
      "benchmarks": {
        "MMLU Pro": 79.9,
        "GPQA Diamond": 69.3,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 54.7,
        "LiveCodeBench": 42.5,
        "SciCode": 36.8,
        "AIME 2025": 57.99999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 40.898,
          "timeToFirstToken": 0.884
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "solar-pro-2-preview-reasoning",
      "name": "Solar Pro 2 (Preview) (Reasoning)",
      "provider": "Upstage",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 76.8,
        "GPQA Diamond": 57.8,
        "Humanity's Last Exam": 5.7,
        "AA-LCR": 0,
        "LiveCodeBench": 46.2,
        "SciCode": 16.400000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "solar-pro-2-preview",
      "name": "Solar Pro 2 (Preview) (Non-reasoning)",
      "provider": "Upstage",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 72.5,
        "GPQA Diamond": 54.400000000000006,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 0,
        "LiveCodeBench": 38.5,
        "SciCode": 27.200000000000003,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "minimax-m1-40k",
      "name": "MiniMax M1 40k",
      "provider": "MiniMax",
      "releaseDate": "2025-06-17",
      "benchmarks": {
        "MMLU Pro": 80.80000000000001,
        "GPQA Diamond": 68.2,
        "Humanity's Last Exam": 7.5,
        "AA-LCR": 51.7,
        "LiveCodeBench": 65.7,
        "SciCode": 37.8,
        "AIME 2025": 13.700000000000001,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.4,
          "output": 2.1
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "minimax-m1-80k",
      "name": "MiniMax M1 80k",
      "provider": "MiniMax",
      "releaseDate": "2025-06-17",
      "benchmarks": {
        "MMLU Pro": 81.6,
        "GPQA Diamond": 69.69999999999999,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 54.300000000000004,
        "LiveCodeBench": 71.1,
        "SciCode": 37.4,
        "AIME 2025": 61,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.4,
          "output": 2.1
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "kimi-k2",
      "name": "Kimi K2",
      "provider": "Kimi",
      "releaseDate": "2025-07-11",
      "benchmarks": {
        "MMLU Pro": 82.39999999999999,
        "GPQA Diamond": 76.6,
        "Humanity's Last Exam": 7.000000000000001,
        "AA-LCR": 51,
        "LiveCodeBench": 55.60000000000001,
        "SciCode": 34.5,
        "AIME 2025": 56.99999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 61.784,
          "timeToFirstToken": 0.735
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "tulu3-405b",
      "name": "Llama 3.1 Tulu3 405B",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-01-30",
      "benchmarks": {
        "MMLU Pro": 71.6,
        "GPQA Diamond": 51.6,
        "Humanity's Last Exam": 3.5000000000000004,
        "AA-LCR": 0,
        "LiveCodeBench": 29.099999999999998,
        "SciCode": 30.2,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "olmo-2-32b",
      "name": "OLMo 2 32B",
      "provider": "Allen Institute for AI",
      "releaseDate": "2025-03-13",
      "benchmarks": {
        "MMLU Pro": 51.1,
        "GPQA Diamond": 32.800000000000004,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 0,
        "LiveCodeBench": 6.800000000000001,
        "SciCode": 8,
        "AIME 2025": 3.3000000000000003,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "granite-3-3-8b-instruct",
      "name": "Granite 3.3 8B (Non-reasoning)",
      "provider": "IBM",
      "releaseDate": "2025-04-16",
      "benchmarks": {
        "MMLU Pro": 46.800000000000004,
        "GPQA Diamond": 33.800000000000004,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 4.3,
        "LiveCodeBench": 12.7,
        "SciCode": 10.100000000000001,
        "AIME 2025": 6.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.03,
          "output": 0.25
        },
        "performance": {
          "outputTokensPerSecond": 578.095,
          "timeToFirstToken": 7.292
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-6-reasoning",
      "name": "GLM-4.6 (Reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-09-30",
      "benchmarks": {
        "MMLU Pro": 82.89999999999999,
        "GPQA Diamond": 78,
        "Humanity's Last Exam": 13.3,
        "AA-LCR": 54.300000000000004,
        "LiveCodeBench": 69.5,
        "SciCode": 38.4,
        "AIME 2025": 86,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.55,
          "output": 2.2
        },
        "performance": {
          "outputTokensPerSecond": 146.148,
          "timeToFirstToken": 0.438
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "glm-4-5v",
      "name": "GLM-4.5V (Non-reasoning)",
      "provider": "Z AI",
      "releaseDate": "2025-08-11",
      "benchmarks": {
        "MMLU Pro": 75.1,
        "GPQA Diamond": 57.3,
        "Humanity's Last Exam": 3.5999999999999996,
        "AA-LCR": 0,
        "LiveCodeBench": 35.199999999999996,
        "SciCode": 18.8,
        "AIME 2025": 15.299999999999999,
        "MMMU Pro": 43,
        "AA-Omniscience": -57,
        "AA-Omniscience Accuracy": 17.3,
        "AA-Omniscience Hallucination Rate": 89.68,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.6,
          "output": 1.8
        },
        "performance": {
          "outputTokensPerSecond": 59.831,
          "timeToFirstToken": 0.67
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "apriel-v1-5-15b-thinker",
      "name": "Apriel-v1.5-15B-Thinker",
      "provider": "ServiceNow",
      "releaseDate": "2025-09-30",
      "benchmarks": {
        "MMLU Pro": 77.3,
        "GPQA Diamond": 71.3,
        "Humanity's Last Exam": 12,
        "AA-LCR": 20,
        "LiveCodeBench": 72.8,
        "SciCode": 34.8,
        "AIME 2025": 87.5,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 146.484,
          "timeToFirstToken": 0.179
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "jamba-1-6-mini",
      "name": "Jamba 1.6 Mini",
      "provider": "AI21 Labs",
      "releaseDate": "2025-03-06",
      "benchmarks": {
        "MMLU Pro": 36.7,
        "GPQA Diamond": 30,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 0,
        "LiveCodeBench": 7.1,
        "SciCode": 10.100000000000001,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 126.147,
          "timeToFirstToken": 0.666
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "jamba-1-6-large",
      "name": "Jamba 1.6 Large",
      "provider": "AI21 Labs",
      "releaseDate": "2025-03-06",
      "benchmarks": {
        "MMLU Pro": 56.49999999999999,
        "GPQA Diamond": 38.7,
        "Humanity's Last Exam": 4,
        "AA-LCR": 0,
        "LiveCodeBench": 17.2,
        "SciCode": 18.4,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 46.522,
          "timeToFirstToken": 0.821
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-32b-instruct-reasoning",
      "name": "Qwen3 32B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 79.80000000000001,
        "GPQA Diamond": 66.8,
        "Humanity's Last Exam": 8.3,
        "AA-LCR": 0,
        "LiveCodeBench": 54.6,
        "SciCode": 35.4,
        "AIME 2025": 73,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 90.814,
          "timeToFirstToken": 0.896
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-235b-a22b-instruct",
      "name": "Qwen3 235B A22B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 76.2,
        "GPQA Diamond": 61.3,
        "Humanity's Last Exam": 4.7,
        "AA-LCR": 0,
        "LiveCodeBench": 34.300000000000004,
        "SciCode": 29.9,
        "AIME 2025": 23.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -54,
        "AA-Omniscience Accuracy": 17.3,
        "AA-Omniscience Hallucination Rate": 86.62,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 46.835,
          "timeToFirstToken": 1.102
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": false,
      "isReasoning": false
    },
    {
      "id": "qwen3-8b-instruct-reasoning",
      "name": "Qwen3 8B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 74.3,
        "GPQA Diamond": 58.9,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 0,
        "LiveCodeBench": 40.6,
        "SciCode": 22.6,
        "AIME 2025": 19,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.18,
          "output": 2.1
        },
        "performance": {
          "outputTokensPerSecond": 86.426,
          "timeToFirstToken": 0.978
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-4b-instruct",
      "name": "Qwen3 4B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 58.599999999999994,
        "GPQA Diamond": 39.800000000000004,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 0,
        "LiveCodeBench": 23.3,
        "SciCode": 16.7,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 84.802,
          "timeToFirstToken": 0.942
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-14b-instruct",
      "name": "Qwen3 14B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 67.5,
        "GPQA Diamond": 47,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 0,
        "LiveCodeBench": 28.000000000000004,
        "SciCode": 26.5,
        "AIME 2025": 57.99999999999999,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.35,
          "output": 1.4
        },
        "performance": {
          "outputTokensPerSecond": 54.681,
          "timeToFirstToken": 1.005
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-8b-instruct",
      "name": "Qwen3 8B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 64.3,
        "GPQA Diamond": 45.2,
        "Humanity's Last Exam": 2.8000000000000003,
        "AA-LCR": 0,
        "LiveCodeBench": 20.200000000000003,
        "SciCode": 16.8,
        "AIME 2025": 24.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.18,
          "output": 0.7
        },
        "performance": {
          "outputTokensPerSecond": 82.542,
          "timeToFirstToken": 0.846
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-4b-instruct-reasoning",
      "name": "Qwen3 4B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 69.6,
        "GPQA Diamond": 52.2,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 0,
        "LiveCodeBench": 46.5,
        "SciCode": 3.5000000000000004,
        "AIME 2025": 22.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.11,
          "output": 1.26
        },
        "performance": {
          "outputTokensPerSecond": 90.749,
          "timeToFirstToken": 0.911
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-14b-instruct-reasoning",
      "name": "Qwen3 14B (Reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 77.4,
        "GPQA Diamond": 60.4,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 0,
        "LiveCodeBench": 52.300000000000004,
        "SciCode": 31.6,
        "AIME 2025": 55.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.35,
          "output": 4.2
        },
        "performance": {
          "outputTokensPerSecond": 58.835,
          "timeToFirstToken": 1.079
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "qwen3-30b-a3b-instruct",
      "name": "Qwen3 30B A3B (Non-reasoning)",
      "provider": "Alibaba",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 71,
        "GPQA Diamond": 51.5,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 0,
        "LiveCodeBench": 32.2,
        "SciCode": 26.400000000000002,
        "AIME 2025": 21.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.2,
          "output": 0.8
        },
        "performance": {
          "outputTokensPerSecond": 83.779,
          "timeToFirstToken": 1.021
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    },
    {
      "id": "seed-oss-36b-instruct",
      "name": "Seed-OSS-36B-Instruct",
      "provider": "ByteDance Seed",
      "releaseDate": "2025-08-20",
      "benchmarks": {
        "MMLU Pro": 81.5,
        "GPQA Diamond": 72.6,
        "Humanity's Last Exam": 9.1,
        "AA-LCR": 57.699999999999996,
        "LiveCodeBench": 76.5,
        "SciCode": 36.5,
        "AIME 2025": 84.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 0,
        "LMArena-Text-Creative-Writing": 0,
        "LMArean-Text-Math": 0,
        "LMArena-Text-Coding": 0,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 0,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 0,
        "LMArena-Vision": 0
      },
      "metadata": {
        "lmarenaRank": null,
        "lmarenaVotes": null,
        "pricing": {
          "input": 0.21,
          "output": 0.57
        },
        "performance": {
          "outputTokensPerSecond": 32.671,
          "timeToFirstToken": 1.629
        }
      },
      "tags": [],
      "description": "",
      "supportsVision": true,
      "isReasoning": false
    }
  ]
}