{
  "metadata": {
    "version": "2.0.0",
    "lastUpdated": "2025-12-16",
    "description": "LLM 벤치마크 통합 데이터",
    "totalModels": 78,
    "defaultModelIds": [
      "gemini-3-pro",
      "claude-opus-4-5",
      "gpt-5-1",
      "gemini-2-5-pro",
      "claude-4-5-sonnet",
      "gpt-5-1",
      "gpt-5",
      "o3",
      "qwen3-max-preview",
      "gpt-5",
      "qwen3-235b-a22b-instruct-2507",
      "deepseek-r1",
      "qwen3-vl-235b-a22b-instruct",
      "gemini-2-5-flash-preview-09-2025",
      "claude-4-5-haiku",
      "qwen3-next-80b-a3b-instruct",
      "deepseek-r1",
      "gpt-5-mini",
      "gpt-5-nano"
    ],
    "dataSources": {
      "lmarena": {
        "lastFetched": "2025-12-16T05:35:39.577Z",
        "totalModels": 283
      },
      "artificialanalysis": {
        "lastFetched": "2025-12-16T05:35:18.889Z",
        "totalModels": 357
      }
    }
  },
  "models": [
    {
      "id": "gemini-3-pro",
      "name": "gemini-3-pro",
      "provider": "1◄─►2",
      "releaseDate": "2025-11-18",
      "benchmarks": {
        "MMLU Pro": 89.8,
        "GPQA Diamond": 90.8,
        "Humanity's Last Exam": 37.2,
        "AA-LCR": 70.7,
        "LiveCodeBench": 91.7,
        "SciCode": 56.10000000000001,
        "AIME 2025": 95.7,
        "MMMU Pro": 80,
        "AA-Omniscience": 13,
        "AA-Omniscience Accuracy": 53.65,
        "AA-Omniscience Hallucination Rate": 87.99,
        "LMArena-Text": 1491,
        "LMArena-Text-Creative-Writing": 1492,
        "LMArean-Text-Math": 1476,
        "LMArena-Text-Coding": 1520,
        "LMArena-Text-Expert": 1499,
        "LMArena-Text-Hard-Prompts": 1504,
        "LMArena-Text-Longer-Query": 1494,
        "LMArena-Text-Multi-Turn": 1499
      },
      "metadata": {
        "lmarenaRank": 1,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 12
        },
        "performance": {
          "outputTokensPerSecond": 118.708,
          "timeToFirstToken": 33.902
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-opus-4-5",
      "name": "Anthropicclaude-opus-4-5-20251101",
      "provider": "3◄─►6",
      "releaseDate": "2025-11-24",
      "benchmarks": {
        "MMLU Pro": 88.9,
        "GPQA Diamond": 81,
        "Humanity's Last Exam": 12.9,
        "AA-LCR": 65.3,
        "LiveCodeBench": 73.8,
        "SciCode": 47,
        "AIME 2025": 62.7,
        "MMMU Pro": 71,
        "AA-Omniscience": -6,
        "AA-Omniscience Accuracy": 38.9,
        "AA-Omniscience Hallucination Rate": 74.22,
        "LMArena-Text": 1464,
        "LMArena-Text-Creative-Writing": 1455,
        "LMArean-Text-Math": 1474,
        "LMArena-Text-Coding": 1516,
        "LMArena-Text-Expert": 1541,
        "LMArena-Text-Hard-Prompts": 1493,
        "LMArena-Text-Longer-Query": 1499,
        "LMArena-Text-Multi-Turn": 1477
      },
      "metadata": {
        "lmarenaRank": 4,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 5,
          "output": 25
        },
        "performance": {
          "outputTokensPerSecond": 78.419,
          "timeToFirstToken": 2.384
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5-1",
      "name": "gpt-5.1-high",
      "provider": "3◄─►9",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 87,
        "GPQA Diamond": 87.3,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75,
        "LiveCodeBench": 86.8,
        "SciCode": 43.3,
        "AIME 2025": 94,
        "MMMU Pro": 76,
        "AA-Omniscience": 2,
        "AA-Omniscience Accuracy": 35.3,
        "AA-Omniscience Hallucination Rate": 51.16,
        "LMArena-Text": 1457,
        "LMArena-Text-Creative-Writing": 1437,
        "LMArean-Text-Math": 1474,
        "LMArena-Text-Coding": 1489,
        "LMArena-Text-Expert": 1481,
        "LMArena-Text-Hard-Prompts": 1473,
        "LMArena-Text-Longer-Query": 1465,
        "LMArena-Text-Multi-Turn": 1471
      },
      "metadata": {
        "lmarenaRank": 6,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 111.41,
          "timeToFirstToken": 30.516
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gemini-2-5-pro",
      "name": "gemini-2.5-pro",
      "provider": "6◄─►11",
      "releaseDate": "2025-06-05",
      "benchmarks": {
        "MMLU Pro": 86.2,
        "GPQA Diamond": 84.39999999999999,
        "Humanity's Last Exam": 21.099999999999998,
        "AA-LCR": 66,
        "LiveCodeBench": 80.10000000000001,
        "SciCode": 42.8,
        "AIME 2025": 87.7,
        "MMMU Pro": 75,
        "AA-Omniscience": -18,
        "AA-Omniscience Accuracy": 37.48,
        "AA-Omniscience Hallucination Rate": 88.67,
        "LMArena-Text": 1451,
        "LMArena-Text-Creative-Writing": 1451,
        "LMArean-Text-Math": 1452,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1464,
        "LMArena-Text-Hard-Prompts": 1463,
        "LMArena-Text-Longer-Query": 1463,
        "LMArena-Text-Multi-Turn": 1453
      },
      "metadata": {
        "lmarenaRank": 7,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 152.105,
          "timeToFirstToken": 35.309
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-4-5-sonnet",
      "name": "Anthropicclaude-sonnet-4-5-20250929",
      "provider": "7◄─►14",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 86,
        "GPQA Diamond": 72.7,
        "Humanity's Last Exam": 7.1,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 59,
        "SciCode": 42.8,
        "AIME 2025": 37,
        "MMMU Pro": 65,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 26.93,
        "AA-Omniscience Hallucination Rate": 51.44,
        "LMArena-Text": 1446,
        "LMArena-Text-Creative-Writing": 1439,
        "LMArean-Text-Math": 1429,
        "LMArena-Text-Coding": 1506,
        "LMArena-Text-Expert": 1485,
        "LMArena-Text-Hard-Prompts": 1473,
        "LMArena-Text-Longer-Query": 1476,
        "LMArena-Text-Multi-Turn": 1477
      },
      "metadata": {
        "lmarenaRank": 10,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 72.232,
          "timeToFirstToken": 1.928
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5-1",
      "name": "gpt-5.1",
      "provider": "10◄─►21",
      "releaseDate": "2025-11-13",
      "benchmarks": {
        "MMLU Pro": 87,
        "GPQA Diamond": 87.3,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75,
        "LiveCodeBench": 86.8,
        "SciCode": 43.3,
        "AIME 2025": 94,
        "MMMU Pro": 76,
        "AA-Omniscience": 2,
        "AA-Omniscience Accuracy": 35.3,
        "AA-Omniscience Hallucination Rate": 51.16,
        "LMArena-Text": 1436,
        "LMArena-Text-Creative-Writing": 1409,
        "LMArean-Text-Math": 1417,
        "LMArena-Text-Coding": 1476,
        "LMArena-Text-Expert": 1467,
        "LMArena-Text-Hard-Prompts": 1454,
        "LMArena-Text-Longer-Query": 1446,
        "LMArena-Text-Multi-Turn": 1447
      },
      "metadata": {
        "lmarenaRank": 14,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 111.41,
          "timeToFirstToken": 30.516
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5",
      "name": "gpt-5-high",
      "provider": "11◄─►20",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 87.1,
        "GPQA Diamond": 85.39999999999999,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75.6,
        "LiveCodeBench": 84.6,
        "SciCode": 42.9,
        "AIME 2025": 94.3,
        "MMMU Pro": 74,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 38.62,
        "AA-Omniscience Hallucination Rate": 80.99,
        "LMArena-Text": 1436,
        "LMArena-Text-Creative-Writing": 1381,
        "LMArean-Text-Math": 1439,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1458,
        "LMArena-Text-Hard-Prompts": 1448,
        "LMArena-Text-Longer-Query": 1417,
        "LMArena-Text-Multi-Turn": 1424
      },
      "metadata": {
        "lmarenaRank": 15,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 114.823,
          "timeToFirstToken": 101.819
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o3",
      "name": "o3-2025-04-16",
      "provider": "12◄─►22",
      "releaseDate": "2025-04-16",
      "benchmarks": {
        "MMLU Pro": 85.3,
        "GPQA Diamond": 82.69999999999999,
        "Humanity's Last Exam": 20,
        "AA-LCR": 69.3,
        "LiveCodeBench": 80.80000000000001,
        "SciCode": 41,
        "AIME 2025": 88.3,
        "MMMU Pro": 70,
        "AA-Omniscience": -17,
        "AA-Omniscience Accuracy": 37.25,
        "AA-Omniscience Hallucination Rate": 86.75,
        "LMArena-Text": 1433,
        "LMArena-Text-Creative-Writing": 1383,
        "LMArean-Text-Math": 1455,
        "LMArena-Text-Coding": 1457,
        "LMArena-Text-Expert": 1443,
        "LMArena-Text-Hard-Prompts": 1440,
        "LMArena-Text-Longer-Query": 1410,
        "LMArena-Text-Multi-Turn": 1420
      },
      "metadata": {
        "lmarenaRank": 16,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 305.395,
          "timeToFirstToken": 12.264
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-max-preview",
      "name": "qwen3-max-preview",
      "provider": "11◄─►25",
      "releaseDate": "2025-09-05",
      "benchmarks": {
        "MMLU Pro": 83.8,
        "GPQA Diamond": 76.4,
        "Humanity's Last Exam": 9.3,
        "AA-LCR": 39.7,
        "LiveCodeBench": 65.10000000000001,
        "SciCode": 37,
        "AIME 2025": 75,
        "MMMU Pro": 0,
        "AA-Omniscience": -45,
        "AA-Omniscience Accuracy": 23.35,
        "AA-Omniscience Hallucination Rate": 89.04,
        "LMArena-Text": 1433,
        "LMArena-Text-Creative-Writing": 1396,
        "LMArean-Text-Math": 1442,
        "LMArena-Text-Coding": 1481,
        "LMArena-Text-Expert": 1467,
        "LMArena-Text-Hard-Prompts": 1457,
        "LMArena-Text-Longer-Query": 1447,
        "LMArena-Text-Multi-Turn": 1444
      },
      "metadata": {
        "lmarenaRank": 17,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.2,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 36.712,
          "timeToFirstToken": 1.765
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "grok-4-1-fast-reasoning",
      "name": "grok-4-1-fast-reasoning",
      "provider": "13◄─►32",
      "releaseDate": "2025-11-19",
      "benchmarks": {
        "MMLU Pro": 85.39999999999999,
        "GPQA Diamond": 85.3,
        "Humanity's Last Exam": 17.599999999999998,
        "AA-LCR": 68,
        "LiveCodeBench": 82.19999999999999,
        "SciCode": 44.2,
        "AIME 2025": 89.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1430,
        "LMArena-Text-Creative-Writing": 1407,
        "LMArean-Text-Math": 1410,
        "LMArena-Text-Coding": 1456,
        "LMArena-Text-Expert": 1438,
        "LMArena-Text-Hard-Prompts": 1440,
        "LMArena-Text-Longer-Query": 1416,
        "LMArena-Text-Multi-Turn": 1409
      },
      "metadata": {
        "lmarenaRank": 18,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 167.984,
          "timeToFirstToken": 13.739
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "glm-4-6",
      "name": "glm-4.6",
      "provider": "15◄─►38",
      "releaseDate": "2025-09-30",
      "benchmarks": {
        "MMLU Pro": 78.4,
        "GPQA Diamond": 63.2,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 26.3,
        "LiveCodeBench": 56.10000000000001,
        "SciCode": 33.1,
        "AIME 2025": 44.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1426,
        "LMArena-Text-Creative-Writing": 1400,
        "LMArean-Text-Math": 1430,
        "LMArena-Text-Coding": 1464,
        "LMArena-Text-Expert": 1441,
        "LMArena-Text-Hard-Prompts": 1443,
        "LMArena-Text-Longer-Query": 1435,
        "LMArena-Text-Multi-Turn": 1421
      },
      "metadata": {
        "lmarenaRank": 21,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 2.2
        },
        "performance": {
          "outputTokensPerSecond": 36.33,
          "timeToFirstToken": 0.597
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5",
      "name": "gpt-5-chat",
      "provider": "17◄─►39",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 87.1,
        "GPQA Diamond": 85.39999999999999,
        "Humanity's Last Exam": 26.5,
        "AA-LCR": 75.6,
        "LiveCodeBench": 84.6,
        "SciCode": 42.9,
        "AIME 2025": 94.3,
        "MMMU Pro": 74,
        "AA-Omniscience": -11,
        "AA-Omniscience Accuracy": 38.62,
        "AA-Omniscience Hallucination Rate": 80.99,
        "LMArena-Text": 1425,
        "LMArena-Text-Creative-Writing": 1388,
        "LMArean-Text-Math": 1415,
        "LMArena-Text-Coding": 1459,
        "LMArena-Text-Expert": 1442,
        "LMArena-Text-Hard-Prompts": 1446,
        "LMArena-Text-Longer-Query": 1436,
        "LMArena-Text-Multi-Turn": 1445
      },
      "metadata": {
        "lmarenaRank": 22,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.25,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 114.823,
          "timeToFirstToken": 101.819
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3-2-0925",
      "name": "deepseek-v3.2-exp",
      "provider": "17◄─►39",
      "releaseDate": "2025-09-29",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 73.8,
        "Humanity's Last Exam": 8.6,
        "AA-LCR": 43,
        "LiveCodeBench": 55.400000000000006,
        "SciCode": 39.900000000000006,
        "AIME 2025": 57.699999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1423,
        "LMArena-Text-Creative-Writing": 1400,
        "LMArean-Text-Math": 1418,
        "LMArena-Text-Coding": 1468,
        "LMArena-Text-Expert": 1423,
        "LMArena-Text-Hard-Prompts": 1446,
        "LMArena-Text-Longer-Query": 1441,
        "LMArena-Text-Multi-Turn": 1428
      },
      "metadata": {
        "lmarenaRank": 24,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 29.845,
          "timeToFirstToken": 1.219
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-235b-a22b-instruct-2507",
      "name": "qwen3-235b-a22b-instruct-2507",
      "provider": "18◄─►39",
      "releaseDate": "2025-07-21",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 75.3,
        "Humanity's Last Exam": 10.6,
        "AA-LCR": 31.2,
        "LiveCodeBench": 52.400000000000006,
        "SciCode": 36,
        "AIME 2025": 71.7,
        "MMMU Pro": 0,
        "AA-Omniscience": -45,
        "AA-Omniscience Accuracy": 17.58,
        "AA-Omniscience Hallucination Rate": 76.4,
        "LMArena-Text": 1421,
        "LMArena-Text-Creative-Writing": 1378,
        "LMArean-Text-Math": 1430,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1447,
        "LMArena-Text-Hard-Prompts": 1449,
        "LMArena-Text-Longer-Query": 1437,
        "LMArena-Text-Multi-Turn": 1432
      },
      "metadata": {
        "lmarenaRank": 27,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 41.92,
          "timeToFirstToken": 1.211
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-r1",
      "name": "deepseek-r1-0528",
      "provider": "18◄─►45",
      "releaseDate": "2025-05-28",
      "benchmarks": {
        "MMLU Pro": 84.89999999999999,
        "GPQA Diamond": 81.3,
        "Humanity's Last Exam": 14.899999999999999,
        "AA-LCR": 54.7,
        "LiveCodeBench": 77,
        "SciCode": 40.300000000000004,
        "AIME 2025": 76,
        "MMMU Pro": 0,
        "AA-Omniscience": -30,
        "AA-Omniscience Accuracy": 29.28,
        "AA-Omniscience Hallucination Rate": 83.36,
        "LMArena-Text": 1418,
        "LMArena-Text-Creative-Writing": 1388,
        "LMArean-Text-Math": 1398,
        "LMArena-Text-Coding": 1462,
        "LMArena-Text-Expert": 1412,
        "LMArena-Text-Hard-Prompts": 1432,
        "LMArena-Text-Longer-Query": 1409,
        "LMArena-Text-Multi-Turn": 1405
      },
      "metadata": {
        "lmarenaRank": 30,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.075,
          "output": 3.5
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3-1-reasoning",
      "name": "deepseek-v3.1",
      "provider": "19◄─►46",
      "releaseDate": "2025-08-21",
      "benchmarks": {
        "MMLU Pro": 85.1,
        "GPQA Diamond": 77.9,
        "Humanity's Last Exam": 13,
        "AA-LCR": 53.300000000000004,
        "LiveCodeBench": 78.4,
        "SciCode": 39.1,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1416,
        "LMArena-Text-Creative-Writing": 1393,
        "LMArean-Text-Math": 1418,
        "LMArena-Text-Coding": 1447,
        "LMArena-Text-Expert": 1430,
        "LMArena-Text-Hard-Prompts": 1431,
        "LMArena-Text-Longer-Query": 1425,
        "LMArena-Text-Multi-Turn": 1404
      },
      "metadata": {
        "lmarenaRank": 32,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.58,
          "output": 1.68
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3-2-reasoning",
      "name": "deepseek-v3.2",
      "provider": "19◄─►47",
      "releaseDate": "2025-12-01",
      "benchmarks": {
        "MMLU Pro": 86.2,
        "GPQA Diamond": 84,
        "Humanity's Last Exam": 22.2,
        "AA-LCR": 65,
        "LiveCodeBench": 86.2,
        "SciCode": 38.9,
        "AIME 2025": 92,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1416,
        "LMArena-Text-Creative-Writing": 1389,
        "LMArean-Text-Math": 1428,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1416,
        "LMArena-Text-Hard-Prompts": 1438,
        "LMArena-Text-Longer-Query": 1435,
        "LMArena-Text-Multi-Turn": 1423
      },
      "metadata": {
        "lmarenaRank": 35,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.28,
          "output": 0.42
        },
        "performance": {
          "outputTokensPerSecond": 29.635,
          "timeToFirstToken": 1.258
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3-1-terminus-reasoning",
      "name": "deepseek-v3.1-terminus",
      "provider": "18◄─►50",
      "releaseDate": "2025-09-22",
      "benchmarks": {
        "MMLU Pro": 85.1,
        "GPQA Diamond": 79.2,
        "Humanity's Last Exam": 15.2,
        "AA-LCR": 65,
        "LiveCodeBench": 79.80000000000001,
        "SciCode": 40.6,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1415,
        "LMArena-Text-Creative-Writing": 1404,
        "LMArean-Text-Math": 1399,
        "LMArena-Text-Coding": 1437,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1422,
        "LMArena-Text-Longer-Query": 1416,
        "LMArena-Text-Multi-Turn": 1392
      },
      "metadata": {
        "lmarenaRank": 36,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-vl-235b-a22b-instruct",
      "name": "qwen3-vl-235b-a22b-instruct",
      "provider": "19◄─►47",
      "releaseDate": "2025-09-23",
      "benchmarks": {
        "MMLU Pro": 82.3,
        "GPQA Diamond": 71.2,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 31.7,
        "LiveCodeBench": 59.4,
        "SciCode": 35.9,
        "AIME 2025": 70.7,
        "MMMU Pro": 68,
        "AA-Omniscience": -54,
        "AA-Omniscience Accuracy": 19.22,
        "AA-Omniscience Hallucination Rate": 90.47,
        "LMArena-Text": 1415,
        "LMArena-Text-Creative-Writing": 1362,
        "LMArean-Text-Math": 1414,
        "LMArena-Text-Coding": 1464,
        "LMArena-Text-Expert": 1443,
        "LMArena-Text-Hard-Prompts": 1439,
        "LMArena-Text-Longer-Query": 1424,
        "LMArena-Text-Multi-Turn": 1422
      },
      "metadata": {
        "lmarenaRank": 37,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 37.114,
          "timeToFirstToken": 1.206
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "mistral-large-3",
      "name": "mistral-large-3",
      "provider": "19◄─►49",
      "releaseDate": "2025-12-02",
      "benchmarks": {
        "MMLU Pro": 80.7,
        "GPQA Diamond": 68,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 34.699999999999996,
        "LiveCodeBench": 46.5,
        "SciCode": 36.199999999999996,
        "AIME 2025": 38,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1414,
        "LMArena-Text-Creative-Writing": 1373,
        "LMArean-Text-Math": 1417,
        "LMArena-Text-Coding": 1470,
        "LMArena-Text-Expert": 1400,
        "LMArena-Text-Hard-Prompts": 1437,
        "LMArena-Text-Longer-Query": 1420,
        "LMArena-Text-Multi-Turn": 1419
      },
      "metadata": {
        "lmarenaRank": 38,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 1.5
        },
        "performance": {
          "outputTokensPerSecond": 53.027,
          "timeToFirstToken": 0.526
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4-1",
      "name": "gpt-4.1-2025-04-14",
      "provider": "27◄─►47",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 80.60000000000001,
        "GPQA Diamond": 66.60000000000001,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 61,
        "LiveCodeBench": 45.7,
        "SciCode": 38.1,
        "AIME 2025": 34.699999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1412,
        "LMArena-Text-Creative-Writing": 1399,
        "LMArean-Text-Math": 1375,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1400,
        "LMArena-Text-Hard-Prompts": 1428,
        "LMArena-Text-Longer-Query": 1424,
        "LMArena-Text-Multi-Turn": 1426
      },
      "metadata": {
        "lmarenaRank": 40,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 94.327,
          "timeToFirstToken": 0.572
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "glm-4.5",
      "name": "glm-4.5",
      "provider": "29◄─►53",
      "releaseDate": "2025-07-28",
      "benchmarks": {
        "MMLU Pro": 83.5,
        "GPQA Diamond": 78.2,
        "Humanity's Last Exam": 12.2,
        "AA-LCR": 48.3,
        "LiveCodeBench": 73.8,
        "SciCode": 34.8,
        "AIME 2025": 73.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1408,
        "LMArena-Text-Creative-Writing": 1374,
        "LMArean-Text-Math": 1420,
        "LMArena-Text-Coding": 1454,
        "LMArena-Text-Expert": 1440,
        "LMArena-Text-Hard-Prompts": 1431,
        "LMArena-Text-Longer-Query": 1422,
        "LMArena-Text-Multi-Turn": 1406
      },
      "metadata": {
        "lmarenaRank": 45,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 2.2
        },
        "performance": {
          "outputTokensPerSecond": 39.679,
          "timeToFirstToken": 0.79
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gemini-2-5-flash",
      "name": "gemini-2.5-flash",
      "provider": "31◄─►51",
      "releaseDate": "2025-05-20",
      "benchmarks": {
        "MMLU Pro": 80.9,
        "GPQA Diamond": 68.30000000000001,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 45.9,
        "LiveCodeBench": 49.5,
        "SciCode": 29.099999999999998,
        "AIME 2025": 60.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1408,
        "LMArena-Text-Creative-Writing": 1398,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1421,
        "LMArena-Text-Expert": 1430,
        "LMArena-Text-Hard-Prompts": 1414,
        "LMArena-Text-Longer-Query": 1422,
        "LMArena-Text-Multi-Turn": 1400
      },
      "metadata": {
        "lmarenaRank": 46,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 238.639,
          "timeToFirstToken": 0.378
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gemini-2-5-flash-preview-09-2025",
      "name": "gemini-2.5-flash-preview-09-2025",
      "provider": "35◄─►57",
      "releaseDate": "2025-09-25",
      "benchmarks": {
        "MMLU Pro": 83.6,
        "GPQA Diamond": 76.6,
        "Humanity's Last Exam": 7.8,
        "AA-LCR": 56.699999999999996,
        "LiveCodeBench": 62.5,
        "SciCode": 37.5,
        "AIME 2025": 56.699999999999996,
        "MMMU Pro": 70,
        "AA-Omniscience": -41,
        "AA-Omniscience Accuracy": 25.77,
        "AA-Omniscience Hallucination Rate": 90.37,
        "LMArena-Text": 1405,
        "LMArena-Text-Creative-Writing": 1380,
        "LMArean-Text-Math": 1423,
        "LMArena-Text-Coding": 1424,
        "LMArena-Text-Expert": 1442,
        "LMArena-Text-Hard-Prompts": 1418,
        "LMArena-Text-Longer-Query": 1419,
        "LMArena-Text-Multi-Turn": 1396
      },
      "metadata": {
        "lmarenaRank": 47,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.3,
          "output": 2.5
        },
        "performance": {
          "outputTokensPerSecond": 240.266,
          "timeToFirstToken": 0.362
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-4-5-haiku",
      "name": "Anthropicclaude-haiku-4-5-20251001",
      "provider": "39◄─►58",
      "releaseDate": "2025-10-15",
      "benchmarks": {
        "MMLU Pro": 80,
        "GPQA Diamond": 64.60000000000001,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 43.7,
        "LiveCodeBench": 51.1,
        "SciCode": 34.4,
        "AIME 2025": 39,
        "MMMU Pro": 55,
        "AA-Omniscience": -8,
        "AA-Omniscience Accuracy": 13.42,
        "AA-Omniscience Hallucination Rate": 24.68,
        "LMArena-Text": 1402,
        "LMArena-Text-Creative-Writing": 1375,
        "LMArean-Text-Math": 1396,
        "LMArena-Text-Coding": 1476,
        "LMArena-Text-Expert": 1436,
        "LMArena-Text-Hard-Prompts": 1433,
        "LMArena-Text-Longer-Query": 1434,
        "LMArena-Text-Multi-Turn": 1419
      },
      "metadata": {
        "lmarenaRank": 48,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1,
          "output": 5
        },
        "performance": {
          "outputTokensPerSecond": 109.756,
          "timeToFirstToken": 0.491
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "grok-4-fast-reasoning",
      "name": "grok-4-fast-reasoning",
      "provider": "39◄─►59",
      "releaseDate": "2025-09-19",
      "benchmarks": {
        "MMLU Pro": 85,
        "GPQA Diamond": 84.7,
        "Humanity's Last Exam": 17,
        "AA-LCR": 64.7,
        "LiveCodeBench": 83.2,
        "SciCode": 44.2,
        "AIME 2025": 89.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1402,
        "LMArena-Text-Creative-Writing": 1376,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1438,
        "LMArena-Text-Expert": 1413,
        "LMArena-Text-Hard-Prompts": 1410,
        "LMArena-Text-Longer-Query": 1409,
        "LMArena-Text-Multi-Turn": 1403
      },
      "metadata": {
        "lmarenaRank": 49,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 236.125,
          "timeToFirstToken": 3.686
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-next-80b-a3b-instruct",
      "name": "qwen3-next-80b-a3b-instruct",
      "provider": "43◄─►61",
      "releaseDate": "2025-09-11",
      "benchmarks": {
        "MMLU Pro": 81.89999999999999,
        "GPQA Diamond": 73.8,
        "Humanity's Last Exam": 7.3,
        "AA-LCR": 51.300000000000004,
        "LiveCodeBench": 68.4,
        "SciCode": 30.7,
        "AIME 2025": 66.3,
        "MMMU Pro": 0,
        "AA-Omniscience": -60,
        "AA-Omniscience Accuracy": 16.72,
        "AA-Omniscience Hallucination Rate": 92.7,
        "LMArena-Text": 1400,
        "LMArena-Text-Creative-Writing": 1317,
        "LMArean-Text-Math": 1426,
        "LMArena-Text-Coding": 1446,
        "LMArena-Text-Expert": 1399,
        "LMArena-Text-Hard-Prompts": 1421,
        "LMArena-Text-Longer-Query": 1395,
        "LMArena-Text-Multi-Turn": 1401
      },
      "metadata": {
        "lmarenaRank": 50,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.5,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 179.091,
          "timeToFirstToken": 1.186
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o1",
      "name": "o1-2024-12-17",
      "provider": "45◄─►59",
      "releaseDate": "2024-12-05",
      "benchmarks": {
        "MMLU Pro": 84.1,
        "GPQA Diamond": 74.7,
        "Humanity's Last Exam": 7.7,
        "AA-LCR": 0,
        "LiveCodeBench": 67.9,
        "SciCode": 35.8,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1400,
        "LMArena-Text-Creative-Writing": 1380,
        "LMArean-Text-Math": 1410,
        "LMArena-Text-Coding": 1430,
        "LMArena-Text-Expert": 1398,
        "LMArena-Text-Hard-Prompts": 1415,
        "LMArena-Text-Longer-Query": 1410,
        "LMArena-Text-Multi-Turn": 1383
      },
      "metadata": {
        "lmarenaRank": 51,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 15,
          "output": 60
        },
        "performance": {
          "outputTokensPerSecond": 184.696,
          "timeToFirstToken": 19.237
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-r1",
      "name": "deepseek-r1",
      "provider": "48◄─►66",
      "releaseDate": "2025-05-28",
      "benchmarks": {
        "MMLU Pro": 84.89999999999999,
        "GPQA Diamond": 81.3,
        "Humanity's Last Exam": 14.899999999999999,
        "AA-LCR": 54.7,
        "LiveCodeBench": 77,
        "SciCode": 40.300000000000004,
        "AIME 2025": 76,
        "MMMU Pro": 0,
        "AA-Omniscience": -30,
        "AA-Omniscience Accuracy": 29.28,
        "AA-Omniscience Hallucination Rate": 83.36,
        "LMArena-Text": 1395,
        "LMArena-Text-Creative-Writing": 1372,
        "LMArean-Text-Math": 1413,
        "LMArena-Text-Coding": 1443,
        "LMArena-Text-Expert": 1394,
        "LMArena-Text-Hard-Prompts": 1415,
        "LMArena-Text-Longer-Query": 1397,
        "LMArena-Text-Multi-Turn": 1406
      },
      "metadata": {
        "lmarenaRank": 56,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.075,
          "output": 3.5
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5-mini",
      "name": "gpt-5-mini-high",
      "provider": "49◄─►68",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 83.7,
        "GPQA Diamond": 82.8,
        "Humanity's Last Exam": 19.7,
        "AA-LCR": 68,
        "LiveCodeBench": 83.8,
        "SciCode": 39.2,
        "AIME 2025": 90.7,
        "MMMU Pro": 70,
        "AA-Omniscience": -20,
        "AA-Omniscience Accuracy": 22.97,
        "AA-Omniscience Hallucination Rate": 55.28,
        "LMArena-Text": 1392,
        "LMArena-Text-Creative-Writing": 1329,
        "LMArean-Text-Math": 1415,
        "LMArena-Text-Coding": 1429,
        "LMArena-Text-Expert": 1403,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1375,
        "LMArena-Text-Multi-Turn": 1374
      },
      "metadata": {
        "lmarenaRank": 58,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.25,
          "output": 2
        },
        "performance": {
          "outputTokensPerSecond": 71.154,
          "timeToFirstToken": 108.592
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3-0324",
      "name": "deepseek-v3-0324",
      "provider": "51◄─►67",
      "releaseDate": "2025-03-25",
      "benchmarks": {
        "MMLU Pro": 81.89999999999999,
        "GPQA Diamond": 65.5,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 41,
        "LiveCodeBench": 40.5,
        "SciCode": 35.8,
        "AIME 2025": 41,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1392,
        "LMArena-Text-Creative-Writing": 1388,
        "LMArean-Text-Math": 1375,
        "LMArena-Text-Coding": 1425,
        "LMArena-Text-Expert": 1390,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1392,
        "LMArena-Text-Multi-Turn": 1405
      },
      "metadata": {
        "lmarenaRank": 59,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.14,
          "output": 1.25
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o4-mini",
      "name": "o4-mini-2025-04-16",
      "provider": "53◄─►68",
      "releaseDate": "2025-04-16",
      "benchmarks": {
        "MMLU Pro": 83.2,
        "GPQA Diamond": 78.4,
        "Humanity's Last Exam": 17.5,
        "AA-LCR": 55.00000000000001,
        "LiveCodeBench": 85.9,
        "SciCode": 46.5,
        "AIME 2025": 90.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1391,
        "LMArena-Text-Creative-Writing": 1339,
        "LMArean-Text-Math": 1420,
        "LMArena-Text-Coding": 1430,
        "LMArena-Text-Expert": 1406,
        "LMArena-Text-Hard-Prompts": 1404,
        "LMArena-Text-Longer-Query": 1368,
        "LMArena-Text-Multi-Turn": 1381
      },
      "metadata": {
        "lmarenaRank": 61,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 124.201,
          "timeToFirstToken": 59.397
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o1-preview",
      "name": "o1-preview",
      "provider": "55◄─►72",
      "releaseDate": "2024-09-12",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1387,
        "LMArena-Text-Creative-Writing": 1364,
        "LMArean-Text-Math": 1387,
        "LMArena-Text-Coding": 1414,
        "LMArena-Text-Expert": 1376,
        "LMArena-Text-Hard-Prompts": 1394,
        "LMArena-Text-Longer-Query": 1380,
        "LMArena-Text-Multi-Turn": 1387
      },
      "metadata": {
        "lmarenaRank": 64,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 16.5,
          "output": 66
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-coder-480b-a35b-instruct",
      "name": "qwen3-coder-480b-a35b-instruct",
      "provider": "57◄─►72",
      "releaseDate": "2025-07-22",
      "benchmarks": {
        "MMLU Pro": 78.8,
        "GPQA Diamond": 61.8,
        "Humanity's Last Exam": 4.3999999999999995,
        "AA-LCR": 42.3,
        "LiveCodeBench": 58.5,
        "SciCode": 35.9,
        "AIME 2025": 39.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1385,
        "LMArena-Text-Creative-Writing": 1360,
        "LMArean-Text-Math": 1379,
        "LMArena-Text-Coding": 1453,
        "LMArena-Text-Expert": 1375,
        "LMArena-Text-Hard-Prompts": 1411,
        "LMArena-Text-Longer-Query": 1406,
        "LMArena-Text-Multi-Turn": 1392
      },
      "metadata": {
        "lmarenaRank": 66,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.5,
          "output": 7.5
        },
        "performance": {
          "outputTokensPerSecond": 45.172,
          "timeToFirstToken": 1.681
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4-1-mini",
      "name": "gpt-4.1-mini-2025-04-14",
      "provider": "62◄─►75",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 78.10000000000001,
        "GPQA Diamond": 66.4,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 42.3,
        "LiveCodeBench": 48.3,
        "SciCode": 40.400000000000006,
        "AIME 2025": 46.300000000000004,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1380,
        "LMArena-Text-Creative-Writing": 1344,
        "LMArean-Text-Math": 1357,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1380,
        "LMArena-Text-Hard-Prompts": 1399,
        "LMArena-Text-Longer-Query": 1387,
        "LMArena-Text-Multi-Turn": 1388
      },
      "metadata": {
        "lmarenaRank": 70,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 1.6
        },
        "performance": {
          "outputTokensPerSecond": 80.873,
          "timeToFirstToken": 0.438
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-235b-a22b-instruct-reasoning",
      "name": "qwen3-235b-a22b",
      "provider": "67◄─►80",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 82.8,
        "GPQA Diamond": 70,
        "Humanity's Last Exam": 11.700000000000001,
        "AA-LCR": 0,
        "LiveCodeBench": 62.2,
        "SciCode": 39.900000000000006,
        "AIME 2025": 82,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1374,
        "LMArena-Text-Creative-Writing": 1328,
        "LMArean-Text-Math": 1401,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1381,
        "LMArena-Text-Hard-Prompts": 1390,
        "LMArena-Text-Longer-Query": 1380,
        "LMArena-Text-Multi-Turn": 1368
      },
      "metadata": {
        "lmarenaRank": 74,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 8.4
        },
        "performance": {
          "outputTokensPerSecond": 62.579,
          "timeToFirstToken": 1.184
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen-2-5-max",
      "name": "qwen2.5-max",
      "provider": "69◄─►80",
      "releaseDate": "2025-01-28",
      "benchmarks": {
        "MMLU Pro": 76.2,
        "GPQA Diamond": 58.699999999999996,
        "Humanity's Last Exam": 4.5,
        "AA-LCR": 0,
        "LiveCodeBench": 35.9,
        "SciCode": 33.7,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1372,
        "LMArena-Text-Creative-Writing": 1352,
        "LMArean-Text-Math": 1369,
        "LMArena-Text-Coding": 1400,
        "LMArena-Text-Expert": 1364,
        "LMArena-Text-Hard-Prompts": 1382,
        "LMArena-Text-Longer-Query": 1383,
        "LMArena-Text-Multi-Turn": 1369
      },
      "metadata": {
        "lmarenaRank": 75,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.6,
          "output": 6.4
        },
        "performance": {
          "outputTokensPerSecond": 27.345,
          "timeToFirstToken": 1.205
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "glm-4-5-air",
      "name": "glm-4.5-air",
      "provider": "71◄─►83",
      "releaseDate": "2025-07-28",
      "benchmarks": {
        "MMLU Pro": 81.5,
        "GPQA Diamond": 73.3,
        "Humanity's Last Exam": 6.800000000000001,
        "AA-LCR": 43.7,
        "LiveCodeBench": 68.4,
        "SciCode": 30.599999999999998,
        "AIME 2025": 80.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1370,
        "LMArena-Text-Creative-Writing": 1326,
        "LMArean-Text-Math": 1396,
        "LMArena-Text-Coding": 1426,
        "LMArena-Text-Expert": 1388,
        "LMArena-Text-Hard-Prompts": 1389,
        "LMArena-Text-Longer-Query": 1380,
        "LMArena-Text-Multi-Turn": 1368
      },
      "metadata": {
        "lmarenaRank": 78,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 1.1
        },
        "performance": {
          "outputTokensPerSecond": 104.588,
          "timeToFirstToken": 0.553
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o3-mini-high",
      "name": "o3-mini-high",
      "provider": "77◄─►92",
      "releaseDate": "2025-01-31",
      "benchmarks": {
        "MMLU Pro": 80.2,
        "GPQA Diamond": 77.3,
        "Humanity's Last Exam": 12.3,
        "AA-LCR": 0,
        "LiveCodeBench": 73.4,
        "SciCode": 39.800000000000004,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1362,
        "LMArena-Text-Creative-Writing": 1310,
        "LMArean-Text-Math": 1409,
        "LMArena-Text-Coding": 1432,
        "LMArena-Text-Expert": 1393,
        "LMArena-Text-Hard-Prompts": 1398,
        "LMArena-Text-Longer-Query": 1371,
        "LMArena-Text-Multi-Turn": 1342
      },
      "metadata": {
        "lmarenaRank": 82,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 146.831,
          "timeToFirstToken": 64.935
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v3",
      "name": "deepseek-v3",
      "provider": "80◄─►101",
      "releaseDate": "2024-12-26",
      "benchmarks": {
        "MMLU Pro": 75.2,
        "GPQA Diamond": 55.7,
        "Humanity's Last Exam": 3.5999999999999996,
        "AA-LCR": 28.999999999999996,
        "LiveCodeBench": 35.9,
        "SciCode": 35.4,
        "AIME 2025": 26,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1357,
        "LMArena-Text-Creative-Writing": 1348,
        "LMArean-Text-Math": 1315,
        "LMArena-Text-Coding": 1385,
        "LMArena-Text-Expert": 1343,
        "LMArena-Text-Hard-Prompts": 1347,
        "LMArena-Text-Longer-Query": 1373,
        "LMArena-Text-Multi-Turn": 1369
      },
      "metadata": {
        "lmarenaRank": 85,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.4,
          "output": 0.89
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-oss-120b-low",
      "name": "gpt-oss-120b",
      "provider": "84◄─►107",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 77.5,
        "GPQA Diamond": 67.2,
        "Humanity's Last Exam": 5.2,
        "AA-LCR": 43.7,
        "LiveCodeBench": 70.7,
        "SciCode": 36,
        "AIME 2025": 66.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1352,
        "LMArena-Text-Creative-Writing": 1282,
        "LMArean-Text-Math": 1387,
        "LMArena-Text-Coding": 1388,
        "LMArena-Text-Expert": 1358,
        "LMArena-Text-Hard-Prompts": 1359,
        "LMArena-Text-Longer-Query": 1325,
        "LMArena-Text-Multi-Turn": 1322
      },
      "metadata": {
        "lmarenaRank": 88,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.15,
          "output": 0.595
        },
        "performance": {
          "outputTokensPerSecond": 282.64,
          "timeToFirstToken": 0.555
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "glm-4-5v-reasoning",
      "name": "glm-4.5v",
      "provider": "82◄─►111",
      "releaseDate": "2025-08-11",
      "benchmarks": {
        "MMLU Pro": 78.8,
        "GPQA Diamond": 68.4,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 60.4,
        "SciCode": 22.1,
        "AIME 2025": 73,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1352,
        "LMArena-Text-Creative-Writing": 1307,
        "LMArean-Text-Math": 1366,
        "LMArena-Text-Coding": 1402,
        "LMArena-Text-Expert": 1403,
        "LMArena-Text-Hard-Prompts": 1370,
        "LMArena-Text-Longer-Query": 1340,
        "LMArena-Text-Multi-Turn": 1354
      },
      "metadata": {
        "lmarenaRank": 90,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.6,
          "output": 1.8
        },
        "performance": {
          "outputTokensPerSecond": 35.435,
          "timeToFirstToken": 0.678
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o3-mini",
      "name": "o3-mini",
      "provider": "87◄─►111",
      "releaseDate": "2025-01-31",
      "benchmarks": {
        "MMLU Pro": 79.10000000000001,
        "GPQA Diamond": 74.8,
        "Humanity's Last Exam": 8.7,
        "AA-LCR": 0,
        "LiveCodeBench": 71.7,
        "SciCode": 39.900000000000006,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1347,
        "LMArena-Text-Creative-Writing": 1301,
        "LMArean-Text-Math": 1386,
        "LMArena-Text-Coding": 1413,
        "LMArena-Text-Expert": 1362,
        "LMArena-Text-Hard-Prompts": 1368,
        "LMArena-Text-Longer-Query": 1358,
        "LMArena-Text-Multi-Turn": 1336
      },
      "metadata": {
        "lmarenaRank": 95,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 1.1,
          "output": 4.4
        },
        "performance": {
          "outputTokensPerSecond": 149.286,
          "timeToFirstToken": 16.271
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "ling-flash-2-0",
      "name": "ling-flash-2.0",
      "provider": "85◄─►113",
      "releaseDate": "2025-09-17",
      "benchmarks": {
        "MMLU Pro": 77.7,
        "GPQA Diamond": 65.7,
        "Humanity's Last Exam": 6.3,
        "AA-LCR": 15,
        "LiveCodeBench": 58.9,
        "SciCode": 28.9,
        "AIME 2025": 65.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1346,
        "LMArena-Text-Creative-Writing": 1272,
        "LMArean-Text-Math": 1359,
        "LMArena-Text-Coding": 1412,
        "LMArena-Text-Expert": 1359,
        "LMArena-Text-Hard-Prompts": 1365,
        "LMArena-Text-Longer-Query": 1329,
        "LMArena-Text-Multi-Turn": 1316
      },
      "metadata": {
        "lmarenaRank": 96,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.14,
          "output": 0.57
        },
        "performance": {
          "outputTokensPerSecond": 49.706,
          "timeToFirstToken": 1.575
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-32b-instruct",
      "name": "qwen3-32b",
      "provider": "85◄─►121",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 72.7,
        "GPQA Diamond": 53.5,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 0,
        "LiveCodeBench": 28.799999999999997,
        "SciCode": 28.000000000000004,
        "AIME 2025": 19.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1345,
        "LMArena-Text-Creative-Writing": 1304,
        "LMArean-Text-Math": 1403,
        "LMArena-Text-Coding": 1405,
        "LMArena-Text-Expert": 1395,
        "LMArena-Text-Hard-Prompts": 1363,
        "LMArena-Text-Longer-Query": 1355,
        "LMArena-Text-Multi-Turn": 1334
      },
      "metadata": {
        "lmarenaRank": 102,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.7,
          "output": 2.8
        },
        "performance": {
          "outputTokensPerSecond": 90.582,
          "timeToFirstToken": 1.117
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4o-2024-05-13",
      "name": "gpt-4o-2024-05-13",
      "provider": "91◄─►111",
      "releaseDate": "2024-05-13",
      "benchmarks": {
        "MMLU Pro": 74,
        "GPQA Diamond": 52.6,
        "Humanity's Last Exam": 2.8000000000000003,
        "AA-LCR": 0,
        "LiveCodeBench": 33.4,
        "SciCode": 30.9,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1344,
        "LMArena-Text-Creative-Writing": 1335,
        "LMArean-Text-Math": 1307,
        "LMArena-Text-Coding": 1367,
        "LMArena-Text-Expert": 1306,
        "LMArena-Text-Hard-Prompts": 1335,
        "LMArena-Text-Longer-Query": 1329,
        "LMArena-Text-Multi-Turn": 1336
      },
      "metadata": {
        "lmarenaRank": 103,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 5,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 118.14,
          "timeToFirstToken": 0.581
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-5-nano",
      "name": "gpt-5-nano-high",
      "provider": "92◄─►123",
      "releaseDate": "2025-08-07",
      "benchmarks": {
        "MMLU Pro": 78,
        "GPQA Diamond": 67.60000000000001,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 41.699999999999996,
        "LiveCodeBench": 78.9,
        "SciCode": 36.6,
        "AIME 2025": 83.7,
        "MMMU Pro": 61,
        "AA-Omniscience": -30,
        "AA-Omniscience Accuracy": 18.28,
        "AA-Omniscience Hallucination Rate": 58.66,
        "LMArena-Text": 1338,
        "LMArena-Text-Creative-Writing": 1252,
        "LMArean-Text-Math": 1356,
        "LMArena-Text-Coding": 1381,
        "LMArena-Text-Expert": 1357,
        "LMArena-Text-Hard-Prompts": 1353,
        "LMArena-Text-Longer-Query": 1332,
        "LMArena-Text-Multi-Turn": 1328
      },
      "metadata": {
        "lmarenaRank": 109,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.05,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 127.927,
          "timeToFirstToken": 104.016
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "o1-mini",
      "name": "o1-mini",
      "provider": "98◄─►123",
      "releaseDate": "2024-09-12",
      "benchmarks": {
        "MMLU Pro": 74.2,
        "GPQA Diamond": 60.3,
        "Humanity's Last Exam": 4.9,
        "AA-LCR": 0,
        "LiveCodeBench": 57.599999999999994,
        "SciCode": 32.300000000000004,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1335,
        "LMArena-Text-Creative-Writing": 1279,
        "LMArean-Text-Math": 1364,
        "LMArena-Text-Coding": 1385,
        "LMArena-Text-Expert": 1347,
        "LMArena-Text-Hard-Prompts": 1357,
        "LMArena-Text-Longer-Query": 1343,
        "LMArena-Text-Multi-Turn": 1319
      },
      "metadata": {
        "lmarenaRank": 112,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4o-2024-08-06",
      "name": "gpt-4o-2024-08-06",
      "provider": "101◄─►126",
      "releaseDate": "2024-08-06",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 52.1,
        "Humanity's Last Exam": 2.9000000000000004,
        "AA-LCR": 0,
        "LiveCodeBench": 31.7,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1334,
        "LMArena-Text-Creative-Writing": 1324,
        "LMArean-Text-Math": 1310,
        "LMArena-Text-Coding": 1358,
        "LMArena-Text-Expert": 1305,
        "LMArena-Text-Hard-Prompts": 1324,
        "LMArena-Text-Longer-Query": 1336,
        "LMArena-Text-Multi-Turn": 1323
      },
      "metadata": {
        "lmarenaRank": 114,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2.5,
          "output": 10
        },
        "performance": {
          "outputTokensPerSecond": 121.814,
          "timeToFirstToken": 0.591
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwq-32b",
      "name": "qwq-32b",
      "provider": "101◄─►126",
      "releaseDate": "2025-03-05",
      "benchmarks": {
        "MMLU Pro": 76.4,
        "GPQA Diamond": 59.3,
        "Humanity's Last Exam": 8.200000000000001,
        "AA-LCR": 25,
        "LiveCodeBench": 63.1,
        "SciCode": 35.8,
        "AIME 2025": 28.999999999999996,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1334,
        "LMArena-Text-Creative-Writing": 1294,
        "LMArean-Text-Math": 1365,
        "LMArena-Text-Coding": 1384,
        "LMArena-Text-Expert": 1357,
        "LMArena-Text-Hard-Prompts": 1354,
        "LMArena-Text-Longer-Query": 1336,
        "LMArena-Text-Multi-Turn": 1319
      },
      "metadata": {
        "lmarenaRank": 115,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.43,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 30.217,
          "timeToFirstToken": 1.376
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen3-30b-a3b-instruct-reasoning",
      "name": "qwen3-30b-a3b",
      "provider": "111◄─►140",
      "releaseDate": "2025-04-28",
      "benchmarks": {
        "MMLU Pro": 77.7,
        "GPQA Diamond": 61.6,
        "Humanity's Last Exam": 6.6000000000000005,
        "AA-LCR": 0,
        "LiveCodeBench": 50.6,
        "SciCode": 28.499999999999996,
        "AIME 2025": 72.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1325,
        "LMArena-Text-Creative-Writing": 1284,
        "LMArean-Text-Math": 1360,
        "LMArena-Text-Coding": 1384,
        "LMArena-Text-Expert": 1342,
        "LMArena-Text-Hard-Prompts": 1342,
        "LMArena-Text-Longer-Query": 1340,
        "LMArena-Text-Multi-Turn": 1318
      },
      "metadata": {
        "lmarenaRank": 123,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 2.4
        },
        "performance": {
          "outputTokensPerSecond": 88.14,
          "timeToFirstToken": 1.138
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4-turbo",
      "name": "gpt-4-turbo-2024-04-09",
      "provider": "119◄─►144",
      "releaseDate": "2023-11-06",
      "benchmarks": {
        "MMLU Pro": 69.39999999999999,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 3.3000000000000003,
        "AA-LCR": 0,
        "LiveCodeBench": 29.099999999999998,
        "SciCode": 31.900000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1323,
        "LMArena-Text-Creative-Writing": 1321,
        "LMArean-Text-Math": 1298,
        "LMArena-Text-Coding": 1344,
        "LMArena-Text-Expert": 1291,
        "LMArena-Text-Hard-Prompts": 1314,
        "LMArena-Text-Longer-Query": 1313,
        "LMArena-Text-Multi-Turn": 1309
      },
      "metadata": {
        "lmarenaRank": 126,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 10,
          "output": 30
        },
        "performance": {
          "outputTokensPerSecond": 37.193,
          "timeToFirstToken": 0.789
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-3-5-haiku",
      "name": "Anthropicclaude-3-5-haiku-20241022",
      "provider": "119◄─►144",
      "releaseDate": "2024-10-22",
      "benchmarks": {
        "MMLU Pro": 63.4,
        "GPQA Diamond": 40.8,
        "Humanity's Last Exam": 3.5000000000000004,
        "AA-LCR": 0,
        "LiveCodeBench": 31.4,
        "SciCode": 27.400000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1322,
        "LMArena-Text-Creative-Writing": 1299,
        "LMArean-Text-Math": 1286,
        "LMArena-Text-Coding": 1383,
        "LMArena-Text-Expert": 1307,
        "LMArena-Text-Hard-Prompts": 1341,
        "LMArena-Text-Longer-Query": 1341,
        "LMArena-Text-Multi-Turn": 1323
      },
      "metadata": {
        "lmarenaRank": 128,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.8,
          "output": 4
        },
        "performance": {
          "outputTokensPerSecond": 49.021,
          "timeToFirstToken": 0.721
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v2-5",
      "name": "deepseek-v2.5-1210",
      "provider": "112◄─►149",
      "releaseDate": "2024-12-10",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1322,
        "LMArena-Text-Creative-Writing": 1307,
        "LMArean-Text-Math": 1296,
        "LMArena-Text-Coding": 1372,
        "LMArena-Text-Expert": 1302,
        "LMArena-Text-Hard-Prompts": 1326,
        "LMArena-Text-Longer-Query": 1335,
        "LMArena-Text-Multi-Turn": 1319
      },
      "metadata": {
        "lmarenaRank": 129,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-3-opus",
      "name": "Anthropicclaude-3-opus-20240229",
      "provider": "119◄─►144",
      "releaseDate": "2024-03-04",
      "benchmarks": {
        "MMLU Pro": 69.6,
        "GPQA Diamond": 48.9,
        "Humanity's Last Exam": 3.1,
        "AA-LCR": 0,
        "LiveCodeBench": 27.900000000000002,
        "SciCode": 23.3,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1321,
        "LMArena-Text-Creative-Writing": 1286,
        "LMArean-Text-Math": 1312,
        "LMArena-Text-Coding": 1351,
        "LMArena-Text-Expert": 1311,
        "LMArena-Text-Hard-Prompts": 1326,
        "LMArena-Text-Longer-Query": 1325,
        "LMArena-Text-Multi-Turn": 1320
      },
      "metadata": {
        "lmarenaRank": 130,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 15,
          "output": 75
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4-1-nano",
      "name": "gpt-4.1-nano-2025-04-14",
      "provider": "117◄─►149",
      "releaseDate": "2025-04-14",
      "benchmarks": {
        "MMLU Pro": 65.7,
        "GPQA Diamond": 51.2,
        "Humanity's Last Exam": 3.9,
        "AA-LCR": 17,
        "LiveCodeBench": 32.6,
        "SciCode": 25.900000000000002,
        "AIME 2025": 24,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1320,
        "LMArena-Text-Creative-Writing": 1306,
        "LMArean-Text-Math": 1278,
        "LMArena-Text-Coding": 1372,
        "LMArena-Text-Expert": 1310,
        "LMArena-Text-Hard-Prompts": 1329,
        "LMArena-Text-Longer-Query": 1326,
        "LMArena-Text-Multi-Turn": 1311
      },
      "metadata": {
        "lmarenaRank": 132,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.1,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 140.379,
          "timeToFirstToken": 0.418
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "ring-flash-2-0",
      "name": "ring-flash-2.0",
      "provider": "119◄─►149",
      "releaseDate": "2025-09-19",
      "benchmarks": {
        "MMLU Pro": 79.3,
        "GPQA Diamond": 72.5,
        "Humanity's Last Exam": 8.9,
        "AA-LCR": 21,
        "LiveCodeBench": 62.8,
        "SciCode": 16.8,
        "AIME 2025": 83.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1319,
        "LMArena-Text-Creative-Writing": 1257,
        "LMArean-Text-Math": 1354,
        "LMArena-Text-Coding": 1390,
        "LMArena-Text-Expert": 1358,
        "LMArena-Text-Hard-Prompts": 1350,
        "LMArena-Text-Longer-Query": 1332,
        "LMArena-Text-Multi-Turn": 1281
      },
      "metadata": {
        "lmarenaRank": 133,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.14,
          "output": 0.57
        },
        "performance": {
          "outputTokensPerSecond": 78.495,
          "timeToFirstToken": 1.565
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-oss-20b",
      "name": "gpt-oss-20b",
      "provider": "119◄─►150",
      "releaseDate": "2025-08-05",
      "benchmarks": {
        "MMLU Pro": 74.8,
        "GPQA Diamond": 68.8,
        "Humanity's Last Exam": 9.8,
        "AA-LCR": 30.7,
        "LiveCodeBench": 77.7,
        "SciCode": 34.4,
        "AIME 2025": 89.3,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1318,
        "LMArena-Text-Creative-Writing": 1238,
        "LMArean-Text-Math": 1338,
        "LMArena-Text-Coding": 1369,
        "LMArena-Text-Expert": 1317,
        "LMArena-Text-Hard-Prompts": 1321,
        "LMArena-Text-Longer-Query": 1304,
        "LMArena-Text-Multi-Turn": 1288
      },
      "metadata": {
        "lmarenaRank": 137,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.07,
          "output": 0.2
        },
        "performance": {
          "outputTokensPerSecond": 313.01,
          "timeToFirstToken": 0.478
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gpt-4o-mini",
      "name": "gpt-4o-mini-2024-07-18",
      "provider": "123◄─►149",
      "releaseDate": "2024-07-18",
      "benchmarks": {
        "MMLU Pro": 64.8,
        "GPQA Diamond": 42.6,
        "Humanity's Last Exam": 4,
        "AA-LCR": 0,
        "LiveCodeBench": 23.400000000000002,
        "SciCode": 22.900000000000002,
        "AIME 2025": 14.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1316,
        "LMArena-Text-Creative-Writing": 1293,
        "LMArean-Text-Math": 1279,
        "LMArena-Text-Coding": 1347,
        "LMArena-Text-Expert": 1279,
        "LMArena-Text-Hard-Prompts": 1308,
        "LMArena-Text-Longer-Query": 1323,
        "LMArena-Text-Multi-Turn": 1306
      },
      "metadata": {
        "lmarenaRank": 140,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.15,
          "output": 0.6
        },
        "performance": {
          "outputTokensPerSecond": 50.102,
          "timeToFirstToken": 0.656
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "mistral-large-2407",
      "name": "mistral-large-2407",
      "provider": "129◄─►154",
      "releaseDate": "2024-07-24",
      "benchmarks": {
        "MMLU Pro": 68.30000000000001,
        "GPQA Diamond": 47.199999999999996,
        "Humanity's Last Exam": 3.2,
        "AA-LCR": 1.7000000000000002,
        "LiveCodeBench": 26.700000000000003,
        "SciCode": 27.1,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1313,
        "LMArena-Text-Creative-Writing": 1286,
        "LMArean-Text-Math": 1290,
        "LMArena-Text-Coding": 1350,
        "LMArena-Text-Expert": 1295,
        "LMArena-Text-Hard-Prompts": 1317,
        "LMArena-Text-Longer-Query": 1302,
        "LMArena-Text-Multi-Turn": 1293
      },
      "metadata": {
        "lmarenaRank": 143,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 6
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "olmo-3-32b-think",
      "name": "olmo-3-32b-think",
      "provider": "123◄─►159",
      "releaseDate": "2025-11-20",
      "benchmarks": {
        "MMLU Pro": 75.9,
        "GPQA Diamond": 61,
        "Humanity's Last Exam": 5.8999999999999995,
        "AA-LCR": 0,
        "LiveCodeBench": 67.2,
        "SciCode": 28.599999999999998,
        "AIME 2025": 73.7,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1310,
        "LMArena-Text-Creative-Writing": 1255,
        "LMArean-Text-Math": 1326,
        "LMArena-Text-Coding": 1363,
        "LMArena-Text-Expert": 1272,
        "LMArena-Text-Hard-Prompts": 1327,
        "LMArena-Text-Longer-Query": 1320,
        "LMArena-Text-Multi-Turn": 1293
      },
      "metadata": {
        "lmarenaRank": 147,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.35
        },
        "performance": {
          "outputTokensPerSecond": 53.402,
          "timeToFirstToken": 0.45
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-v2-5",
      "name": "deepseek-v2.5",
      "provider": "141◄─►159",
      "releaseDate": "2024-12-10",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1305,
        "LMArena-Text-Creative-Writing": 1264,
        "LMArean-Text-Math": 1290,
        "LMArena-Text-Coding": 1365,
        "LMArena-Text-Expert": 1291,
        "LMArena-Text-Hard-Prompts": 1319,
        "LMArena-Text-Longer-Query": 1312,
        "LMArena-Text-Multi-Turn": 1289
      },
      "metadata": {
        "lmarenaRank": 151,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen2-5-72b-instruct",
      "name": "qwen2.5-72b-instruct",
      "provider": "147◄─►159",
      "releaseDate": "2024-09-19",
      "benchmarks": {
        "MMLU Pro": 72,
        "GPQA Diamond": 49.1,
        "Humanity's Last Exam": 4.2,
        "AA-LCR": 20.3,
        "LiveCodeBench": 27.6,
        "SciCode": 26.700000000000003,
        "AIME 2025": 14.000000000000002,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1301,
        "LMArena-Text-Creative-Writing": 1252,
        "LMArean-Text-Math": 1299,
        "LMArena-Text-Coding": 1353,
        "LMArena-Text-Expert": 1291,
        "LMArena-Text-Hard-Prompts": 1315,
        "LMArena-Text-Longer-Query": 1315,
        "LMArena-Text-Multi-Turn": 1295
      },
      "metadata": {
        "lmarenaRank": 157,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 45.699,
          "timeToFirstToken": 1.174
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "jamba-1-5-large",
      "name": "jamba-1.5-large",
      "provider": "157◄─►172",
      "releaseDate": "2024-08-22",
      "benchmarks": {
        "MMLU Pro": 57.199999999999996,
        "GPQA Diamond": 42.699999999999996,
        "Humanity's Last Exam": 4,
        "AA-LCR": 0,
        "LiveCodeBench": 14.299999999999999,
        "SciCode": 16.3,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1287,
        "LMArena-Text-Creative-Writing": 1259,
        "LMArean-Text-Math": 1247,
        "LMArena-Text-Coding": 1309,
        "LMArena-Text-Expert": 1279,
        "LMArena-Text-Hard-Prompts": 1281,
        "LMArena-Text-Longer-Query": 1259,
        "LMArena-Text-Multi-Turn": 1263
      },
      "metadata": {
        "lmarenaRank": 162,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2,
          "output": 8
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-3-sonnet",
      "name": "Anthropicclaude-3-sonnet-20240229",
      "provider": "161◄─►178",
      "releaseDate": "2024-03-04",
      "benchmarks": {
        "MMLU Pro": 57.9,
        "GPQA Diamond": 40,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 0,
        "LiveCodeBench": 17.5,
        "SciCode": 22.900000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1280,
        "LMArena-Text-Creative-Writing": 1244,
        "LMArean-Text-Math": 1255,
        "LMArena-Text-Coding": 1314,
        "LMArena-Text-Expert": 1264,
        "LMArena-Text-Hard-Prompts": 1280,
        "LMArena-Text-Longer-Query": 1282,
        "LMArena-Text-Multi-Turn": 1276
      },
      "metadata": {
        "lmarenaRank": 169,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 3,
          "output": 15
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen2-5-coder-32b-instruct",
      "name": "qwen2.5-coder-32b-instruct",
      "provider": "167◄─►187",
      "releaseDate": "2024-11-11",
      "benchmarks": {
        "MMLU Pro": 63.5,
        "GPQA Diamond": 41.699999999999996,
        "Humanity's Last Exam": 3.8,
        "AA-LCR": 0,
        "LiveCodeBench": 29.5,
        "SciCode": 27.1,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1269,
        "LMArena-Text-Creative-Writing": 1202,
        "LMArean-Text-Math": 1272,
        "LMArena-Text-Coding": 1340,
        "LMArena-Text-Expert": 1272,
        "LMArena-Text-Hard-Prompts": 1301,
        "LMArena-Text-Longer-Query": 1284,
        "LMArena-Text-Multi-Turn": 1247
      },
      "metadata": {
        "lmarenaRank": 178,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.13,
          "output": 0.175
        },
        "performance": {
          "outputTokensPerSecond": 43.205,
          "timeToFirstToken": 1.581
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-coder-v2",
      "name": "deepseek-coder-v2",
      "provider": "175◄─►189",
      "releaseDate": "2024-06-17",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1263,
        "LMArena-Text-Creative-Writing": 1202,
        "LMArean-Text-Math": 1271,
        "LMArena-Text-Coding": 1339,
        "LMArena-Text-Expert": 1260,
        "LMArena-Text-Hard-Prompts": 1285,
        "LMArena-Text-Longer-Query": 1283,
        "LMArena-Text-Multi-Turn": 1233
      },
      "metadata": {
        "lmarenaRank": 181,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwen2-72b-instruct",
      "name": "qwen2-72b-instruct",
      "provider": "176◄─►189",
      "releaseDate": "2024-06-07",
      "benchmarks": {
        "MMLU Pro": 62.2,
        "GPQA Diamond": 37.1,
        "Humanity's Last Exam": 3.6999999999999997,
        "AA-LCR": 0,
        "LiveCodeBench": 15.9,
        "SciCode": 22.900000000000002,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1261,
        "LMArena-Text-Creative-Writing": 1222,
        "LMArean-Text-Math": 1275,
        "LMArena-Text-Coding": 1292,
        "LMArena-Text-Expert": 1253,
        "LMArena-Text-Hard-Prompts": 1270,
        "LMArena-Text-Longer-Query": 1257,
        "LMArena-Text-Multi-Turn": 1241
      },
      "metadata": {
        "lmarenaRank": 183,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "claude-3-haiku",
      "name": "Anthropicclaude-3-haiku-20240307",
      "provider": "178◄─►189",
      "releaseDate": "2024-03-04",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 15.4,
        "SciCode": 18.6,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1260,
        "LMArena-Text-Creative-Writing": 1213,
        "LMArean-Text-Math": 1231,
        "LMArena-Text-Coding": 1297,
        "LMArena-Text-Expert": 1244,
        "LMArena-Text-Hard-Prompts": 1263,
        "LMArena-Text-Longer-Query": 1262,
        "LMArena-Text-Multi-Turn": 1243
      },
      "metadata": {
        "lmarenaRank": 184,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.25,
          "output": 1.25
        },
        "performance": {
          "outputTokensPerSecond": 105.003,
          "timeToFirstToken": 0.436
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "phi-4",
      "name": "Azurephi-4",
      "provider": "181◄─►189",
      "releaseDate": "2024-12-12",
      "benchmarks": {
        "MMLU Pro": 71.39999999999999,
        "GPQA Diamond": 57.49999999999999,
        "Humanity's Last Exam": 4.1000000000000005,
        "AA-LCR": 0,
        "LiveCodeBench": 23.1,
        "SciCode": 26,
        "AIME 2025": 18,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1255,
        "LMArena-Text-Creative-Writing": 1209,
        "LMArean-Text-Math": 1267,
        "LMArena-Text-Coding": 1304,
        "LMArena-Text-Expert": 1262,
        "LMArena-Text-Hard-Prompts": 1275,
        "LMArena-Text-Longer-Query": 1264,
        "LMArena-Text-Multi-Turn": 1239
      },
      "metadata": {
        "lmarenaRank": 187,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.125,
          "output": 0.5
        },
        "performance": {
          "outputTokensPerSecond": 9.533,
          "timeToFirstToken": 0.55
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "jamba-1-5-mini",
      "name": "jamba-1.5-mini",
      "provider": "188◄─►203",
      "releaseDate": "2024-08-22",
      "benchmarks": {
        "MMLU Pro": 37.1,
        "GPQA Diamond": 30.2,
        "Humanity's Last Exam": 5.1,
        "AA-LCR": 0,
        "LiveCodeBench": 6.2,
        "SciCode": 8,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1238,
        "LMArena-Text-Creative-Writing": 1209,
        "LMArean-Text-Math": 1188,
        "LMArena-Text-Coding": 1263,
        "LMArena-Text-Expert": 1205,
        "LMArena-Text-Hard-Prompts": 1234,
        "LMArena-Text-Longer-Query": 1214,
        "LMArena-Text-Multi-Turn": 1203
      },
      "metadata": {
        "lmarenaRank": 192,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.2,
          "output": 0.4
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gemini-1-0-pro",
      "name": "gemini-pro-dev-api",
      "provider": "190◄─►207",
      "releaseDate": "2023-12-06",
      "benchmarks": {
        "MMLU Pro": 43.1,
        "GPQA Diamond": 27.700000000000003,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 0,
        "LiveCodeBench": 11.600000000000001,
        "SciCode": 11.700000000000001,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1233,
        "LMArena-Text-Creative-Writing": 1200,
        "LMArean-Text-Math": 1192,
        "LMArena-Text-Coding": 1236,
        "LMArena-Text-Expert": 1170,
        "LMArena-Text-Hard-Prompts": 1229,
        "LMArena-Text-Longer-Query": 1219,
        "LMArena-Text-Multi-Turn": 1184
      },
      "metadata": {
        "lmarenaRank": 195,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "mistral-medium",
      "name": "mistral-medium",
      "provider": "194◄─►210",
      "releaseDate": "2023-12-11",
      "benchmarks": {
        "MMLU Pro": 49.1,
        "GPQA Diamond": 34.9,
        "Humanity's Last Exam": 3.4000000000000004,
        "AA-LCR": 0,
        "LiveCodeBench": 9.9,
        "SciCode": 11.799999999999999,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1222,
        "LMArena-Text-Creative-Writing": 1193,
        "LMArean-Text-Math": 1222,
        "LMArena-Text-Coding": 1257,
        "LMArena-Text-Expert": 1219,
        "LMArena-Text-Hard-Prompts": 1233,
        "LMArena-Text-Longer-Query": 1217,
        "LMArena-Text-Multi-Turn": 1194
      },
      "metadata": {
        "lmarenaRank": 205,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 2.75,
          "output": 8.1
        },
        "performance": {
          "outputTokensPerSecond": 76.194,
          "timeToFirstToken": 0.409
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "gemini-1-0-pro",
      "name": "gemini-pro",
      "provider": "192◄─►213",
      "releaseDate": "2023-12-06",
      "benchmarks": {
        "MMLU Pro": 43.1,
        "GPQA Diamond": 27.700000000000003,
        "Humanity's Last Exam": 4.6,
        "AA-LCR": 0,
        "LiveCodeBench": 11.600000000000001,
        "SciCode": 11.700000000000001,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1220,
        "LMArena-Text-Creative-Writing": 1181,
        "LMArean-Text-Math": 1196,
        "LMArena-Text-Coding": 1245,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1226,
        "LMArena-Text-Longer-Query": 0,
        "LMArena-Text-Multi-Turn": 1198
      },
      "metadata": {
        "lmarenaRank": 207,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "deepseek-llm-67b-chat",
      "name": "deepseek-llm-67b-chat",
      "provider": "213◄─►237",
      "releaseDate": "2023-11-29",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1182,
        "LMArena-Text-Creative-Writing": 1131,
        "LMArean-Text-Math": 1156,
        "LMArena-Text-Coding": 1213,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1173,
        "LMArena-Text-Longer-Query": 1178,
        "LMArena-Text-Multi-Turn": 1150
      },
      "metadata": {
        "lmarenaRank": 223,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "qwq-32b-preview",
      "name": "qwq-32b-preview",
      "provider": "228◄─►251",
      "releaseDate": "2024-11-27",
      "benchmarks": {
        "MMLU Pro": 64.8,
        "GPQA Diamond": 55.7,
        "Humanity's Last Exam": 4.8,
        "AA-LCR": 0,
        "LiveCodeBench": 33.7,
        "SciCode": 3.8,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1158,
        "LMArena-Text-Creative-Writing": 1107,
        "LMArean-Text-Math": 1220,
        "LMArena-Text-Coding": 1171,
        "LMArena-Text-Expert": 1151,
        "LMArena-Text-Hard-Prompts": 1167,
        "LMArena-Text-Longer-Query": 1178,
        "LMArena-Text-Multi-Turn": 1146
      },
      "metadata": {
        "lmarenaRank": 238,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.12,
          "output": 0.18
        },
        "performance": {
          "outputTokensPerSecond": 100.132,
          "timeToFirstToken": 0.235
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "palm-2",
      "name": "palm-2",
      "provider": "240◄─►262",
      "releaseDate": "2023-05-10",
      "benchmarks": {
        "MMLU Pro": 0,
        "GPQA Diamond": 0,
        "Humanity's Last Exam": 0,
        "AA-LCR": 0,
        "LiveCodeBench": 0,
        "SciCode": 0,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1135,
        "LMArena-Text-Creative-Writing": 1085,
        "LMArean-Text-Math": 1113,
        "LMArena-Text-Coding": 1149,
        "LMArena-Text-Expert": 0,
        "LMArena-Text-Hard-Prompts": 1142,
        "LMArena-Text-Longer-Query": 1131,
        "LMArena-Text-Multi-Turn": 1088
      },
      "metadata": {
        "lmarenaRank": 253,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0,
          "output": 0
        },
        "performance": {
          "outputTokensPerSecond": 0,
          "timeToFirstToken": 0
        }
      },
      "tags": [],
      "description": ""
    },
    {
      "id": "mistral-7b-instruct",
      "name": "mistral-7b-instruct",
      "provider": "258◄─►268",
      "releaseDate": "2023-09-27",
      "benchmarks": {
        "MMLU Pro": 24.5,
        "GPQA Diamond": 17.7,
        "Humanity's Last Exam": 4.3,
        "AA-LCR": 0,
        "LiveCodeBench": 4.6,
        "SciCode": 2.4,
        "AIME 2025": 0,
        "MMMU Pro": 0,
        "AA-Omniscience": 0,
        "AA-Omniscience Accuracy": 0,
        "AA-Omniscience Hallucination Rate": 0,
        "LMArena-Text": 1109,
        "LMArena-Text-Creative-Writing": 1091,
        "LMArean-Text-Math": 1082,
        "LMArena-Text-Coding": 1140,
        "LMArena-Text-Expert": 1074,
        "LMArena-Text-Hard-Prompts": 1111,
        "LMArena-Text-Longer-Query": 1092,
        "LMArena-Text-Multi-Turn": 1080
      },
      "metadata": {
        "lmarenaRank": 266,
        "lmarenaVotes": 0,
        "pricing": {
          "input": 0.25,
          "output": 0.25
        },
        "performance": {
          "outputTokensPerSecond": 125.067,
          "timeToFirstToken": 0.346
        }
      },
      "tags": [],
      "description": ""
    }
  ]
}