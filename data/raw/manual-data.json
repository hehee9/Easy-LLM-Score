{
  "models": {
    "gpt-5-1": {
      "MMMU Pro": 76,
      "AA-Omniscience": 2,
      "AA-Omniscience Accuracy": 35.30,
      "AA-Omniscience Hallucination Rate": 51.16
    },
    "gpt-5": {
      "MMMU Pro": 74,
      "AA-Omniscience": -11,
      "AA-Omniscience Accuracy": 38.62,
      "AA-Omniscience Hallucination Rate": 80.99
    },
    "gpt-5-mini": {
      "MMMU Pro": 70,
      "AA-Omniscience": -20,
      "AA-Omniscience Accuracy": 22.97,
      "AA-Omniscience Hallucination Rate": 55.28
    },
    "gpt-5-nano": {
      "MMMU Pro": 61,
      "AA-Omniscience": -30,
      "AA-Omniscience Accuracy": 18.28,
      "AA-Omniscience Hallucination Rate": 58.66
    },

    "o3": {
      "MMMU Pro": 70,
      "AA-Omniscience": -17,
      "AA-Omniscience Accuracy": 37.25,
      "AA-Omniscience Hallucination Rate": 86.75
    },



    "gemini-3-pro": {
      "MMMU Pro": 80,
      "AA-Omniscience": 13,
      "AA-Omniscience Accuracy": 53.65,
      "AA-Omniscience Hallucination Rate": 87.99
    },
    "gemini-2.5-pro": {
      "MMMU Pro": 75,
      "AA-Omniscience": -18,
      "AA-Omniscience Accuracy": 37.48,
      "AA-Omniscience Hallucination Rate": 88.67
    },
    "gemini-2-5-flash-preview-09-2025": {
      "MMMU Pro": 70,
      "AA-Omniscience": -41,
      "AA-Omniscience Accuracy": 25.77,
      "AA-Omniscience Hallucination Rate": 90.37
    },



    "claude-4-5-haiku": {
      "MMMU Pro": 55,
      "AA-Omniscience": -8,
      "AA-Omniscience Accuracy": 13.42,
      "AA-Omniscience Hallucination Rate": 24.68
    },
    "claude-4-5-sonnet": {
      "MMMU Pro": 65,
      "AA-Omniscience": -11,
      "AA-Omniscience Accuracy": 26.93,
      "AA-Omniscience Hallucination Rate": 51.44
    },
    "claude-opus-4-5": {
      "MMMU Pro": 71,
      "AA-Omniscience": -6,
      "AA-Omniscience Accuracy": 38.90,
      "AA-Omniscience Hallucination Rate": 74.22
    },



    "qwen3-max-preview": {
      "MMMU Pro": 0,
      "AA-Omniscience": -45,
      "AA-Omniscience Accuracy": 23.35,
      "AA-Omniscience Hallucination Rate": 89.04
    },
    "qwen3-235b-a22b-instruct-2507": {
      "MMMU Pro": 0,
      "AA-Omniscience": -45,
      "AA-Omniscience Accuracy": 17.58,
      "AA-Omniscience Hallucination Rate": 76.40
    },
    "qwen3-vl-235b-a22b-instruct": {
      "MMMU Pro": 68,
      "AA-Omniscience": -54,
      "AA-Omniscience Accuracy": 19.22,
      "AA-Omniscience Hallucination Rate": 90.47
    },
    "qwen3-next-80b-a3b-instruct": {
      "MMMU Pro": 0,
      "AA-Omniscience": -60,
      "AA-Omniscience Accuracy": 16.72,
      "AA-Omniscience Hallucination Rate": 92.70
    },

    "deepseek-v3-2": {
      "MMMU Pro": 0,
      "AA-Omniscience": -49,
      "AA-Omniscience Accuracy": 22.77,
      "AA-Omniscience Hallucination Rate": 92.51
    },
    "deepseek-v3-1-terminus": {
      "MMMU Pro": 0,
      "AA-Omniscience": -45,
      "AA-Omniscience Accuracy": 22.62,
      "AA-Omniscience Hallucination Rate": 86.84
    },
    "deepseek-r1": {
      "MMMU Pro": 0,
      "AA-Omniscience": -30,
      "AA-Omniscience Accuracy": 29.28,
      "AA-Omniscience Hallucination Rate": 83.36
    }
  },
  "metadata": {
    "lastUpdated": "2025-12-14",
    "description": "Manually entered benchmark scores not available via API"
  }
}